{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias, physics_kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                            out_channels=4 * self.hidden_dim,\n",
    "                            kernel_size=self.kernel_size,\n",
    "                            padding=self.padding,\n",
    "                            bias=self.bias)\n",
    "\n",
    "        self.physics_conv_x = nn.Conv2d(in_channels=self.input_dim,\n",
    "                                        out_channels=self.hidden_dim,\n",
    "                                        kernel_size=physics_kernel_size,\n",
    "                                        padding=physics_kernel_size[0] // 2,\n",
    "                                        bias=False)\n",
    "\n",
    "        self.physics_conv_y = nn.Conv2d(in_channels=self.input_dim,\n",
    "                                        out_channels=self.hidden_dim,\n",
    "                                        kernel_size=physics_kernel_size,\n",
    "                                        padding=physics_kernel_size[1] // 2,\n",
    "                                        bias=False)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        # Check if input_tensor has an extra dimension (sequence length)\n",
    "        if input_tensor.dim() == 5:\n",
    "            input_tensor = input_tensor.squeeze(1)\n",
    "\n",
    "        # Ensure the number of channels in input_tensor matches the input_dim\n",
    "        if input_tensor.size(1) != self.input_dim:\n",
    "            raise ValueError(f\"Expected input_tensor to have {self.input_dim} channels, but got {input_tensor.size(1)} channels instead\")\n",
    "\n",
    "        # Ensure the number of channels in h_cur matches the hidden_dim\n",
    "        if h_cur.size(1) != self.hidden_dim:\n",
    "            raise ValueError(f\"Expected h_cur to have {self.hidden_dim} channels, but got {h_cur.size(1)} channels instead\")\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "\n",
    "        # Compute physics-based convolutions\n",
    "        physics_conv_x = self.physics_conv_x(input_tensor)\n",
    "        physics_conv_y = self.physics_conv_y(input_tensor)\n",
    "\n",
    "        i = torch.sigmoid(cc_i + physics_conv_x)\n",
    "        f = torch.sigmoid(cc_f + physics_conv_x)\n",
    "        o = torch.sigmoid(cc_o + physics_conv_y)\n",
    "        g = torch.tanh(cc_g + physics_conv_y)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, physics_kernel_size,output_dim,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias,\n",
    "                                          physics_kernel_size=physics_kernel_size))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        self.output_conv = nn.Conv2d(in_channels=hidden_dim[-1],\n",
    "                                      out_channels=output_dim,\n",
    "                                      kernel_size=1,\n",
    "                                      padding=0)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if input_tensor.dim() == 4:\n",
    "            # (b, h, w, c) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(0, 3, 1, 2).unsqueeze(1)\n",
    "        elif input_tensor.dim() == 5:\n",
    "            if not self.batch_first:\n",
    "                # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "                input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, t, _, h, w = input_tensor.size()\n",
    "\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        # Remove the sequence length dimension before applying the output convolution\n",
    "        output = self.output_conv(layer_output_list[0].squeeze(1))\n",
    "        # Permute the output to have shape (b, h, w, c)\n",
    "        output = output.permute(0, 2, 3, 1)\n",
    "        return output, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sushen/anaconda3/envs/pinn/lib/python3.11/site-packages/matplotlib/animation.py:892: UserWarning: Animation was deleted without rendering anything. This is most likely not intended. To prevent deletion, assign the Animation to a variable, e.g. `anim`, that exists until you output the Animation using `plt.show()` or `anim.save()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load radar data\n",
    "movies = np.load('data/radar_movies.npy')\n",
    "movies.shape # (980, 40, 40, 20) -- here each movie is of length 20\n",
    "\n",
    "# in our model we will use the first four images as inputs and predict the\n",
    "# fifth image\n",
    "x = movies[:, :, :,  :4]\n",
    "y = movies[:, :, :, 4:5]\n",
    "\n",
    "\n",
    "# function: animation of a sequence of radar data (shape = nx,ny,ntime)\n",
    "def animate(x):\n",
    "  fig, ax = plt.subplots()\n",
    "  vmax = np.max(x)\n",
    "  im = ax.imshow(x[:,:,0], vmin=0, vmax=vmax)\n",
    "  fig.colorbar(im)\n",
    "  plt.axis('off')\n",
    "  def anim_(i):\n",
    "      im.set_data(x[:,:,i])\n",
    "      ax.set_title(str(i+1) + '/' + str(x.shape[2]))\n",
    "  anim = animation.FuncAnimation(\n",
    "      fig, anim_, interval=300, frames=x.shape[2], repeat_delay=1000)\n",
    "  plt.show()\n",
    "\n",
    "# i_plt = 340\n",
    "# i_plt = 123\n",
    "i_plt = np.int32(np.random.sample() * movies.shape[0])\n",
    "animate(x[i_plt,:,:,:])\n",
    "plt.show()\n",
    "\n",
    "# train validate test split\n",
    "tvt = np.tile(['train','train','train','validate','test'], y.shape[0])[:y.shape[0]]\n",
    "x_train = x[np.where(tvt == 'train')]\n",
    "y_train = y[np.where(tvt == 'train')]\n",
    "x_validate = x[np.where(tvt == 'validate')]\n",
    "y_validate = y[np.where(tvt == 'validate')]\n",
    "x_test = x[np.where(tvt == 'test')]\n",
    "y_test = y[np.where(tvt == 'test')]\n",
    "\n",
    "n_test = x_test.shape[0]\n",
    "i_plt = np.int32(np.random.sample() * n_test)\n",
    "true = np.append(x_test[i_plt,:,:,:], y_test[i_plt,:,:,:], axis=2)\n",
    "# plot an input/output pair\n",
    "i_plt = 20\n",
    "i_plt = np.int32(np.random.sample() * x_train.shape[0])\n",
    "for jj in range(4):\n",
    "  plt.subplot(1,5,jj+1)\n",
    "  plt.imshow(x_train[i_plt,:,:,jj])\n",
    "  plt.axis('off')\n",
    "  plt.title('input')\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(y_train[i_plt,:,:,0])\n",
    "plt.title('target output')\n",
    "plt.axis('off')\n",
    "plt.show()# load radar data\n",
    "movies = np.load('data/radar_movies.npy')\n",
    "movies.shape # (980, 40, 40, 20) -- here each movie is of length 20\n",
    "\n",
    "# in our model we will use the first four images as inputs and predict the\n",
    "# fifth image\n",
    "x = movies[:, :, :,  :4]\n",
    "y = movies[:, :, :, 4:5]\n",
    "\n",
    "\n",
    "# function: animation of a sequence of radar data (shape = nx,ny,ntime)\n",
    "def animate(x):\n",
    "  fig, ax = plt.subplots()\n",
    "  vmax = np.max(x)\n",
    "  im = ax.imshow(x[:,:,0], vmin=0, vmax=vmax)\n",
    "  fig.colorbar(im)\n",
    "  plt.axis('off')\n",
    "  def anim_(i):\n",
    "      im.set_data(x[:,:,i])\n",
    "      ax.set_title(str(i+1) + '/' + str(x.shape[2]))\n",
    "  anim = animation.FuncAnimation(\n",
    "      fig, anim_, interval=300, frames=x.shape[2], repeat_delay=1000)\n",
    "  plt.show()\n",
    "\n",
    "# i_plt = 340\n",
    "# i_plt = 123\n",
    "i_plt = np.int32(np.random.sample() * movies.shape[0])\n",
    "animate(x[i_plt,:,:,:])\n",
    "plt.show()\n",
    "\n",
    "# train validate test split\n",
    "tvt = np.tile(['train','train','train','validate','test'], y.shape[0])[:y.shape[0]]\n",
    "x_train = x[np.where(tvt == 'train')]\n",
    "y_train = y[np.where(tvt == 'train')]\n",
    "x_validate = x[np.where(tvt == 'validate')]\n",
    "y_validate = y[np.where(tvt == 'validate')]\n",
    "x_test = x[np.where(tvt == 'test')]\n",
    "y_test = y[np.where(tvt == 'test')]\n",
    "\n",
    "n_test = x_test.shape[0]\n",
    "i_plt = np.int32(np.random.sample() * n_test)\n",
    "true = np.append(x_test[i_plt,:,:,:], y_test[i_plt,:,:,:], axis=2)\n",
    "# plot an input/output pair\n",
    "i_plt = 20\n",
    "i_plt = np.int32(np.random.sample() * x_train.shape[0])\n",
    "for jj in range(4):\n",
    "  plt.subplot(1,5,jj+1)\n",
    "  plt.imshow(x_train[i_plt,:,:,jj])\n",
    "  plt.axis('off')\n",
    "  plt.title('input')\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(y_train[i_plt,:,:,0])\n",
    "plt.title('target output')\n",
    "plt.axis('off')\n",
    "plt.show()# load radar data\n",
    "movies = np.load('data/radar_movies.npy')\n",
    "movies.shape # (980, 40, 40, 20) -- here each movie is of length 20\n",
    "\n",
    "# in our model we will use the first four images as inputs and predict the\n",
    "# fifth image\n",
    "x = movies[:, :, :,  :4]\n",
    "y = movies[:, :, :, 4:5]\n",
    "\n",
    "\n",
    "# function: animation of a sequence of radar data (shape = nx,ny,ntime)\n",
    "def animate(x):\n",
    "  fig, ax = plt.subplots()\n",
    "  vmax = np.max(x)\n",
    "  im = ax.imshow(x[:,:,0], vmin=0, vmax=vmax)\n",
    "  fig.colorbar(im)\n",
    "  plt.axis('off')\n",
    "  def anim_(i):\n",
    "      im.set_data(x[:,:,i])\n",
    "      ax.set_title(str(i+1) + '/' + str(x.shape[2]))\n",
    "  anim = animation.FuncAnimation(\n",
    "      fig, anim_, interval=300, frames=x.shape[2], repeat_delay=1000)\n",
    "  plt.show()\n",
    "\n",
    "# i_plt = 340\n",
    "# i_plt = 123\n",
    "i_plt = np.int32(np.random.sample() * movies.shape[0])\n",
    "animate(x[i_plt,:,:,:])\n",
    "plt.show()\n",
    "\n",
    "# train validate test split\n",
    "tvt = np.tile(['train','train','train','validate','test'], y.shape[0])[:y.shape[0]]\n",
    "x_train = x[np.where(tvt == 'train')]\n",
    "y_train = y[np.where(tvt == 'train')]\n",
    "x_validate = x[np.where(tvt == 'validate')]\n",
    "y_validate = y[np.where(tvt == 'validate')]\n",
    "x_test = x[np.where(tvt == 'test')]\n",
    "y_test = y[np.where(tvt == 'test')]\n",
    "\n",
    "n_test = x_test.shape[0]\n",
    "i_plt = np.int32(np.random.sample() * n_test)\n",
    "true = np.append(x_test[i_plt,:,:,:], y_test[i_plt,:,:,:], axis=2)\n",
    "# plot an input/output pair\n",
    "i_plt = 20\n",
    "i_plt = np.int32(np.random.sample() * x_train.shape[0])\n",
    "for jj in range(4):\n",
    "  plt.subplot(1,5,jj+1)\n",
    "  plt.imshow(x_train[i_plt,:,:,jj])\n",
    "  plt.axis('off')\n",
    "  plt.title('input')\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(y_train[i_plt,:,:,0])\n",
    "plt.title('target output')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 950.7386, Val Loss: 1050.9084\n",
      "Epoch [2/1000], Train Loss: 918.0862, Val Loss: 1037.2325\n",
      "Epoch [3/1000], Train Loss: 907.7013, Val Loss: 1027.0621\n",
      "Epoch [4/1000], Train Loss: 897.9994, Val Loss: 1017.3604\n",
      "Epoch [5/1000], Train Loss: 888.9235, Val Loss: 1007.7018\n",
      "Epoch [6/1000], Train Loss: 880.3337, Val Loss: 998.7704\n",
      "Epoch [7/1000], Train Loss: 872.0970, Val Loss: 990.7799\n",
      "Epoch [8/1000], Train Loss: 864.5840, Val Loss: 983.1666\n",
      "Epoch [9/1000], Train Loss: 857.2725, Val Loss: 975.3822\n",
      "Epoch [10/1000], Train Loss: 850.2126, Val Loss: 967.8960\n",
      "Epoch [11/1000], Train Loss: 843.5942, Val Loss: 960.7309\n",
      "Epoch [12/1000], Train Loss: 836.8330, Val Loss: 954.1015\n",
      "Epoch [13/1000], Train Loss: 830.5438, Val Loss: 947.3594\n",
      "Epoch [14/1000], Train Loss: 824.3359, Val Loss: 940.8047\n",
      "Epoch [15/1000], Train Loss: 818.0953, Val Loss: 934.3096\n",
      "Epoch [16/1000], Train Loss: 812.1406, Val Loss: 928.5597\n",
      "Epoch [17/1000], Train Loss: 806.3961, Val Loss: 922.9542\n",
      "Epoch [18/1000], Train Loss: 800.9531, Val Loss: 917.7305\n",
      "Epoch [19/1000], Train Loss: 795.6974, Val Loss: 911.3724\n",
      "Epoch [20/1000], Train Loss: 790.3863, Val Loss: 905.7654\n",
      "Epoch [21/1000], Train Loss: 785.1747, Val Loss: 899.9085\n",
      "Epoch [22/1000], Train Loss: 780.2463, Val Loss: 894.7917\n",
      "Epoch [23/1000], Train Loss: 775.2012, Val Loss: 889.9316\n",
      "Epoch [24/1000], Train Loss: 770.7387, Val Loss: 884.9517\n",
      "Epoch [25/1000], Train Loss: 766.2502, Val Loss: 879.9469\n",
      "Epoch [26/1000], Train Loss: 761.4381, Val Loss: 874.9989\n",
      "Epoch [27/1000], Train Loss: 756.9116, Val Loss: 870.2415\n",
      "Epoch [28/1000], Train Loss: 752.4997, Val Loss: 866.5065\n",
      "Epoch [29/1000], Train Loss: 748.4456, Val Loss: 861.5314\n",
      "Epoch [30/1000], Train Loss: 744.8378, Val Loss: 859.7129\n",
      "Epoch [31/1000], Train Loss: 745.2515, Val Loss: 857.5739\n",
      "Epoch [32/1000], Train Loss: 739.7530, Val Loss: 851.6650\n",
      "Epoch [33/1000], Train Loss: 739.5793, Val Loss: 850.7940\n",
      "Epoch [34/1000], Train Loss: 734.2786, Val Loss: 844.6309\n",
      "Epoch [35/1000], Train Loss: 729.6167, Val Loss: 840.6002\n",
      "Epoch [36/1000], Train Loss: 725.7999, Val Loss: 836.8943\n",
      "Epoch [37/1000], Train Loss: 721.7893, Val Loss: 832.6381\n",
      "Epoch [38/1000], Train Loss: 717.9169, Val Loss: 828.9962\n",
      "Epoch [39/1000], Train Loss: 714.1659, Val Loss: 825.2744\n",
      "Epoch [40/1000], Train Loss: 710.6815, Val Loss: 822.1150\n",
      "Epoch [41/1000], Train Loss: 707.5670, Val Loss: 818.0190\n",
      "Epoch [42/1000], Train Loss: 704.4683, Val Loss: 818.5304\n",
      "Epoch [43/1000], Train Loss: 702.3883, Val Loss: 811.4786\n",
      "Epoch [44/1000], Train Loss: 698.4389, Val Loss: 808.3224\n",
      "Epoch [45/1000], Train Loss: 695.6953, Val Loss: 806.0960\n",
      "Epoch [46/1000], Train Loss: 692.6801, Val Loss: 802.9063\n",
      "Epoch [47/1000], Train Loss: 689.8244, Val Loss: 799.8747\n",
      "Epoch [48/1000], Train Loss: 686.4207, Val Loss: 796.7147\n",
      "Epoch [49/1000], Train Loss: 684.4051, Val Loss: 793.7565\n",
      "Epoch [50/1000], Train Loss: 681.5200, Val Loss: 790.6778\n",
      "Epoch [51/1000], Train Loss: 679.6476, Val Loss: 788.7622\n",
      "Epoch [52/1000], Train Loss: 676.7769, Val Loss: 786.2890\n",
      "Epoch [53/1000], Train Loss: 674.8563, Val Loss: 782.5636\n",
      "Epoch [54/1000], Train Loss: 671.1830, Val Loss: 779.5018\n",
      "Epoch [55/1000], Train Loss: 668.6513, Val Loss: 777.6340\n",
      "Epoch [56/1000], Train Loss: 665.8677, Val Loss: 774.6870\n",
      "Epoch [57/1000], Train Loss: 662.8166, Val Loss: 772.2582\n",
      "Epoch [58/1000], Train Loss: 660.7953, Val Loss: 771.9221\n",
      "Epoch [59/1000], Train Loss: 658.4580, Val Loss: 766.8921\n",
      "Epoch [60/1000], Train Loss: 655.5247, Val Loss: 766.9055\n",
      "Epoch [61/1000], Train Loss: 654.1689, Val Loss: 762.0275\n",
      "Epoch [62/1000], Train Loss: 652.1242, Val Loss: 762.2108\n",
      "Epoch [63/1000], Train Loss: 650.5651, Val Loss: 758.1549\n",
      "Epoch [64/1000], Train Loss: 648.3919, Val Loss: 771.5697\n",
      "Epoch [65/1000], Train Loss: 649.0045, Val Loss: 754.8461\n",
      "Epoch [66/1000], Train Loss: 644.2965, Val Loss: 751.6870\n",
      "Epoch [67/1000], Train Loss: 641.5021, Val Loss: 749.1551\n",
      "Epoch [68/1000], Train Loss: 639.3534, Val Loss: 747.5934\n",
      "Epoch [69/1000], Train Loss: 638.7752, Val Loss: 753.7503\n",
      "Epoch [70/1000], Train Loss: 639.6655, Val Loss: 747.1187\n",
      "Epoch [71/1000], Train Loss: 637.3864, Val Loss: 742.5777\n",
      "Epoch [72/1000], Train Loss: 632.1827, Val Loss: 738.7373\n",
      "Epoch [73/1000], Train Loss: 630.4879, Val Loss: 741.4543\n",
      "Epoch [74/1000], Train Loss: 629.3365, Val Loss: 735.9112\n",
      "Epoch [75/1000], Train Loss: 625.7622, Val Loss: 735.7775\n",
      "Epoch [76/1000], Train Loss: 626.7290, Val Loss: 736.7922\n",
      "Epoch [77/1000], Train Loss: 623.1816, Val Loss: 732.6870\n",
      "Epoch [78/1000], Train Loss: 621.1270, Val Loss: 727.6230\n",
      "Epoch [79/1000], Train Loss: 619.2507, Val Loss: 726.7369\n",
      "Epoch [80/1000], Train Loss: 617.6354, Val Loss: 724.8622\n",
      "Epoch [81/1000], Train Loss: 616.0712, Val Loss: 723.5852\n",
      "Epoch [82/1000], Train Loss: 613.8071, Val Loss: 720.7093\n",
      "Epoch [83/1000], Train Loss: 621.4009, Val Loss: 724.0953\n",
      "Epoch [84/1000], Train Loss: 617.8160, Val Loss: 721.2337\n",
      "Epoch [85/1000], Train Loss: 613.4180, Val Loss: 717.5173\n",
      "Epoch [86/1000], Train Loss: 609.2871, Val Loss: 718.4096\n",
      "Epoch [87/1000], Train Loss: 607.2442, Val Loss: 714.4632\n",
      "Epoch [88/1000], Train Loss: 604.9447, Val Loss: 711.0915\n",
      "Epoch [89/1000], Train Loss: 607.0589, Val Loss: 715.2249\n",
      "Epoch [90/1000], Train Loss: 614.1103, Val Loss: 712.2306\n",
      "Epoch [91/1000], Train Loss: 605.0658, Val Loss: 709.9015\n",
      "Epoch [92/1000], Train Loss: 601.1069, Val Loss: 708.7809\n",
      "Epoch [93/1000], Train Loss: 601.4790, Val Loss: 705.0860\n",
      "Epoch [94/1000], Train Loss: 598.6212, Val Loss: 707.6987\n",
      "Epoch [95/1000], Train Loss: 600.5236, Val Loss: 710.0032\n",
      "Epoch [96/1000], Train Loss: 635.7832, Val Loss: 733.1597\n",
      "Epoch [97/1000], Train Loss: 627.2216, Val Loss: 719.8146\n",
      "Epoch [98/1000], Train Loss: 614.3896, Val Loss: 730.3854\n",
      "Epoch [99/1000], Train Loss: 606.6753, Val Loss: 705.7368\n",
      "Epoch [100/1000], Train Loss: 599.3576, Val Loss: 702.1312\n",
      "Epoch [101/1000], Train Loss: 596.1665, Val Loss: 713.1060\n",
      "Epoch [102/1000], Train Loss: 592.7620, Val Loss: 696.8662\n",
      "Epoch [103/1000], Train Loss: 589.7439, Val Loss: 693.4956\n",
      "Epoch [104/1000], Train Loss: 588.1116, Val Loss: 696.9514\n",
      "Epoch [105/1000], Train Loss: 590.5999, Val Loss: 709.5815\n",
      "Epoch [106/1000], Train Loss: 597.6004, Val Loss: 709.7252\n",
      "Epoch [107/1000], Train Loss: 612.9290, Val Loss: 712.5171\n",
      "Epoch [108/1000], Train Loss: 598.6855, Val Loss: 697.8291\n",
      "Epoch [109/1000], Train Loss: 592.0667, Val Loss: 696.1547\n",
      "Epoch [110/1000], Train Loss: 588.0283, Val Loss: 691.1406\n",
      "Epoch [111/1000], Train Loss: 583.5712, Val Loss: 687.4721\n",
      "Epoch [112/1000], Train Loss: 581.5405, Val Loss: 688.7095\n",
      "Epoch [113/1000], Train Loss: 579.3987, Val Loss: 686.2169\n",
      "Epoch [114/1000], Train Loss: 578.7226, Val Loss: 683.9434\n",
      "Epoch [115/1000], Train Loss: 575.7064, Val Loss: 685.5428\n",
      "Epoch [116/1000], Train Loss: 575.9152, Val Loss: 689.6920\n",
      "Epoch [117/1000], Train Loss: 574.9271, Val Loss: 679.9126\n",
      "Epoch [118/1000], Train Loss: 575.9095, Val Loss: 689.5349\n",
      "Epoch [119/1000], Train Loss: 574.2644, Val Loss: 677.5680\n",
      "Epoch [120/1000], Train Loss: 569.7578, Val Loss: 674.4756\n",
      "Epoch [121/1000], Train Loss: 569.8562, Val Loss: 675.8138\n",
      "Epoch [122/1000], Train Loss: 566.5595, Val Loss: 672.7085\n",
      "Epoch [123/1000], Train Loss: 567.2640, Val Loss: 680.4640\n",
      "Epoch [124/1000], Train Loss: 566.7342, Val Loss: 672.7398\n",
      "Epoch [125/1000], Train Loss: 565.6689, Val Loss: 670.9057\n",
      "Epoch [126/1000], Train Loss: 561.6069, Val Loss: 669.7979\n",
      "Epoch [127/1000], Train Loss: 568.3032, Val Loss: 670.5090\n",
      "Epoch [128/1000], Train Loss: 560.8138, Val Loss: 666.7730\n",
      "Epoch [129/1000], Train Loss: 558.7460, Val Loss: 669.0397\n",
      "Epoch [130/1000], Train Loss: 566.7421, Val Loss: 674.2666\n",
      "Epoch [131/1000], Train Loss: 574.3999, Val Loss: 674.7783\n",
      "Epoch [132/1000], Train Loss: 563.4550, Val Loss: 668.7371\n",
      "Epoch [133/1000], Train Loss: 576.1929, Val Loss: 700.6645\n",
      "Epoch [134/1000], Train Loss: 577.2453, Val Loss: 672.8719\n",
      "Epoch [135/1000], Train Loss: 568.9393, Val Loss: 669.3354\n",
      "Epoch [136/1000], Train Loss: 559.0713, Val Loss: 665.2961\n",
      "Epoch [137/1000], Train Loss: 559.4852, Val Loss: 662.0230\n",
      "Epoch [138/1000], Train Loss: 553.3506, Val Loss: 660.1563\n",
      "Epoch [139/1000], Train Loss: 554.9476, Val Loss: 659.7943\n",
      "Epoch [140/1000], Train Loss: 550.5277, Val Loss: 660.6978\n",
      "Epoch [141/1000], Train Loss: 550.4596, Val Loss: 656.7449\n",
      "Epoch [142/1000], Train Loss: 548.1268, Val Loss: 665.2140\n",
      "Epoch [143/1000], Train Loss: 549.5686, Val Loss: 654.3223\n",
      "Epoch [144/1000], Train Loss: 545.3221, Val Loss: 653.1489\n",
      "Epoch [145/1000], Train Loss: 543.9841, Val Loss: 651.6873\n",
      "Epoch [146/1000], Train Loss: 542.3142, Val Loss: 652.5275\n",
      "Epoch [147/1000], Train Loss: 545.3027, Val Loss: 667.1960\n",
      "Epoch [148/1000], Train Loss: 551.0993, Val Loss: 651.8520\n",
      "Epoch [149/1000], Train Loss: 543.6629, Val Loss: 651.3347\n",
      "Epoch [150/1000], Train Loss: 540.5961, Val Loss: 647.6863\n",
      "Epoch [151/1000], Train Loss: 538.7171, Val Loss: 647.0314\n",
      "Epoch [152/1000], Train Loss: 537.1046, Val Loss: 646.6384\n",
      "Epoch [153/1000], Train Loss: 536.9602, Val Loss: 646.2469\n",
      "Epoch [154/1000], Train Loss: 536.2367, Val Loss: 650.1640\n",
      "Epoch [155/1000], Train Loss: 534.6644, Val Loss: 644.3879\n",
      "Epoch [156/1000], Train Loss: 533.6519, Val Loss: 642.5975\n",
      "Epoch [157/1000], Train Loss: 537.9376, Val Loss: 653.3239\n",
      "Epoch [158/1000], Train Loss: 537.0578, Val Loss: 642.6304\n",
      "Epoch [159/1000], Train Loss: 532.4142, Val Loss: 642.6517\n",
      "Epoch [160/1000], Train Loss: 530.5939, Val Loss: 639.5646\n",
      "Epoch [161/1000], Train Loss: 528.4781, Val Loss: 638.8838\n",
      "Epoch [162/1000], Train Loss: 530.7393, Val Loss: 654.1231\n",
      "Epoch [163/1000], Train Loss: 531.3697, Val Loss: 638.9587\n",
      "Epoch [164/1000], Train Loss: 535.5627, Val Loss: 646.8000\n",
      "Epoch [165/1000], Train Loss: 541.1495, Val Loss: 658.1068\n",
      "Epoch [166/1000], Train Loss: 535.8264, Val Loss: 648.4738\n",
      "Epoch [167/1000], Train Loss: 561.8210, Val Loss: 653.2836\n",
      "Epoch [168/1000], Train Loss: 537.2693, Val Loss: 641.6415\n",
      "Epoch [169/1000], Train Loss: 532.1498, Val Loss: 644.3428\n",
      "Epoch [170/1000], Train Loss: 530.6486, Val Loss: 646.4161\n",
      "Epoch [171/1000], Train Loss: 527.2274, Val Loss: 634.3100\n",
      "Epoch [172/1000], Train Loss: 524.4241, Val Loss: 631.6761\n",
      "Epoch [173/1000], Train Loss: 521.1554, Val Loss: 633.0679\n",
      "Epoch [174/1000], Train Loss: 520.7580, Val Loss: 635.9538\n",
      "Epoch [175/1000], Train Loss: 521.1450, Val Loss: 631.6967\n",
      "Epoch [176/1000], Train Loss: 518.7307, Val Loss: 628.5294\n",
      "Epoch [177/1000], Train Loss: 517.3181, Val Loss: 632.9864\n",
      "Epoch [178/1000], Train Loss: 519.2103, Val Loss: 631.8183\n",
      "Epoch [179/1000], Train Loss: 522.5822, Val Loss: 635.1712\n",
      "Epoch [180/1000], Train Loss: 535.2247, Val Loss: 638.0131\n",
      "Epoch [181/1000], Train Loss: 527.2710, Val Loss: 627.7267\n",
      "Epoch [182/1000], Train Loss: 518.5003, Val Loss: 624.2652\n",
      "Epoch [183/1000], Train Loss: 515.6335, Val Loss: 624.2359\n",
      "Epoch [184/1000], Train Loss: 514.2918, Val Loss: 630.2965\n",
      "Epoch [185/1000], Train Loss: 519.9747, Val Loss: 636.6354\n",
      "Epoch [186/1000], Train Loss: 519.4895, Val Loss: 625.0046\n",
      "Epoch [187/1000], Train Loss: 513.6853, Val Loss: 624.6472\n",
      "Epoch [188/1000], Train Loss: 513.1958, Val Loss: 624.8424\n",
      "Epoch [189/1000], Train Loss: 511.6494, Val Loss: 622.2215\n",
      "Epoch [190/1000], Train Loss: 509.6696, Val Loss: 625.0358\n",
      "Epoch [191/1000], Train Loss: 509.2421, Val Loss: 623.0046\n",
      "Epoch [192/1000], Train Loss: 510.4767, Val Loss: 618.9759\n",
      "Epoch [193/1000], Train Loss: 515.7832, Val Loss: 627.5247\n",
      "Epoch [194/1000], Train Loss: 512.5287, Val Loss: 618.2348\n",
      "Epoch [195/1000], Train Loss: 507.5498, Val Loss: 618.2372\n",
      "Epoch [196/1000], Train Loss: 504.4847, Val Loss: 619.9258\n",
      "Epoch [197/1000], Train Loss: 511.9471, Val Loss: 641.3149\n",
      "Epoch [198/1000], Train Loss: 513.0818, Val Loss: 629.2549\n",
      "Epoch [199/1000], Train Loss: 520.4828, Val Loss: 622.2571\n",
      "Epoch [200/1000], Train Loss: 509.1053, Val Loss: 669.6836\n",
      "Epoch [201/1000], Train Loss: 552.4600, Val Loss: 653.4393\n",
      "Epoch [202/1000], Train Loss: 531.4041, Val Loss: 644.6442\n",
      "Epoch [203/1000], Train Loss: 521.8491, Val Loss: 620.1834\n",
      "Epoch [204/1000], Train Loss: 508.6656, Val Loss: 613.9475\n",
      "Epoch [205/1000], Train Loss: 503.7806, Val Loss: 615.4623\n",
      "Epoch [206/1000], Train Loss: 504.2172, Val Loss: 626.8230\n",
      "Epoch [207/1000], Train Loss: 513.1387, Val Loss: 620.3535\n",
      "Epoch [208/1000], Train Loss: 503.2360, Val Loss: 612.4189\n",
      "Epoch [209/1000], Train Loss: 500.8910, Val Loss: 613.8381\n",
      "Epoch [210/1000], Train Loss: 500.9598, Val Loss: 611.4039\n",
      "Epoch [211/1000], Train Loss: 498.9687, Val Loss: 611.0151\n",
      "Epoch [212/1000], Train Loss: 499.0251, Val Loss: 610.1276\n",
      "Epoch [213/1000], Train Loss: 496.6904, Val Loss: 608.6057\n",
      "Epoch [214/1000], Train Loss: 494.8139, Val Loss: 608.1791\n",
      "Epoch [215/1000], Train Loss: 497.7265, Val Loss: 612.5744\n",
      "Epoch [216/1000], Train Loss: 501.7898, Val Loss: 610.2145\n",
      "Epoch [217/1000], Train Loss: 498.2554, Val Loss: 610.1678\n",
      "Epoch [218/1000], Train Loss: 498.6211, Val Loss: 625.6640\n",
      "Epoch [219/1000], Train Loss: 500.5699, Val Loss: 610.0105\n",
      "Epoch [220/1000], Train Loss: 491.9772, Val Loss: 606.3048\n",
      "Epoch [221/1000], Train Loss: 491.2357, Val Loss: 610.4679\n",
      "Epoch [222/1000], Train Loss: 493.4955, Val Loss: 614.5337\n",
      "Epoch [223/1000], Train Loss: 489.4825, Val Loss: 609.3498\n",
      "Epoch [224/1000], Train Loss: 489.3091, Val Loss: 604.9891\n",
      "Epoch [225/1000], Train Loss: 487.4450, Val Loss: 604.2971\n",
      "Epoch [226/1000], Train Loss: 490.1448, Val Loss: 606.4172\n",
      "Epoch [227/1000], Train Loss: 488.5388, Val Loss: 604.4453\n",
      "Epoch [228/1000], Train Loss: 486.0340, Val Loss: 613.5141\n",
      "Epoch [229/1000], Train Loss: 486.5198, Val Loss: 603.8477\n",
      "Epoch [230/1000], Train Loss: 488.8111, Val Loss: 619.5662\n",
      "Epoch [231/1000], Train Loss: 492.5637, Val Loss: 606.7431\n",
      "Epoch [232/1000], Train Loss: 485.4667, Val Loss: 604.6712\n",
      "Epoch [233/1000], Train Loss: 490.2333, Val Loss: 606.0505\n",
      "Epoch [234/1000], Train Loss: 485.8248, Val Loss: 601.7549\n",
      "Epoch [235/1000], Train Loss: 483.0482, Val Loss: 602.4575\n",
      "Epoch [236/1000], Train Loss: 488.4745, Val Loss: 601.8076\n",
      "Epoch [237/1000], Train Loss: 486.1963, Val Loss: 601.8106\n",
      "Epoch [238/1000], Train Loss: 484.6214, Val Loss: 603.7041\n",
      "Epoch [239/1000], Train Loss: 481.9836, Val Loss: 601.4714\n",
      "Epoch [240/1000], Train Loss: 478.9945, Val Loss: 606.6251\n",
      "Epoch [241/1000], Train Loss: 481.0777, Val Loss: 626.5983\n",
      "Epoch [242/1000], Train Loss: 488.3180, Val Loss: 602.9530\n",
      "Epoch [243/1000], Train Loss: 479.4463, Val Loss: 598.3980\n",
      "Epoch [244/1000], Train Loss: 481.6434, Val Loss: 612.1660\n",
      "Epoch [245/1000], Train Loss: 490.5169, Val Loss: 605.6843\n",
      "Epoch [246/1000], Train Loss: 486.0191, Val Loss: 606.3269\n",
      "Epoch [247/1000], Train Loss: 476.9321, Val Loss: 596.9901\n",
      "Epoch [248/1000], Train Loss: 476.0107, Val Loss: 597.0454\n",
      "Epoch [249/1000], Train Loss: 474.5466, Val Loss: 605.0639\n",
      "Epoch [250/1000], Train Loss: 481.5253, Val Loss: 614.6437\n",
      "Epoch [251/1000], Train Loss: 484.9655, Val Loss: 597.0235\n",
      "Epoch [252/1000], Train Loss: 476.7035, Val Loss: 597.3142\n",
      "Epoch [253/1000], Train Loss: 476.9397, Val Loss: 598.0667\n",
      "Epoch [254/1000], Train Loss: 474.0007, Val Loss: 595.8988\n",
      "Epoch [255/1000], Train Loss: 476.1475, Val Loss: 602.6034\n",
      "Epoch [256/1000], Train Loss: 473.1132, Val Loss: 596.5315\n",
      "Epoch [257/1000], Train Loss: 470.0689, Val Loss: 594.5817\n",
      "Epoch [258/1000], Train Loss: 471.0139, Val Loss: 594.3723\n",
      "Epoch [259/1000], Train Loss: 469.7343, Val Loss: 597.6103\n",
      "Epoch [260/1000], Train Loss: 469.1279, Val Loss: 592.5038\n",
      "Epoch [261/1000], Train Loss: 468.8500, Val Loss: 593.9144\n",
      "Epoch [262/1000], Train Loss: 475.7793, Val Loss: 594.0954\n",
      "Epoch [263/1000], Train Loss: 471.5470, Val Loss: 593.7966\n",
      "Epoch [264/1000], Train Loss: 473.6540, Val Loss: 600.2156\n",
      "Epoch [265/1000], Train Loss: 478.8076, Val Loss: 610.5413\n",
      "Epoch [266/1000], Train Loss: 471.5948, Val Loss: 605.9426\n",
      "Epoch [267/1000], Train Loss: 469.8210, Val Loss: 598.1397\n",
      "Epoch [268/1000], Train Loss: 465.5297, Val Loss: 592.9977\n",
      "Epoch [269/1000], Train Loss: 466.5885, Val Loss: 591.9608\n",
      "Epoch [270/1000], Train Loss: 464.2767, Val Loss: 591.4889\n",
      "Epoch [271/1000], Train Loss: 470.3257, Val Loss: 604.8268\n",
      "Epoch [272/1000], Train Loss: 481.2779, Val Loss: 595.4840\n",
      "Epoch [273/1000], Train Loss: 473.8953, Val Loss: 597.2358\n",
      "Epoch [274/1000], Train Loss: 467.3761, Val Loss: 594.8706\n",
      "Epoch [275/1000], Train Loss: 466.0947, Val Loss: 592.8733\n",
      "Epoch [276/1000], Train Loss: 467.0403, Val Loss: 611.2965\n",
      "Epoch [277/1000], Train Loss: 472.9884, Val Loss: 613.7859\n",
      "Epoch [278/1000], Train Loss: 483.8438, Val Loss: 595.9307\n",
      "Epoch [279/1000], Train Loss: 467.3499, Val Loss: 591.6091\n",
      "Epoch [280/1000], Train Loss: 461.9608, Val Loss: 590.8759\n",
      "Epoch [281/1000], Train Loss: 460.3609, Val Loss: 590.3367\n",
      "Epoch [282/1000], Train Loss: 458.6087, Val Loss: 589.5043\n",
      "Epoch [283/1000], Train Loss: 466.1615, Val Loss: 604.8414\n",
      "Epoch [284/1000], Train Loss: 466.0287, Val Loss: 596.8682\n",
      "Epoch [285/1000], Train Loss: 467.1469, Val Loss: 604.1281\n",
      "Epoch [286/1000], Train Loss: 491.6133, Val Loss: 673.3198\n",
      "Epoch [287/1000], Train Loss: 529.6553, Val Loss: 625.4423\n",
      "Epoch [288/1000], Train Loss: 495.1898, Val Loss: 631.8930\n",
      "Epoch [289/1000], Train Loss: 501.1883, Val Loss: 671.3107\n",
      "Epoch [290/1000], Train Loss: 500.0359, Val Loss: 626.5675\n",
      "Epoch [291/1000], Train Loss: 485.3518, Val Loss: 606.9995\n",
      "Epoch [292/1000], Train Loss: 473.7214, Val Loss: 609.8037\n",
      "Epoch [293/1000], Train Loss: 470.3557, Val Loss: 608.8314\n",
      "Epoch [294/1000], Train Loss: 465.2467, Val Loss: 609.8022\n",
      "Epoch [295/1000], Train Loss: 481.6615, Val Loss: 604.6372\n",
      "Epoch [296/1000], Train Loss: 472.7703, Val Loss: 595.0041\n",
      "Epoch [297/1000], Train Loss: 481.3834, Val Loss: 615.5447\n",
      "Epoch [298/1000], Train Loss: 471.6830, Val Loss: 594.9089\n",
      "Epoch [299/1000], Train Loss: 463.3988, Val Loss: 596.8194\n",
      "Epoch [300/1000], Train Loss: 462.0765, Val Loss: 592.0428\n",
      "Epoch [301/1000], Train Loss: 475.8357, Val Loss: 601.5240\n",
      "Epoch [302/1000], Train Loss: 469.2838, Val Loss: 596.9037\n",
      "Epoch [303/1000], Train Loss: 460.5895, Val Loss: 594.2392\n",
      "Epoch [304/1000], Train Loss: 459.1810, Val Loss: 602.5307\n",
      "Epoch [305/1000], Train Loss: 460.1728, Val Loss: 587.4556\n",
      "Epoch [306/1000], Train Loss: 454.4990, Val Loss: 598.1568\n",
      "Epoch [307/1000], Train Loss: 462.6856, Val Loss: 587.7410\n",
      "Epoch [308/1000], Train Loss: 453.5546, Val Loss: 600.9874\n",
      "Epoch [309/1000], Train Loss: 479.7375, Val Loss: 609.9065\n",
      "Epoch [310/1000], Train Loss: 461.2685, Val Loss: 584.7190\n",
      "Epoch [311/1000], Train Loss: 454.6441, Val Loss: 585.6597\n",
      "Epoch [312/1000], Train Loss: 455.0111, Val Loss: 587.5471\n",
      "Epoch [313/1000], Train Loss: 451.0085, Val Loss: 585.4858\n",
      "Epoch [314/1000], Train Loss: 450.2700, Val Loss: 586.5002\n",
      "Epoch [315/1000], Train Loss: 449.9704, Val Loss: 585.0567\n",
      "Epoch [316/1000], Train Loss: 448.1559, Val Loss: 588.7903\n",
      "Epoch [317/1000], Train Loss: 453.6397, Val Loss: 612.1539\n",
      "Epoch [318/1000], Train Loss: 456.8989, Val Loss: 583.5304\n",
      "Epoch [319/1000], Train Loss: 450.2196, Val Loss: 594.2576\n",
      "Epoch [320/1000], Train Loss: 455.4723, Val Loss: 590.1474\n",
      "Epoch [321/1000], Train Loss: 452.3887, Val Loss: 588.2554\n",
      "Epoch [322/1000], Train Loss: 452.3079, Val Loss: 584.0568\n",
      "Epoch [323/1000], Train Loss: 452.8088, Val Loss: 587.6399\n",
      "Epoch [324/1000], Train Loss: 447.1371, Val Loss: 587.7330\n",
      "Epoch [325/1000], Train Loss: 452.9179, Val Loss: 585.7587\n",
      "Epoch [326/1000], Train Loss: 446.1383, Val Loss: 582.3752\n",
      "Epoch [327/1000], Train Loss: 445.8787, Val Loss: 587.1595\n",
      "Epoch [328/1000], Train Loss: 444.7173, Val Loss: 584.4421\n",
      "Epoch [329/1000], Train Loss: 447.5706, Val Loss: 584.4712\n",
      "Epoch [330/1000], Train Loss: 442.8709, Val Loss: 584.2702\n",
      "Epoch [331/1000], Train Loss: 442.9733, Val Loss: 584.8505\n",
      "Epoch [332/1000], Train Loss: 445.2614, Val Loss: 595.9965\n",
      "Epoch [333/1000], Train Loss: 445.8559, Val Loss: 584.8974\n",
      "Epoch [334/1000], Train Loss: 443.6366, Val Loss: 585.8966\n",
      "Epoch [335/1000], Train Loss: 442.5085, Val Loss: 590.8136\n",
      "Epoch [336/1000], Train Loss: 446.6490, Val Loss: 581.9463\n",
      "Epoch [337/1000], Train Loss: 447.3421, Val Loss: 596.3484\n",
      "Epoch [338/1000], Train Loss: 461.4007, Val Loss: 585.9211\n",
      "Epoch [339/1000], Train Loss: 448.8664, Val Loss: 597.7135\n",
      "Epoch [340/1000], Train Loss: 453.9899, Val Loss: 588.3550\n",
      "Epoch [341/1000], Train Loss: 446.2216, Val Loss: 588.5210\n",
      "Epoch [342/1000], Train Loss: 441.9697, Val Loss: 583.9341\n",
      "Epoch [343/1000], Train Loss: 439.3919, Val Loss: 584.7773\n",
      "Epoch [344/1000], Train Loss: 441.2675, Val Loss: 579.3042\n",
      "Epoch [345/1000], Train Loss: 442.6954, Val Loss: 604.2215\n",
      "Epoch [346/1000], Train Loss: 466.9446, Val Loss: 640.8386\n",
      "Epoch [347/1000], Train Loss: 468.2053, Val Loss: 602.2932\n",
      "Epoch [348/1000], Train Loss: 452.8740, Val Loss: 582.4639\n",
      "Epoch [349/1000], Train Loss: 440.6927, Val Loss: 583.8135\n",
      "Epoch [350/1000], Train Loss: 441.7440, Val Loss: 593.3503\n",
      "Epoch [351/1000], Train Loss: 443.4122, Val Loss: 580.2331\n",
      "Epoch [352/1000], Train Loss: 436.4055, Val Loss: 582.9961\n",
      "Epoch [353/1000], Train Loss: 436.1925, Val Loss: 580.3673\n",
      "Epoch [354/1000], Train Loss: 437.0631, Val Loss: 583.6990\n",
      "Epoch [355/1000], Train Loss: 436.1756, Val Loss: 581.4710\n",
      "Epoch [356/1000], Train Loss: 433.7720, Val Loss: 581.8623\n",
      "Epoch [357/1000], Train Loss: 434.2669, Val Loss: 579.5697\n",
      "Epoch [358/1000], Train Loss: 444.7677, Val Loss: 582.9019\n",
      "Epoch [359/1000], Train Loss: 469.1616, Val Loss: 603.5339\n",
      "Epoch [360/1000], Train Loss: 459.8385, Val Loss: 590.3769\n",
      "Epoch [361/1000], Train Loss: 458.8225, Val Loss: 653.5569\n",
      "Epoch [362/1000], Train Loss: 457.4345, Val Loss: 587.4344\n",
      "Epoch [363/1000], Train Loss: 439.9990, Val Loss: 586.4760\n",
      "Epoch [364/1000], Train Loss: 445.8226, Val Loss: 588.1870\n",
      "Epoch [365/1000], Train Loss: 441.6732, Val Loss: 583.6620\n",
      "Epoch [366/1000], Train Loss: 433.9455, Val Loss: 582.9606\n",
      "Epoch [367/1000], Train Loss: 438.5998, Val Loss: 599.8518\n",
      "Epoch [368/1000], Train Loss: 437.3083, Val Loss: 596.5064\n",
      "Epoch [369/1000], Train Loss: 435.3286, Val Loss: 586.5585\n",
      "Epoch [370/1000], Train Loss: 433.5306, Val Loss: 579.5566\n",
      "Epoch [371/1000], Train Loss: 433.6363, Val Loss: 599.3821\n",
      "Epoch [372/1000], Train Loss: 443.8878, Val Loss: 608.1339\n",
      "Epoch [373/1000], Train Loss: 442.4565, Val Loss: 590.7071\n",
      "Epoch [374/1000], Train Loss: 442.6323, Val Loss: 580.7898\n",
      "Epoch [375/1000], Train Loss: 432.6505, Val Loss: 581.3826\n",
      "Epoch [376/1000], Train Loss: 436.7120, Val Loss: 591.6865\n",
      "Epoch [377/1000], Train Loss: 437.6624, Val Loss: 580.7181\n",
      "Epoch [378/1000], Train Loss: 430.9856, Val Loss: 614.5098\n",
      "Epoch [379/1000], Train Loss: 447.9873, Val Loss: 579.7963\n",
      "Epoch [380/1000], Train Loss: 444.9500, Val Loss: 581.5545\n",
      "Epoch [381/1000], Train Loss: 439.9499, Val Loss: 605.7610\n",
      "Epoch [382/1000], Train Loss: 455.6279, Val Loss: 597.7761\n",
      "Epoch [383/1000], Train Loss: 433.5344, Val Loss: 580.9053\n",
      "Epoch [384/1000], Train Loss: 433.1220, Val Loss: 584.3292\n",
      "Epoch [385/1000], Train Loss: 432.4662, Val Loss: 580.2652\n",
      "Epoch [386/1000], Train Loss: 431.7876, Val Loss: 577.2752\n",
      "Epoch [387/1000], Train Loss: 438.9753, Val Loss: 581.6536\n",
      "Epoch [388/1000], Train Loss: 431.4217, Val Loss: 578.9477\n",
      "Epoch [389/1000], Train Loss: 428.3342, Val Loss: 581.0477\n",
      "Epoch [390/1000], Train Loss: 434.1946, Val Loss: 596.1247\n",
      "Epoch [391/1000], Train Loss: 439.4368, Val Loss: 584.5324\n",
      "Epoch [392/1000], Train Loss: 431.3022, Val Loss: 593.5986\n",
      "Epoch [393/1000], Train Loss: 434.5840, Val Loss: 580.8110\n",
      "Epoch [394/1000], Train Loss: 431.0836, Val Loss: 580.2877\n",
      "Epoch [395/1000], Train Loss: 427.5986, Val Loss: 580.3307\n",
      "Epoch [396/1000], Train Loss: 424.0277, Val Loss: 588.4374\n",
      "Epoch [397/1000], Train Loss: 426.8862, Val Loss: 580.1420\n",
      "Epoch [398/1000], Train Loss: 424.5226, Val Loss: 580.2222\n",
      "Epoch [399/1000], Train Loss: 424.2875, Val Loss: 585.5403\n",
      "Epoch [400/1000], Train Loss: 426.7942, Val Loss: 584.9688\n",
      "Epoch [401/1000], Train Loss: 427.9908, Val Loss: 577.5984\n",
      "Epoch [402/1000], Train Loss: 423.0737, Val Loss: 576.1576\n",
      "Epoch [403/1000], Train Loss: 424.4013, Val Loss: 580.1001\n",
      "Epoch [404/1000], Train Loss: 420.2908, Val Loss: 579.5804\n",
      "Epoch [405/1000], Train Loss: 426.7133, Val Loss: 578.4012\n",
      "Epoch [406/1000], Train Loss: 431.2635, Val Loss: 618.8564\n",
      "Epoch [407/1000], Train Loss: 449.0211, Val Loss: 619.3947\n",
      "Epoch [408/1000], Train Loss: 444.7763, Val Loss: 582.5978\n",
      "Epoch [409/1000], Train Loss: 431.5786, Val Loss: 580.3310\n",
      "Epoch [410/1000], Train Loss: 425.3847, Val Loss: 594.1914\n",
      "Epoch [411/1000], Train Loss: 428.9947, Val Loss: 595.2465\n",
      "Epoch [412/1000], Train Loss: 425.4748, Val Loss: 582.2812\n",
      "Epoch [413/1000], Train Loss: 421.9968, Val Loss: 581.3304\n",
      "Epoch [414/1000], Train Loss: 418.9756, Val Loss: 582.8904\n",
      "Epoch [415/1000], Train Loss: 417.6774, Val Loss: 578.5154\n",
      "Epoch [416/1000], Train Loss: 419.2236, Val Loss: 589.5273\n",
      "Epoch [417/1000], Train Loss: 439.1940, Val Loss: 658.0622\n",
      "Epoch [418/1000], Train Loss: 458.2707, Val Loss: 585.5376\n",
      "Epoch [419/1000], Train Loss: 430.0931, Val Loss: 577.7224\n",
      "Epoch [420/1000], Train Loss: 420.5141, Val Loss: 584.5185\n",
      "Epoch [421/1000], Train Loss: 425.3860, Val Loss: 589.3142\n",
      "Epoch [422/1000], Train Loss: 431.4264, Val Loss: 592.1847\n",
      "Epoch [423/1000], Train Loss: 424.2254, Val Loss: 588.9205\n",
      "Epoch [424/1000], Train Loss: 420.4896, Val Loss: 578.3778\n",
      "Epoch [425/1000], Train Loss: 417.1467, Val Loss: 579.6303\n",
      "Epoch [426/1000], Train Loss: 416.8950, Val Loss: 577.2373\n",
      "Epoch [427/1000], Train Loss: 415.5756, Val Loss: 582.9580\n",
      "Epoch [428/1000], Train Loss: 415.3038, Val Loss: 578.7258\n",
      "Epoch [429/1000], Train Loss: 417.8189, Val Loss: 585.9731\n",
      "Epoch [430/1000], Train Loss: 420.3095, Val Loss: 606.7990\n",
      "Epoch [431/1000], Train Loss: 429.6996, Val Loss: 578.5437\n",
      "Epoch [432/1000], Train Loss: 419.4366, Val Loss: 575.7424\n",
      "Epoch [433/1000], Train Loss: 419.8215, Val Loss: 577.5647\n",
      "Epoch [434/1000], Train Loss: 414.6939, Val Loss: 580.8424\n",
      "Epoch [435/1000], Train Loss: 414.6600, Val Loss: 580.0016\n",
      "Epoch [436/1000], Train Loss: 416.7325, Val Loss: 581.0185\n",
      "Epoch [437/1000], Train Loss: 416.6728, Val Loss: 582.2148\n",
      "Epoch [438/1000], Train Loss: 413.6610, Val Loss: 592.3350\n",
      "Epoch [439/1000], Train Loss: 428.7156, Val Loss: 587.1856\n",
      "Epoch [440/1000], Train Loss: 428.7460, Val Loss: 581.4033\n",
      "Epoch [441/1000], Train Loss: 421.3237, Val Loss: 583.3626\n",
      "Epoch [442/1000], Train Loss: 416.9025, Val Loss: 589.5598\n",
      "Epoch [443/1000], Train Loss: 416.8343, Val Loss: 577.0964\n",
      "Epoch [444/1000], Train Loss: 413.2861, Val Loss: 577.4788\n",
      "Epoch [445/1000], Train Loss: 412.1334, Val Loss: 576.1328\n",
      "Epoch [446/1000], Train Loss: 421.4396, Val Loss: 587.4896\n",
      "Epoch [447/1000], Train Loss: 416.9234, Val Loss: 582.8996\n",
      "Epoch [448/1000], Train Loss: 418.6179, Val Loss: 585.0577\n",
      "Epoch [449/1000], Train Loss: 413.0271, Val Loss: 574.7645\n",
      "Epoch [450/1000], Train Loss: 410.2449, Val Loss: 580.5394\n",
      "Epoch [451/1000], Train Loss: 410.5294, Val Loss: 576.3063\n",
      "Epoch [452/1000], Train Loss: 413.1126, Val Loss: 578.9040\n",
      "Epoch [453/1000], Train Loss: 410.6460, Val Loss: 578.1156\n",
      "Epoch [454/1000], Train Loss: 409.7111, Val Loss: 579.5601\n",
      "Epoch [455/1000], Train Loss: 417.7429, Val Loss: 610.9901\n",
      "Epoch [456/1000], Train Loss: 435.6949, Val Loss: 608.5806\n",
      "Epoch [457/1000], Train Loss: 447.5804, Val Loss: 589.1327\n",
      "Epoch [458/1000], Train Loss: 425.2309, Val Loss: 578.7684\n",
      "Epoch [459/1000], Train Loss: 420.2936, Val Loss: 583.0768\n",
      "Epoch [460/1000], Train Loss: 412.6565, Val Loss: 581.0937\n",
      "Epoch [461/1000], Train Loss: 416.9730, Val Loss: 574.9309\n",
      "Epoch [462/1000], Train Loss: 413.3172, Val Loss: 581.6723\n",
      "Epoch [463/1000], Train Loss: 408.8214, Val Loss: 596.7585\n",
      "Epoch [464/1000], Train Loss: 416.1295, Val Loss: 591.1378\n",
      "Epoch [465/1000], Train Loss: 409.2547, Val Loss: 577.0187\n",
      "Epoch [466/1000], Train Loss: 414.5585, Val Loss: 597.6618\n",
      "Epoch [467/1000], Train Loss: 413.0255, Val Loss: 579.7882\n",
      "Epoch [468/1000], Train Loss: 408.9008, Val Loss: 576.0351\n",
      "Epoch [469/1000], Train Loss: 407.3085, Val Loss: 587.0782\n",
      "Epoch [470/1000], Train Loss: 419.3790, Val Loss: 588.2945\n",
      "Epoch [471/1000], Train Loss: 409.9973, Val Loss: 577.6578\n",
      "Epoch [472/1000], Train Loss: 408.9383, Val Loss: 580.7179\n",
      "Epoch [473/1000], Train Loss: 408.1205, Val Loss: 584.4619\n",
      "Epoch [474/1000], Train Loss: 411.9984, Val Loss: 598.7937\n",
      "Epoch [475/1000], Train Loss: 411.3077, Val Loss: 576.9739\n",
      "Epoch [476/1000], Train Loss: 405.3501, Val Loss: 575.3958\n",
      "Epoch [477/1000], Train Loss: 409.0389, Val Loss: 574.4419\n",
      "Epoch [478/1000], Train Loss: 409.8488, Val Loss: 574.6594\n",
      "Epoch [479/1000], Train Loss: 411.6482, Val Loss: 577.2716\n",
      "Epoch [480/1000], Train Loss: 409.1487, Val Loss: 582.0176\n",
      "Epoch [481/1000], Train Loss: 425.0654, Val Loss: 598.2363\n",
      "Epoch [482/1000], Train Loss: 420.5150, Val Loss: 580.5662\n",
      "Epoch [483/1000], Train Loss: 415.3348, Val Loss: 581.3483\n",
      "Epoch [484/1000], Train Loss: 421.6570, Val Loss: 598.1560\n",
      "Epoch [485/1000], Train Loss: 419.2260, Val Loss: 580.0849\n",
      "Epoch [486/1000], Train Loss: 407.3938, Val Loss: 576.1500\n",
      "Epoch [487/1000], Train Loss: 406.4470, Val Loss: 579.1779\n",
      "Epoch [488/1000], Train Loss: 409.5408, Val Loss: 583.0140\n",
      "Epoch [489/1000], Train Loss: 405.2158, Val Loss: 588.0457\n",
      "Epoch [490/1000], Train Loss: 402.7235, Val Loss: 576.8504\n",
      "Epoch [491/1000], Train Loss: 406.2834, Val Loss: 579.8271\n",
      "Epoch [492/1000], Train Loss: 408.4829, Val Loss: 588.0445\n",
      "Epoch [493/1000], Train Loss: 409.4936, Val Loss: 617.1188\n",
      "Epoch [494/1000], Train Loss: 433.7940, Val Loss: 595.7518\n",
      "Epoch [495/1000], Train Loss: 419.0507, Val Loss: 579.2071\n",
      "Epoch [496/1000], Train Loss: 407.0453, Val Loss: 591.1692\n",
      "Epoch [497/1000], Train Loss: 407.8973, Val Loss: 576.6930\n",
      "Epoch [498/1000], Train Loss: 411.1917, Val Loss: 590.3839\n",
      "Epoch [499/1000], Train Loss: 409.4906, Val Loss: 590.5720\n",
      "Epoch [500/1000], Train Loss: 406.7077, Val Loss: 610.3234\n",
      "Epoch [501/1000], Train Loss: 417.0031, Val Loss: 594.6357\n",
      "Epoch [502/1000], Train Loss: 413.5405, Val Loss: 589.3833\n",
      "Epoch [503/1000], Train Loss: 479.7766, Val Loss: 623.5953\n",
      "Epoch [504/1000], Train Loss: 434.9436, Val Loss: 581.8614\n",
      "Epoch [505/1000], Train Loss: 416.3669, Val Loss: 579.5960\n",
      "Epoch [506/1000], Train Loss: 409.8539, Val Loss: 578.5456\n",
      "Epoch [507/1000], Train Loss: 405.1232, Val Loss: 584.2535\n",
      "Epoch [508/1000], Train Loss: 407.2937, Val Loss: 573.9226\n",
      "Epoch [509/1000], Train Loss: 398.9669, Val Loss: 575.6984\n",
      "Epoch [510/1000], Train Loss: 398.3472, Val Loss: 575.4844\n",
      "Epoch [511/1000], Train Loss: 398.4779, Val Loss: 576.4644\n",
      "Epoch [512/1000], Train Loss: 398.9530, Val Loss: 582.7722\n",
      "Epoch [513/1000], Train Loss: 400.8943, Val Loss: 615.5366\n",
      "Epoch [514/1000], Train Loss: 425.6250, Val Loss: 598.8303\n",
      "Epoch [515/1000], Train Loss: 432.0202, Val Loss: 591.8276\n",
      "Epoch [516/1000], Train Loss: 415.3379, Val Loss: 582.3230\n",
      "Epoch [517/1000], Train Loss: 403.8069, Val Loss: 579.8079\n",
      "Epoch [518/1000], Train Loss: 423.4518, Val Loss: 611.6485\n",
      "Epoch [519/1000], Train Loss: 411.8073, Val Loss: 580.7452\n",
      "Epoch [520/1000], Train Loss: 405.9545, Val Loss: 590.7123\n",
      "Epoch [521/1000], Train Loss: 422.6755, Val Loss: 603.3830\n",
      "Epoch [522/1000], Train Loss: 428.5063, Val Loss: 587.2587\n",
      "Epoch [523/1000], Train Loss: 448.9835, Val Loss: 612.6825\n",
      "Epoch [524/1000], Train Loss: 417.4885, Val Loss: 584.9342\n",
      "Epoch [525/1000], Train Loss: 411.8065, Val Loss: 589.0110\n",
      "Epoch [526/1000], Train Loss: 401.6335, Val Loss: 578.3071\n",
      "Epoch [527/1000], Train Loss: 401.2176, Val Loss: 598.1763\n",
      "Epoch [528/1000], Train Loss: 417.5361, Val Loss: 580.9678\n",
      "Epoch [529/1000], Train Loss: 402.3909, Val Loss: 585.9419\n",
      "Epoch [530/1000], Train Loss: 397.0662, Val Loss: 578.8281\n",
      "Epoch [531/1000], Train Loss: 400.6559, Val Loss: 600.7641\n",
      "Epoch [532/1000], Train Loss: 407.1359, Val Loss: 600.5401\n",
      "Epoch [533/1000], Train Loss: 419.4130, Val Loss: 583.5604\n",
      "Epoch [534/1000], Train Loss: 401.7665, Val Loss: 581.5799\n",
      "Epoch [535/1000], Train Loss: 400.9429, Val Loss: 578.6690\n",
      "Epoch [536/1000], Train Loss: 395.5620, Val Loss: 576.6206\n",
      "Epoch [537/1000], Train Loss: 393.4603, Val Loss: 577.2450\n",
      "Epoch [538/1000], Train Loss: 393.9193, Val Loss: 576.2924\n",
      "Epoch [539/1000], Train Loss: 395.1149, Val Loss: 581.6620\n",
      "Epoch [540/1000], Train Loss: 394.7186, Val Loss: 591.0156\n",
      "Epoch [541/1000], Train Loss: 399.5937, Val Loss: 578.1783\n",
      "Epoch [542/1000], Train Loss: 392.5032, Val Loss: 577.3549\n",
      "Epoch [543/1000], Train Loss: 392.4454, Val Loss: 577.4740\n",
      "Epoch [544/1000], Train Loss: 393.2226, Val Loss: 577.5232\n",
      "Epoch [545/1000], Train Loss: 397.1022, Val Loss: 579.2779\n",
      "Epoch [546/1000], Train Loss: 394.2091, Val Loss: 575.5511\n",
      "Epoch [547/1000], Train Loss: 393.7061, Val Loss: 577.3785\n",
      "Epoch [548/1000], Train Loss: 396.3829, Val Loss: 608.3636\n",
      "Epoch [549/1000], Train Loss: 401.6051, Val Loss: 579.8261\n",
      "Epoch [550/1000], Train Loss: 393.1991, Val Loss: 578.1419\n",
      "Epoch [551/1000], Train Loss: 392.5061, Val Loss: 587.2676\n",
      "Epoch [552/1000], Train Loss: 394.4686, Val Loss: 576.2949\n",
      "Epoch [553/1000], Train Loss: 394.6499, Val Loss: 576.6395\n",
      "Epoch [554/1000], Train Loss: 390.2654, Val Loss: 583.8097\n",
      "Epoch [555/1000], Train Loss: 404.6791, Val Loss: 578.8261\n",
      "Epoch [556/1000], Train Loss: 397.6187, Val Loss: 580.0446\n",
      "Epoch [557/1000], Train Loss: 390.3817, Val Loss: 582.2506\n",
      "Epoch [558/1000], Train Loss: 389.6794, Val Loss: 581.4259\n",
      "Epoch [559/1000], Train Loss: 394.4197, Val Loss: 587.2216\n",
      "Epoch [560/1000], Train Loss: 392.8077, Val Loss: 580.4765\n",
      "Epoch [561/1000], Train Loss: 390.1978, Val Loss: 579.2017\n",
      "Epoch [562/1000], Train Loss: 395.6895, Val Loss: 583.9816\n",
      "Epoch [563/1000], Train Loss: 408.3175, Val Loss: 607.2650\n",
      "Epoch [564/1000], Train Loss: 413.2816, Val Loss: 601.7264\n",
      "Epoch [565/1000], Train Loss: 410.3159, Val Loss: 580.6931\n",
      "Epoch [566/1000], Train Loss: 395.4777, Val Loss: 578.1644\n",
      "Epoch [567/1000], Train Loss: 390.7074, Val Loss: 573.7164\n",
      "Epoch [568/1000], Train Loss: 395.3673, Val Loss: 579.9134\n",
      "Epoch [569/1000], Train Loss: 397.0715, Val Loss: 586.9750\n",
      "Epoch [570/1000], Train Loss: 391.0609, Val Loss: 581.2046\n",
      "Epoch [571/1000], Train Loss: 391.5582, Val Loss: 588.1704\n",
      "Epoch [572/1000], Train Loss: 389.3866, Val Loss: 580.3366\n",
      "Epoch [573/1000], Train Loss: 388.9424, Val Loss: 602.1433\n",
      "Epoch [574/1000], Train Loss: 395.7598, Val Loss: 593.8461\n",
      "Epoch [575/1000], Train Loss: 402.7980, Val Loss: 580.0927\n",
      "Epoch [576/1000], Train Loss: 402.0712, Val Loss: 584.1232\n",
      "Epoch [577/1000], Train Loss: 392.1243, Val Loss: 576.1787\n",
      "Epoch [578/1000], Train Loss: 387.7871, Val Loss: 576.5118\n",
      "Epoch [579/1000], Train Loss: 395.7471, Val Loss: 583.8819\n",
      "Epoch [580/1000], Train Loss: 394.5270, Val Loss: 578.1612\n",
      "Epoch [581/1000], Train Loss: 388.1106, Val Loss: 580.1156\n",
      "Epoch [582/1000], Train Loss: 386.7155, Val Loss: 577.9690\n",
      "Epoch [583/1000], Train Loss: 405.9931, Val Loss: 578.0963\n",
      "Epoch [584/1000], Train Loss: 391.8752, Val Loss: 576.7621\n",
      "Epoch [585/1000], Train Loss: 400.5498, Val Loss: 644.4703\n",
      "Epoch [586/1000], Train Loss: 422.3233, Val Loss: 579.4765\n",
      "Epoch [587/1000], Train Loss: 399.3806, Val Loss: 594.4682\n",
      "Epoch [588/1000], Train Loss: 404.3860, Val Loss: 587.6355\n",
      "Epoch [589/1000], Train Loss: 392.2550, Val Loss: 582.1460\n",
      "Epoch [590/1000], Train Loss: 388.5387, Val Loss: 577.6835\n",
      "Epoch [591/1000], Train Loss: 388.6114, Val Loss: 581.5766\n",
      "Epoch [592/1000], Train Loss: 388.0806, Val Loss: 583.0224\n",
      "Epoch [593/1000], Train Loss: 386.4538, Val Loss: 578.6204\n",
      "Epoch [594/1000], Train Loss: 388.7214, Val Loss: 598.1893\n",
      "Epoch [595/1000], Train Loss: 421.6350, Val Loss: 585.4972\n",
      "Epoch [596/1000], Train Loss: 402.9877, Val Loss: 604.9571\n",
      "Epoch [597/1000], Train Loss: 400.2236, Val Loss: 579.7179\n",
      "Epoch [598/1000], Train Loss: 390.1293, Val Loss: 580.4844\n",
      "Epoch [599/1000], Train Loss: 385.2341, Val Loss: 578.1205\n",
      "Epoch [600/1000], Train Loss: 383.5767, Val Loss: 579.3273\n",
      "Epoch [601/1000], Train Loss: 385.1300, Val Loss: 578.2443\n",
      "Epoch [602/1000], Train Loss: 383.3832, Val Loss: 576.4482\n",
      "Epoch [603/1000], Train Loss: 385.6068, Val Loss: 592.4227\n",
      "Epoch [604/1000], Train Loss: 390.2792, Val Loss: 580.5712\n",
      "Epoch [605/1000], Train Loss: 384.1269, Val Loss: 604.4564\n",
      "Epoch [606/1000], Train Loss: 397.0937, Val Loss: 594.6137\n",
      "Epoch [607/1000], Train Loss: 420.4735, Val Loss: 627.2456\n",
      "Epoch [608/1000], Train Loss: 411.6336, Val Loss: 582.1704\n",
      "Epoch [609/1000], Train Loss: 396.5494, Val Loss: 578.5357\n",
      "Epoch [610/1000], Train Loss: 387.9500, Val Loss: 576.1891\n",
      "Epoch [611/1000], Train Loss: 391.0095, Val Loss: 580.5206\n",
      "Epoch [612/1000], Train Loss: 389.2135, Val Loss: 578.6731\n",
      "Epoch [613/1000], Train Loss: 384.8570, Val Loss: 583.9408\n",
      "Epoch [614/1000], Train Loss: 394.6768, Val Loss: 578.8288\n",
      "Epoch [615/1000], Train Loss: 384.7669, Val Loss: 578.1389\n",
      "Epoch [616/1000], Train Loss: 384.3690, Val Loss: 592.7934\n",
      "Epoch [617/1000], Train Loss: 390.2184, Val Loss: 579.0670\n",
      "Epoch [618/1000], Train Loss: 382.7641, Val Loss: 589.0809\n",
      "Epoch [619/1000], Train Loss: 414.2955, Val Loss: 597.3395\n",
      "Epoch [620/1000], Train Loss: 412.3229, Val Loss: 582.2378\n",
      "Epoch [621/1000], Train Loss: 388.9683, Val Loss: 581.8089\n",
      "Epoch [622/1000], Train Loss: 391.5498, Val Loss: 585.4883\n",
      "Epoch [623/1000], Train Loss: 397.6545, Val Loss: 618.7932\n",
      "Epoch [624/1000], Train Loss: 395.1970, Val Loss: 590.7057\n",
      "Epoch [625/1000], Train Loss: 392.6106, Val Loss: 598.5411\n",
      "Epoch [626/1000], Train Loss: 394.6177, Val Loss: 581.0131\n",
      "Epoch [627/1000], Train Loss: 385.9334, Val Loss: 578.6668\n",
      "Epoch [628/1000], Train Loss: 380.8948, Val Loss: 579.7258\n",
      "Epoch [629/1000], Train Loss: 383.5120, Val Loss: 579.5021\n",
      "Epoch [630/1000], Train Loss: 388.9099, Val Loss: 581.4057\n",
      "Epoch [631/1000], Train Loss: 397.7705, Val Loss: 591.3388\n",
      "Epoch [632/1000], Train Loss: 386.6413, Val Loss: 591.5170\n",
      "Epoch [633/1000], Train Loss: 398.2655, Val Loss: 600.9308\n",
      "Epoch [634/1000], Train Loss: 414.7288, Val Loss: 597.8156\n",
      "Epoch [635/1000], Train Loss: 392.4657, Val Loss: 584.7449\n",
      "Epoch [636/1000], Train Loss: 399.0930, Val Loss: 585.0881\n",
      "Epoch [637/1000], Train Loss: 388.4870, Val Loss: 585.7306\n",
      "Epoch [638/1000], Train Loss: 382.2943, Val Loss: 581.1689\n",
      "Epoch [639/1000], Train Loss: 380.2187, Val Loss: 585.6271\n",
      "Epoch [640/1000], Train Loss: 402.9094, Val Loss: 594.0900\n",
      "Epoch [641/1000], Train Loss: 399.5194, Val Loss: 589.4471\n",
      "Epoch [642/1000], Train Loss: 381.5245, Val Loss: 590.1360\n",
      "Epoch [643/1000], Train Loss: 382.5464, Val Loss: 584.7939\n",
      "Epoch [644/1000], Train Loss: 380.1257, Val Loss: 588.2248\n",
      "Epoch [645/1000], Train Loss: 383.8604, Val Loss: 590.0117\n",
      "Epoch [646/1000], Train Loss: 387.7187, Val Loss: 585.9082\n",
      "Epoch [647/1000], Train Loss: 398.7251, Val Loss: 611.6015\n",
      "Epoch [648/1000], Train Loss: 388.8883, Val Loss: 582.4593\n",
      "Epoch [649/1000], Train Loss: 380.2325, Val Loss: 588.8936\n",
      "Epoch [650/1000], Train Loss: 385.6580, Val Loss: 578.9238\n",
      "Epoch [651/1000], Train Loss: 378.8191, Val Loss: 583.2440\n",
      "Epoch [652/1000], Train Loss: 378.3812, Val Loss: 579.8816\n",
      "Epoch [653/1000], Train Loss: 377.3790, Val Loss: 589.0895\n",
      "Epoch [654/1000], Train Loss: 393.9841, Val Loss: 589.2736\n",
      "Epoch [655/1000], Train Loss: 404.2816, Val Loss: 596.5870\n",
      "Epoch [656/1000], Train Loss: 391.3629, Val Loss: 597.8374\n",
      "Epoch [657/1000], Train Loss: 400.6735, Val Loss: 583.7620\n",
      "Epoch [658/1000], Train Loss: 382.3109, Val Loss: 582.2124\n",
      "Epoch [659/1000], Train Loss: 378.4411, Val Loss: 583.6961\n",
      "Epoch [660/1000], Train Loss: 379.0067, Val Loss: 580.3374\n",
      "Epoch [661/1000], Train Loss: 376.7331, Val Loss: 579.2520\n",
      "Epoch [662/1000], Train Loss: 376.3623, Val Loss: 585.4582\n",
      "Epoch [663/1000], Train Loss: 376.8188, Val Loss: 596.6972\n",
      "Epoch [664/1000], Train Loss: 406.5909, Val Loss: 611.5462\n",
      "Epoch [665/1000], Train Loss: 393.0815, Val Loss: 589.2344\n",
      "Epoch [666/1000], Train Loss: 382.3247, Val Loss: 580.7101\n",
      "Epoch [667/1000], Train Loss: 389.0505, Val Loss: 582.7041\n",
      "Epoch [668/1000], Train Loss: 380.3466, Val Loss: 580.2840\n",
      "Epoch [669/1000], Train Loss: 384.9626, Val Loss: 580.7157\n",
      "Epoch [670/1000], Train Loss: 380.1233, Val Loss: 579.9658\n",
      "Epoch [671/1000], Train Loss: 381.9198, Val Loss: 606.9783\n",
      "Epoch [672/1000], Train Loss: 401.2893, Val Loss: 584.5236\n",
      "Epoch [673/1000], Train Loss: 393.4589, Val Loss: 578.3386\n",
      "Epoch [674/1000], Train Loss: 415.8056, Val Loss: 613.4829\n",
      "Epoch [675/1000], Train Loss: 413.4199, Val Loss: 598.8332\n",
      "Epoch [676/1000], Train Loss: 388.4889, Val Loss: 581.8042\n",
      "Epoch [677/1000], Train Loss: 382.1444, Val Loss: 581.5068\n",
      "Epoch [678/1000], Train Loss: 378.8260, Val Loss: 582.9641\n",
      "Epoch [679/1000], Train Loss: 376.2376, Val Loss: 580.8915\n",
      "Epoch [680/1000], Train Loss: 374.7977, Val Loss: 588.7105\n",
      "Epoch [681/1000], Train Loss: 377.6495, Val Loss: 601.6765\n",
      "Epoch [682/1000], Train Loss: 374.9723, Val Loss: 582.4656\n",
      "Epoch [683/1000], Train Loss: 376.8711, Val Loss: 581.9701\n",
      "Epoch [684/1000], Train Loss: 376.3223, Val Loss: 579.8160\n",
      "Epoch [685/1000], Train Loss: 375.4507, Val Loss: 582.0374\n",
      "Epoch [686/1000], Train Loss: 389.1748, Val Loss: 585.3309\n",
      "Epoch [687/1000], Train Loss: 381.3663, Val Loss: 580.7138\n",
      "Epoch [688/1000], Train Loss: 376.0818, Val Loss: 581.4203\n",
      "Epoch [689/1000], Train Loss: 370.7681, Val Loss: 582.4800\n",
      "Epoch [690/1000], Train Loss: 370.1505, Val Loss: 596.8176\n",
      "Epoch [691/1000], Train Loss: 399.7825, Val Loss: 599.9339\n",
      "Epoch [692/1000], Train Loss: 386.5196, Val Loss: 579.1523\n",
      "Epoch [693/1000], Train Loss: 378.9552, Val Loss: 583.2178\n",
      "Epoch [694/1000], Train Loss: 374.0652, Val Loss: 579.8896\n",
      "Epoch [695/1000], Train Loss: 372.3828, Val Loss: 584.2950\n",
      "Epoch [696/1000], Train Loss: 384.6595, Val Loss: 583.3483\n",
      "Epoch [697/1000], Train Loss: 381.7046, Val Loss: 582.0418\n",
      "Epoch [698/1000], Train Loss: 406.2747, Val Loss: 586.0119\n",
      "Epoch [699/1000], Train Loss: 392.1910, Val Loss: 599.7188\n",
      "Epoch [700/1000], Train Loss: 381.9782, Val Loss: 580.9679\n",
      "Epoch [701/1000], Train Loss: 374.6779, Val Loss: 582.0436\n",
      "Epoch [702/1000], Train Loss: 372.3776, Val Loss: 583.9482\n",
      "Epoch [703/1000], Train Loss: 369.2426, Val Loss: 583.1337\n",
      "Epoch [704/1000], Train Loss: 368.8931, Val Loss: 589.7186\n",
      "Epoch [705/1000], Train Loss: 376.1467, Val Loss: 583.2150\n",
      "Epoch [706/1000], Train Loss: 391.5511, Val Loss: 587.9046\n",
      "Epoch [707/1000], Train Loss: 401.0074, Val Loss: 604.4003\n",
      "Epoch [708/1000], Train Loss: 387.8485, Val Loss: 583.7834\n",
      "Epoch [709/1000], Train Loss: 379.2472, Val Loss: 582.3796\n",
      "Epoch [710/1000], Train Loss: 374.1888, Val Loss: 585.6236\n",
      "Epoch [711/1000], Train Loss: 373.4520, Val Loss: 580.6993\n",
      "Epoch [712/1000], Train Loss: 371.5755, Val Loss: 588.8459\n",
      "Epoch [713/1000], Train Loss: 375.3684, Val Loss: 580.6228\n",
      "Epoch [714/1000], Train Loss: 385.8286, Val Loss: 598.8523\n",
      "Epoch [715/1000], Train Loss: 380.2595, Val Loss: 586.4756\n",
      "Epoch [716/1000], Train Loss: 388.6846, Val Loss: 592.9779\n",
      "Epoch [717/1000], Train Loss: 380.6918, Val Loss: 583.3923\n",
      "Epoch [718/1000], Train Loss: 383.1246, Val Loss: 584.7898\n",
      "Epoch [719/1000], Train Loss: 381.6631, Val Loss: 583.5769\n",
      "Epoch [720/1000], Train Loss: 371.0206, Val Loss: 586.8918\n",
      "Epoch [721/1000], Train Loss: 371.0785, Val Loss: 583.6026\n",
      "Epoch [722/1000], Train Loss: 370.4662, Val Loss: 582.1060\n",
      "Epoch [723/1000], Train Loss: 373.8141, Val Loss: 583.6082\n",
      "Epoch [724/1000], Train Loss: 376.5020, Val Loss: 613.8747\n",
      "Epoch [725/1000], Train Loss: 377.8157, Val Loss: 589.2087\n",
      "Epoch [726/1000], Train Loss: 372.7338, Val Loss: 587.7829\n",
      "Epoch [727/1000], Train Loss: 375.1387, Val Loss: 585.7244\n",
      "Epoch [728/1000], Train Loss: 372.0160, Val Loss: 581.7423\n",
      "Epoch [729/1000], Train Loss: 369.6489, Val Loss: 580.0817\n",
      "Epoch [730/1000], Train Loss: 371.8694, Val Loss: 580.6157\n",
      "Epoch [731/1000], Train Loss: 369.0920, Val Loss: 581.1442\n",
      "Epoch [732/1000], Train Loss: 378.2533, Val Loss: 593.9726\n",
      "Epoch [733/1000], Train Loss: 375.8695, Val Loss: 582.0019\n",
      "Epoch [734/1000], Train Loss: 368.2907, Val Loss: 582.7144\n",
      "Epoch [735/1000], Train Loss: 366.7731, Val Loss: 581.9316\n",
      "Epoch [736/1000], Train Loss: 365.5945, Val Loss: 581.9881\n",
      "Epoch [737/1000], Train Loss: 386.3530, Val Loss: 632.2989\n",
      "Epoch [738/1000], Train Loss: 409.1766, Val Loss: 591.1522\n",
      "Epoch [739/1000], Train Loss: 402.6151, Val Loss: 589.7242\n",
      "Epoch [740/1000], Train Loss: 385.8312, Val Loss: 587.3439\n",
      "Epoch [741/1000], Train Loss: 390.2467, Val Loss: 612.6299\n",
      "Epoch [742/1000], Train Loss: 387.3175, Val Loss: 597.4514\n",
      "Epoch [743/1000], Train Loss: 405.1496, Val Loss: 586.9907\n",
      "Epoch [744/1000], Train Loss: 376.8090, Val Loss: 584.2345\n",
      "Epoch [745/1000], Train Loss: 371.9878, Val Loss: 581.0660\n",
      "Epoch [746/1000], Train Loss: 378.2092, Val Loss: 596.2494\n",
      "Epoch [747/1000], Train Loss: 369.9888, Val Loss: 583.9298\n",
      "Epoch [748/1000], Train Loss: 367.7685, Val Loss: 584.9626\n",
      "Epoch [749/1000], Train Loss: 367.5035, Val Loss: 584.1627\n",
      "Epoch [750/1000], Train Loss: 369.1431, Val Loss: 583.1368\n",
      "Epoch [751/1000], Train Loss: 370.2188, Val Loss: 585.6699\n",
      "Epoch [752/1000], Train Loss: 366.4096, Val Loss: 583.1654\n",
      "Epoch [753/1000], Train Loss: 365.3314, Val Loss: 584.3641\n",
      "Epoch [754/1000], Train Loss: 370.3698, Val Loss: 581.8413\n",
      "Epoch [755/1000], Train Loss: 365.1569, Val Loss: 582.8188\n",
      "Epoch [756/1000], Train Loss: 367.5239, Val Loss: 582.9653\n",
      "Epoch [757/1000], Train Loss: 369.0055, Val Loss: 584.8809\n",
      "Epoch [758/1000], Train Loss: 366.1818, Val Loss: 584.3528\n",
      "Epoch [759/1000], Train Loss: 374.0338, Val Loss: 583.8428\n",
      "Epoch [760/1000], Train Loss: 377.0094, Val Loss: 586.9583\n",
      "Epoch [761/1000], Train Loss: 369.2976, Val Loss: 589.6852\n",
      "Epoch [762/1000], Train Loss: 371.7581, Val Loss: 614.0194\n",
      "Epoch [763/1000], Train Loss: 375.3166, Val Loss: 598.2082\n",
      "Epoch [764/1000], Train Loss: 368.6955, Val Loss: 585.8386\n",
      "Epoch [765/1000], Train Loss: 367.2243, Val Loss: 583.5927\n",
      "Epoch [766/1000], Train Loss: 368.2627, Val Loss: 583.3090\n",
      "Epoch [767/1000], Train Loss: 368.5364, Val Loss: 597.2254\n",
      "Epoch [768/1000], Train Loss: 376.0615, Val Loss: 592.4885\n",
      "Epoch [769/1000], Train Loss: 380.5333, Val Loss: 590.2946\n",
      "Epoch [770/1000], Train Loss: 374.2870, Val Loss: 584.1976\n",
      "Epoch [771/1000], Train Loss: 366.3798, Val Loss: 609.3610\n",
      "Epoch [772/1000], Train Loss: 370.3817, Val Loss: 585.7181\n",
      "Epoch [773/1000], Train Loss: 368.1133, Val Loss: 589.6935\n",
      "Epoch [774/1000], Train Loss: 375.6813, Val Loss: 607.2165\n",
      "Epoch [775/1000], Train Loss: 373.8332, Val Loss: 600.1814\n",
      "Epoch [776/1000], Train Loss: 376.6379, Val Loss: 593.2604\n",
      "Epoch [777/1000], Train Loss: 372.0636, Val Loss: 584.9201\n",
      "Epoch [778/1000], Train Loss: 368.4492, Val Loss: 590.2878\n",
      "Epoch [779/1000], Train Loss: 368.4234, Val Loss: 584.3031\n",
      "Epoch [780/1000], Train Loss: 369.1994, Val Loss: 584.8815\n",
      "Epoch [781/1000], Train Loss: 369.0246, Val Loss: 583.1481\n",
      "Epoch [782/1000], Train Loss: 368.6493, Val Loss: 581.0573\n",
      "Epoch [783/1000], Train Loss: 362.8801, Val Loss: 584.1747\n",
      "Epoch [784/1000], Train Loss: 362.2331, Val Loss: 583.8076\n",
      "Epoch [785/1000], Train Loss: 363.2727, Val Loss: 583.8572\n",
      "Epoch [786/1000], Train Loss: 363.0239, Val Loss: 586.3531\n",
      "Epoch [787/1000], Train Loss: 362.6908, Val Loss: 584.3741\n",
      "Epoch [788/1000], Train Loss: 360.5626, Val Loss: 589.1755\n",
      "Epoch [789/1000], Train Loss: 365.9236, Val Loss: 586.2477\n",
      "Epoch [790/1000], Train Loss: 362.1842, Val Loss: 598.9074\n",
      "Epoch [791/1000], Train Loss: 363.6525, Val Loss: 583.3605\n",
      "Epoch [792/1000], Train Loss: 367.8068, Val Loss: 583.3569\n",
      "Epoch [793/1000], Train Loss: 381.7819, Val Loss: 585.2374\n",
      "Epoch [794/1000], Train Loss: 371.4205, Val Loss: 601.2336\n",
      "Epoch [795/1000], Train Loss: 390.7761, Val Loss: 603.4491\n",
      "Epoch [796/1000], Train Loss: 398.3349, Val Loss: 588.1433\n",
      "Epoch [797/1000], Train Loss: 369.4852, Val Loss: 589.0473\n",
      "Epoch [798/1000], Train Loss: 370.2248, Val Loss: 590.8014\n",
      "Epoch [799/1000], Train Loss: 363.0971, Val Loss: 584.5396\n",
      "Epoch [800/1000], Train Loss: 364.9297, Val Loss: 593.8353\n",
      "Epoch [801/1000], Train Loss: 374.2878, Val Loss: 587.8063\n",
      "Epoch [802/1000], Train Loss: 384.8084, Val Loss: 587.0269\n",
      "Epoch [803/1000], Train Loss: 376.7495, Val Loss: 584.4987\n",
      "Epoch [804/1000], Train Loss: 363.9720, Val Loss: 582.5557\n",
      "Epoch [805/1000], Train Loss: 359.6866, Val Loss: 585.6881\n",
      "Epoch [806/1000], Train Loss: 362.1660, Val Loss: 582.3040\n",
      "Epoch [807/1000], Train Loss: 361.9999, Val Loss: 582.6447\n",
      "Epoch [808/1000], Train Loss: 360.8642, Val Loss: 582.7599\n",
      "Epoch [809/1000], Train Loss: 360.2908, Val Loss: 584.1527\n",
      "Epoch [810/1000], Train Loss: 359.2352, Val Loss: 583.0128\n",
      "Epoch [811/1000], Train Loss: 359.1038, Val Loss: 582.6890\n",
      "Epoch [812/1000], Train Loss: 365.3224, Val Loss: 619.1062\n",
      "Epoch [813/1000], Train Loss: 385.2881, Val Loss: 593.7726\n",
      "Epoch [814/1000], Train Loss: 372.0504, Val Loss: 589.2926\n",
      "Epoch [815/1000], Train Loss: 373.9926, Val Loss: 591.2442\n",
      "Epoch [816/1000], Train Loss: 362.4821, Val Loss: 595.8717\n",
      "Epoch [817/1000], Train Loss: 360.2331, Val Loss: 585.8775\n",
      "Epoch [818/1000], Train Loss: 357.8870, Val Loss: 587.1392\n",
      "Epoch [819/1000], Train Loss: 359.6919, Val Loss: 594.6850\n",
      "Epoch [820/1000], Train Loss: 379.3529, Val Loss: 592.4887\n",
      "Epoch [821/1000], Train Loss: 386.0658, Val Loss: 615.6664\n",
      "Epoch [822/1000], Train Loss: 394.1623, Val Loss: 617.7109\n",
      "Epoch [823/1000], Train Loss: 396.9410, Val Loss: 615.5139\n",
      "Epoch [824/1000], Train Loss: 382.6468, Val Loss: 592.9719\n",
      "Epoch [825/1000], Train Loss: 370.6670, Val Loss: 598.6986\n",
      "Epoch [826/1000], Train Loss: 364.9236, Val Loss: 591.4629\n",
      "Epoch [827/1000], Train Loss: 379.1270, Val Loss: 600.1027\n",
      "Epoch [828/1000], Train Loss: 381.7957, Val Loss: 611.5546\n",
      "Epoch [829/1000], Train Loss: 366.0452, Val Loss: 595.2988\n",
      "Epoch [830/1000], Train Loss: 364.5231, Val Loss: 587.5840\n",
      "Epoch [831/1000], Train Loss: 362.0582, Val Loss: 589.6957\n",
      "Epoch [832/1000], Train Loss: 374.2117, Val Loss: 607.9812\n",
      "Epoch [833/1000], Train Loss: 366.7714, Val Loss: 586.7186\n",
      "Epoch [834/1000], Train Loss: 362.0340, Val Loss: 593.7624\n",
      "Epoch [835/1000], Train Loss: 359.6401, Val Loss: 587.1628\n",
      "Epoch [836/1000], Train Loss: 359.2044, Val Loss: 600.2410\n",
      "Epoch [837/1000], Train Loss: 386.7614, Val Loss: 609.6344\n",
      "Epoch [838/1000], Train Loss: 375.9598, Val Loss: 600.7095\n",
      "Epoch [839/1000], Train Loss: 378.0106, Val Loss: 587.2683\n",
      "Epoch [840/1000], Train Loss: 362.3444, Val Loss: 588.3797\n",
      "Epoch [841/1000], Train Loss: 359.6701, Val Loss: 598.7296\n",
      "Epoch [842/1000], Train Loss: 381.3220, Val Loss: 674.5078\n",
      "Epoch [843/1000], Train Loss: 411.6453, Val Loss: 607.3746\n",
      "Epoch [844/1000], Train Loss: 384.3114, Val Loss: 590.2957\n",
      "Epoch [845/1000], Train Loss: 363.0323, Val Loss: 587.3141\n",
      "Epoch [846/1000], Train Loss: 363.9736, Val Loss: 590.5436\n",
      "Epoch [847/1000], Train Loss: 359.3001, Val Loss: 597.1587\n",
      "Epoch [848/1000], Train Loss: 359.3028, Val Loss: 587.2460\n",
      "Epoch [849/1000], Train Loss: 355.3390, Val Loss: 588.6281\n",
      "Epoch [850/1000], Train Loss: 357.6686, Val Loss: 596.4572\n",
      "Epoch [851/1000], Train Loss: 355.1704, Val Loss: 593.2064\n",
      "Epoch [852/1000], Train Loss: 362.1186, Val Loss: 604.8726\n",
      "Epoch [853/1000], Train Loss: 395.6073, Val Loss: 686.0043\n",
      "Epoch [854/1000], Train Loss: 432.6402, Val Loss: 629.0555\n",
      "Epoch [855/1000], Train Loss: 394.5550, Val Loss: 597.1640\n",
      "Epoch [856/1000], Train Loss: 371.5676, Val Loss: 592.1291\n",
      "Epoch [857/1000], Train Loss: 362.6757, Val Loss: 585.3782\n",
      "Epoch [858/1000], Train Loss: 361.1037, Val Loss: 586.7574\n",
      "Epoch [859/1000], Train Loss: 358.1932, Val Loss: 590.6117\n",
      "Epoch [860/1000], Train Loss: 357.8326, Val Loss: 633.1916\n",
      "Epoch [861/1000], Train Loss: 384.7617, Val Loss: 625.4708\n",
      "Epoch [862/1000], Train Loss: 375.3675, Val Loss: 589.4044\n",
      "Epoch [863/1000], Train Loss: 361.4887, Val Loss: 590.9496\n",
      "Epoch [864/1000], Train Loss: 369.7553, Val Loss: 614.4490\n",
      "Epoch [865/1000], Train Loss: 366.9902, Val Loss: 596.0803\n",
      "Epoch [866/1000], Train Loss: 371.1875, Val Loss: 622.3311\n",
      "Epoch [867/1000], Train Loss: 377.3131, Val Loss: 599.4804\n",
      "Epoch [868/1000], Train Loss: 379.9462, Val Loss: 588.0867\n",
      "Epoch [869/1000], Train Loss: 367.3548, Val Loss: 587.1127\n",
      "Epoch [870/1000], Train Loss: 366.1466, Val Loss: 584.6783\n",
      "Epoch [871/1000], Train Loss: 358.3206, Val Loss: 587.6186\n",
      "Epoch [872/1000], Train Loss: 354.4666, Val Loss: 588.1888\n",
      "Epoch [873/1000], Train Loss: 353.0536, Val Loss: 586.5385\n",
      "Epoch [874/1000], Train Loss: 353.0465, Val Loss: 588.3423\n",
      "Epoch [875/1000], Train Loss: 354.5770, Val Loss: 586.9463\n",
      "Epoch [876/1000], Train Loss: 354.5701, Val Loss: 585.9899\n",
      "Epoch [877/1000], Train Loss: 351.8500, Val Loss: 586.6762\n",
      "Epoch [878/1000], Train Loss: 351.9939, Val Loss: 602.5867\n",
      "Epoch [879/1000], Train Loss: 388.9947, Val Loss: 588.0458\n",
      "Epoch [880/1000], Train Loss: 372.2083, Val Loss: 590.9154\n",
      "Epoch [881/1000], Train Loss: 365.8504, Val Loss: 589.3383\n",
      "Epoch [882/1000], Train Loss: 355.9940, Val Loss: 590.4388\n",
      "Epoch [883/1000], Train Loss: 355.9918, Val Loss: 610.7970\n",
      "Epoch [884/1000], Train Loss: 358.5580, Val Loss: 587.6658\n",
      "Epoch [885/1000], Train Loss: 359.7249, Val Loss: 604.5049\n",
      "Epoch [886/1000], Train Loss: 370.2644, Val Loss: 591.3592\n",
      "Epoch [887/1000], Train Loss: 355.9142, Val Loss: 588.2800\n",
      "Epoch [888/1000], Train Loss: 352.3081, Val Loss: 591.7654\n",
      "Epoch [889/1000], Train Loss: 353.2256, Val Loss: 627.0156\n",
      "Epoch [890/1000], Train Loss: 385.2950, Val Loss: 642.0453\n",
      "Epoch [891/1000], Train Loss: 367.0839, Val Loss: 589.8259\n",
      "Epoch [892/1000], Train Loss: 354.8860, Val Loss: 614.7827\n",
      "Epoch [893/1000], Train Loss: 366.1726, Val Loss: 591.0111\n",
      "Epoch [894/1000], Train Loss: 355.4625, Val Loss: 601.5084\n",
      "Epoch [895/1000], Train Loss: 356.2470, Val Loss: 594.7795\n",
      "Epoch [896/1000], Train Loss: 362.7512, Val Loss: 593.3313\n",
      "Epoch [897/1000], Train Loss: 354.6409, Val Loss: 597.8181\n",
      "Epoch [898/1000], Train Loss: 354.8250, Val Loss: 586.3490\n",
      "Epoch [899/1000], Train Loss: 349.9487, Val Loss: 594.5327\n",
      "Epoch [900/1000], Train Loss: 353.0113, Val Loss: 590.3712\n",
      "Epoch [901/1000], Train Loss: 349.4720, Val Loss: 590.2379\n",
      "Epoch [902/1000], Train Loss: 353.9073, Val Loss: 590.6976\n",
      "Epoch [903/1000], Train Loss: 353.0580, Val Loss: 589.2545\n",
      "Epoch [904/1000], Train Loss: 358.8994, Val Loss: 601.1160\n",
      "Epoch [905/1000], Train Loss: 359.9802, Val Loss: 590.4018\n",
      "Epoch [906/1000], Train Loss: 352.4363, Val Loss: 589.1750\n",
      "Epoch [907/1000], Train Loss: 360.0555, Val Loss: 596.2925\n",
      "Epoch [908/1000], Train Loss: 374.0164, Val Loss: 621.0391\n",
      "Epoch [909/1000], Train Loss: 386.6573, Val Loss: 592.4199\n",
      "Epoch [910/1000], Train Loss: 364.6830, Val Loss: 598.6868\n",
      "Epoch [911/1000], Train Loss: 365.1797, Val Loss: 590.7995\n",
      "Epoch [912/1000], Train Loss: 353.7094, Val Loss: 589.7359\n",
      "Epoch [913/1000], Train Loss: 350.3382, Val Loss: 591.1389\n",
      "Epoch [914/1000], Train Loss: 353.6528, Val Loss: 587.8408\n",
      "Epoch [915/1000], Train Loss: 354.9090, Val Loss: 596.5740\n",
      "Epoch [916/1000], Train Loss: 350.3808, Val Loss: 594.0647\n",
      "Epoch [917/1000], Train Loss: 354.4995, Val Loss: 586.9461\n",
      "Epoch [918/1000], Train Loss: 372.2405, Val Loss: 592.1110\n",
      "Epoch [919/1000], Train Loss: 355.9234, Val Loss: 590.8970\n",
      "Epoch [920/1000], Train Loss: 350.9401, Val Loss: 618.3241\n",
      "Epoch [921/1000], Train Loss: 356.0306, Val Loss: 629.7113\n",
      "Epoch [922/1000], Train Loss: 393.0612, Val Loss: 599.4540\n",
      "Epoch [923/1000], Train Loss: 375.4937, Val Loss: 593.1754\n",
      "Epoch [924/1000], Train Loss: 361.8056, Val Loss: 590.7541\n",
      "Epoch [925/1000], Train Loss: 352.7794, Val Loss: 590.7800\n",
      "Epoch [926/1000], Train Loss: 349.8262, Val Loss: 590.5848\n",
      "Epoch [927/1000], Train Loss: 350.8097, Val Loss: 627.6276\n",
      "Epoch [928/1000], Train Loss: 386.2848, Val Loss: 623.0974\n",
      "Epoch [929/1000], Train Loss: 360.6342, Val Loss: 611.0562\n",
      "Epoch [930/1000], Train Loss: 356.9177, Val Loss: 588.8109\n",
      "Epoch [931/1000], Train Loss: 354.5053, Val Loss: 597.6860\n",
      "Epoch [932/1000], Train Loss: 364.0513, Val Loss: 594.4393\n",
      "Epoch [933/1000], Train Loss: 357.0216, Val Loss: 591.4084\n",
      "Epoch [934/1000], Train Loss: 353.1749, Val Loss: 592.8140\n",
      "Epoch [935/1000], Train Loss: 353.1555, Val Loss: 598.7488\n",
      "Epoch [936/1000], Train Loss: 354.1234, Val Loss: 596.7903\n",
      "Epoch [937/1000], Train Loss: 355.6588, Val Loss: 614.9676\n",
      "Epoch [938/1000], Train Loss: 351.7971, Val Loss: 590.6752\n",
      "Epoch [939/1000], Train Loss: 348.9482, Val Loss: 612.1857\n",
      "Epoch [940/1000], Train Loss: 366.7597, Val Loss: 627.5660\n",
      "Epoch [941/1000], Train Loss: 401.8247, Val Loss: 613.2875\n",
      "Epoch [942/1000], Train Loss: 377.0508, Val Loss: 597.9253\n",
      "Epoch [943/1000], Train Loss: 359.7786, Val Loss: 590.8643\n",
      "Epoch [944/1000], Train Loss: 351.6147, Val Loss: 599.0531\n",
      "Epoch [945/1000], Train Loss: 354.8494, Val Loss: 593.9008\n",
      "Epoch [946/1000], Train Loss: 351.7957, Val Loss: 593.2635\n",
      "Epoch [947/1000], Train Loss: 349.1082, Val Loss: 600.0747\n",
      "Epoch [948/1000], Train Loss: 347.4361, Val Loss: 591.8612\n",
      "Epoch [949/1000], Train Loss: 346.4694, Val Loss: 589.4061\n",
      "Epoch [950/1000], Train Loss: 348.2951, Val Loss: 592.7270\n",
      "Epoch [951/1000], Train Loss: 346.7894, Val Loss: 591.5971\n",
      "Epoch [952/1000], Train Loss: 347.9926, Val Loss: 593.3795\n",
      "Epoch [953/1000], Train Loss: 350.3038, Val Loss: 592.5036\n",
      "Epoch [954/1000], Train Loss: 366.0055, Val Loss: 595.7119\n",
      "Epoch [955/1000], Train Loss: 385.1546, Val Loss: 591.3400\n",
      "Epoch [956/1000], Train Loss: 360.7494, Val Loss: 592.8158\n",
      "Epoch [957/1000], Train Loss: 357.9234, Val Loss: 602.2060\n",
      "Epoch [958/1000], Train Loss: 351.1948, Val Loss: 612.1992\n",
      "Epoch [959/1000], Train Loss: 364.7574, Val Loss: 623.7347\n",
      "Epoch [960/1000], Train Loss: 374.9901, Val Loss: 600.4005\n",
      "Epoch [961/1000], Train Loss: 363.0805, Val Loss: 613.9376\n",
      "Epoch [962/1000], Train Loss: 371.0671, Val Loss: 612.6786\n",
      "Epoch [963/1000], Train Loss: 361.7840, Val Loss: 591.8316\n",
      "Epoch [964/1000], Train Loss: 348.3372, Val Loss: 591.4059\n",
      "Epoch [965/1000], Train Loss: 347.5137, Val Loss: 593.4596\n",
      "Epoch [966/1000], Train Loss: 371.9423, Val Loss: 620.0275\n",
      "Epoch [967/1000], Train Loss: 376.3277, Val Loss: 602.8701\n",
      "Epoch [968/1000], Train Loss: 380.8843, Val Loss: 610.7219\n",
      "Epoch [969/1000], Train Loss: 369.5210, Val Loss: 594.0772\n",
      "Epoch [970/1000], Train Loss: 359.2900, Val Loss: 593.9836\n",
      "Epoch [971/1000], Train Loss: 348.2818, Val Loss: 589.9669\n",
      "Epoch [972/1000], Train Loss: 349.1733, Val Loss: 592.5437\n",
      "Epoch [973/1000], Train Loss: 362.5018, Val Loss: 590.4241\n",
      "Epoch [974/1000], Train Loss: 354.4795, Val Loss: 592.5090\n",
      "Epoch [975/1000], Train Loss: 348.8444, Val Loss: 590.1396\n",
      "Epoch [976/1000], Train Loss: 359.6024, Val Loss: 614.2689\n",
      "Epoch [977/1000], Train Loss: 356.9982, Val Loss: 597.4414\n",
      "Epoch [978/1000], Train Loss: 359.8810, Val Loss: 590.4036\n",
      "Epoch [979/1000], Train Loss: 347.1440, Val Loss: 592.5902\n",
      "Epoch [980/1000], Train Loss: 345.4562, Val Loss: 591.2271\n",
      "Epoch [981/1000], Train Loss: 343.8324, Val Loss: 592.1692\n",
      "Epoch [982/1000], Train Loss: 342.6473, Val Loss: 590.8450\n",
      "Epoch [983/1000], Train Loss: 342.9707, Val Loss: 591.6066\n",
      "Epoch [984/1000], Train Loss: 353.8222, Val Loss: 595.0028\n",
      "Epoch [985/1000], Train Loss: 349.5422, Val Loss: 594.5114\n",
      "Epoch [986/1000], Train Loss: 355.1204, Val Loss: 598.5894\n",
      "Epoch [987/1000], Train Loss: 351.0887, Val Loss: 609.2717\n",
      "Epoch [988/1000], Train Loss: 347.7677, Val Loss: 589.6791\n",
      "Epoch [989/1000], Train Loss: 352.7601, Val Loss: 601.7115\n",
      "Epoch [990/1000], Train Loss: 363.0675, Val Loss: 595.0005\n",
      "Epoch [991/1000], Train Loss: 346.5369, Val Loss: 592.8265\n",
      "Epoch [992/1000], Train Loss: 354.5298, Val Loss: 599.4564\n",
      "Epoch [993/1000], Train Loss: 350.0466, Val Loss: 602.1601\n",
      "Epoch [994/1000], Train Loss: 349.1211, Val Loss: 603.3763\n",
      "Epoch [995/1000], Train Loss: 341.4826, Val Loss: 590.6567\n",
      "Epoch [996/1000], Train Loss: 348.2813, Val Loss: 590.8264\n",
      "Epoch [997/1000], Train Loss: 345.4429, Val Loss: 591.8837\n",
      "Epoch [998/1000], Train Loss: 342.8656, Val Loss: 594.5872\n",
      "Epoch [999/1000], Train Loss: 341.2106, Val Loss: 596.9291\n",
      "Epoch [1000/1000], Train Loss: 343.5734, Val Loss: 604.0475\n",
      "Test Loss: 598.3487\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(x_validate).float(), torch.from_numpy(y_validate).float())\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test).float())\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the ConvLSTM model\n",
    "input_dim = 4\n",
    "hidden_dim = 40  # Use a list for multiple layers\n",
    "kernel_size = (3, 3)  # Use a list for multiple layers\n",
    "num_layers = 2\n",
    "physics_kernel_size = (3, 3)  # Add physics kernel size\n",
    "\n",
    "model = ConvLSTM(input_dim, hidden_dim, kernel_size, num_layers, physics_kernel_size,output_dim=1, batch_first=True)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(batch_x)\n",
    "        loss  = criterion(outputs, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            outputs, _ = model(batch_x)\n",
    "            loss  = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs, _ = model(batch_x)\n",
    "        loss  = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item() * batch_x.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import os\n",
    "\n",
    "def animate2(x1, x2,i_plt ,output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    vmax = np.max(np.append(x1, x2))\n",
    "    vmin = np.min(np.append(x1, x2))\n",
    "\n",
    "    frames = min(x1.shape[2], x2.shape[2])  # Get the minimum number of frames\n",
    "\n",
    "    for i in range(frames):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(18, 5))  # Adjust the figure size as needed\n",
    "\n",
    "        im1 = ax[0].imshow(x1[:, :, i], vmin=vmin, vmax=vmax)\n",
    "        im2 = ax[1].imshow(x2[:, :, i], vmin=vmin, vmax=vmax)\n",
    "\n",
    "        diff = x1[:, :, i] - x2[:, :, i]\n",
    "        vmin_diff, vmax_diff = np.min(diff), np.max(diff)\n",
    "        norm = mcolors.Normalize(vmin=vmin_diff, vmax=vmax_diff)\n",
    "        im3 = ax[2].imshow(diff, cmap='viridis', norm=norm)\n",
    "\n",
    "        fig.subplots_adjust(right=0.8)\n",
    "        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "        fig.colorbar(im3, cax=cbar_ax)\n",
    "\n",
    "        # Add labels\n",
    "        ax[0].set_title('Predicted')\n",
    "        ax[1].set_title('True')\n",
    "        ax[2].set_title('Difference')\n",
    "\n",
    "        # Remove axis ticks\n",
    "        for a in ax:\n",
    "            a.set_xticks([])\n",
    "            a.set_yticks([])\n",
    "\n",
    "        # Calculate MAE and RMSE for the current frame\n",
    "        mae = np.mean(np.abs(diff))\n",
    "        rmse = np.sqrt(np.mean(diff ** 2))\n",
    "\n",
    "        # Add MAE and RMSE to the plot title\n",
    "        fig.suptitle(f'ConvLSTM | i = {i_plt} Predicted Frame - MAE: {mae:.4f}, RMSE: {rmse:.4f}', fontsize=12)\n",
    "\n",
    "        # Save the current frame as an image file\n",
    "        filename = f'ConvLSTMframe_{i_plt:04d}_{i+1:04d}.png'\n",
    "        filepath = os.path.join(output_folder, filename)\n",
    "        plt.savefig(filepath, bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Print MAE and RMSE for the current frame\n",
    "        print(f'Frame {i+1} - MAE: {mae:.4f}, RMSE: {rmse:.4f}')\n",
    "\n",
    "    print(f\"Images saved in the folder: {output_folder}\")\n",
    "\n",
    "\n",
    "# # Convert y_test to a PyTorch tensor\n",
    "# y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "# # Assuming your model is defined and trained\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "# y_test_pred = torch.empty_like(y_test_tensor)  # Initialize an empty tensor to store predictions\n",
    "\n",
    "# with torch.no_grad():  # Disable gradient computation for inference\n",
    "#     for i, (batch_x, batch_y) in enumerate(test_loader):\n",
    "#         batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "#         outputs = model(batch_x)  # Get the model outputs for this batch\n",
    "\n",
    "#         # Check if the output is a list\n",
    "#         if isinstance(outputs, list):\n",
    "#             outputs = outputs[-1]  # Assuming the last element is the final output\n",
    "\n",
    "#             # Check if the last element is a tensor\n",
    "#             if isinstance(outputs, torch.Tensor):\n",
    "#                 # Reshape the output to match the expected shape\n",
    "#                 if len(outputs.shape) == 5:\n",
    "#                     outputs = outputs.squeeze(2)  # Remove the singleton dimension\n",
    "\n",
    "#                 y_test_pred[i * test_loader.batch_size:(i + 1) * test_loader.batch_size] = outputs  # Store the predictions\n",
    "#             else:\n",
    "#                 print(f\"Warning: Unexpected output type for batch {i}: {type(outputs)}\")\n",
    "\n",
    "# # Now you have y_test_pred ready to use\n",
    "\n",
    "\n",
    "# # Assuming x_test, y_test, and y_test_pred are PyTorch tensors\n",
    "# n_test = x_test.shape[0]\n",
    "# i_plt = np.int32(np.random.sample() * n_test)\n",
    "# i_plt = 111\n",
    "# print('Showing test case i =', i_plt)\n",
    "\n",
    "# # Convert NumPy arrays to PyTorch tensors\n",
    "# x_test_tensor = torch.from_numpy(x_test).float()\n",
    "# y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "# Repeat the channel dimension of y_test_tensor to match x_test_tensor\n",
    "\n",
    "\n",
    "# print(f'shape of x test tensor: {x_test_tensor[i_plt].shape}')\n",
    "# print(f'shape of y test tensor: {y_test_tensor[i_plt].shape}')\n",
    "# print(f'shape of y test pred: {y_test_pred[i_plt].shape}')\n",
    "# for i_plt in range(10):\n",
    "#     true = np.append(x_test[i_plt,:,:,:], y_test[i_plt,:,:,:], axis=2)\n",
    "#     pred = np.append(x_test[i_plt,:,:,:], y_test_pred[i_plt,:,:,:], axis=2)\n",
    "\n",
    "#     anim = animate2(pred, true,'/home/sushen/PhysNet-RadarNowcast/2d-advection/pinn_plot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i_plt, (batch_x, batch_y) in enumerate(test_loader):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output, _ = model(batch_x)\n",
    "        \n",
    "        # Compute data loss\n",
    "        data_loss = criterion(output, batch_y)\n",
    "\n",
    "        # Combine losses\n",
    "        loss = data_loss\n",
    "\n",
    "        test_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        # Plot the results for the current batch\n",
    "        for j in range(batch_x.size(0)):\n",
    "            print(f'Batch {i_plt}, Sample {j}, batch_x.shape: {batch_x[j].shape}, batch_y.shape: {batch_y[j].shape}, output.shape: {output[j].shape}')\n",
    "            true = np.append(batch_x[j].cpu().numpy(), batch_y[j].cpu().numpy(), axis=2)\n",
    "            pred = np.append(batch_x[j].cpu().numpy(), output[j].cpu().numpy(), axis=2)\n",
    "            anim = animate2(pred, true, i_plt * batch_x.size(0) + j, '/home/sushen/PhysNet-RadarNowcast/2d-advection/pinn_plot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
