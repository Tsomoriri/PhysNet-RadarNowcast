{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super(Mlp, self).__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    #print(f'B={B}, H={H}, W={W},C={C}')\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super(WindowAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=2, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.red = nn.Linear(2 * dim, dim)\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x, hx=None):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        if hx is not None:\n",
    "            hx = self.norm1(hx)\n",
    "            x = torch.cat((x, hx), -1)\n",
    "            x = self.red(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        # FFN\n",
    "        x = x.view(B, H * W, C)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.\n",
    "        patch_size (int): Patch token size.\n",
    "        in_chans (int): Number of input image channels.\n",
    "        embed_dim (int): Number of linear projection output channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W,C = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchInflated(nn.Module):\n",
    "    r\"\"\" Tensor to Patch Inflating\n",
    "\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels.\n",
    "        embed_dim (int): Number of linear projection output channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, embed_dim, input_resolution, stride=2, padding=1, output_padding=1):\n",
    "        super(PatchInflated, self).__init__()\n",
    "\n",
    "        stride = to_2tuple(stride)\n",
    "        padding = to_2tuple(padding)\n",
    "        output_padding = to_2tuple(output_padding)\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        self.ConvT = nn.ConvTranspose2d(in_channels=embed_dim, out_channels=in_chans, kernel_size=(3, 3),\n",
    "                                        stride=stride, padding=padding, output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.ConvT(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlocks(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm):\n",
    "        super(SwinTransformerBlocks, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "    def forward(self, xt, hx):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            if index == 0:\n",
    "                x = layer(xt, hx)\n",
    "                outputs.append(x)\n",
    "\n",
    "            else:\n",
    "                if index % 2 == 0:\n",
    "                    x = layer(outputs[-1], xt)\n",
    "                    outputs.append(x)\n",
    "\n",
    "                if index % 2 == 1:\n",
    "                    x = layer(outputs[-1], None)\n",
    "                    outputs.append(x)\n",
    "\n",
    "        return outputs[-1]\n",
    "\n",
    "\n",
    "class SwinLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size, depth,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm):\n",
    "        super(SwinLSTMCell, self).__init__()\n",
    "\n",
    "        self.Swin = SwinTransformerBlocks(dim=dim, input_resolution=input_resolution, depth=depth,\n",
    "                                          num_heads=num_heads, window_size=window_size, mlp_ratio=mlp_ratio,\n",
    "                                          qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, attn_drop=attn_drop,\n",
    "                                          drop_path=drop_path, norm_layer=norm_layer)\n",
    "\n",
    "    def forward(self, xt, hidden_states):\n",
    "        if hidden_states is None:\n",
    "            B, L, C = xt.shape\n",
    "            hx = torch.zeros(B, L, C).to(xt.device)\n",
    "            cx = torch.zeros(B, L, C).to(xt.device)\n",
    "\n",
    "        else:\n",
    "            hx, cx = hidden_states\n",
    "\n",
    "        Ft = self.Swin(xt, hx)\n",
    "\n",
    "        gate = torch.sigmoid(Ft)\n",
    "        cell = torch.tanh(Ft)\n",
    "\n",
    "        cy = gate * (cx + cell)\n",
    "        hy = gate * torch.tanh(cy)\n",
    "        hx = hy\n",
    "        cx = cy\n",
    "\n",
    "        return hx, (hx, cx)\n",
    "\n",
    "\n",
    "class STconvert(nn.Module):\n",
    "    r\"\"\" STconvert\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size.\n",
    "        patch_size (int | tuple(int)): Patch size.\n",
    "        in_chans (int): Number of input image channels.\n",
    "        embed_dim (int): Patch embedding dimension.\n",
    "        depths (tuple(int)): Depth of Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim, depths, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super(STconvert, self).__init__()\n",
    "\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size,\n",
    "                                      in_chans=in_chans, embed_dim=embed_dim)\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "\n",
    "        self.PatchInflated = PatchInflated(in_chans=in_chans, embed_dim=embed_dim, input_resolution=patches_resolution)\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = SwinLSTMCell(dim=embed_dim,\n",
    "                                 input_resolution=(patches_resolution[0], patches_resolution[1]),\n",
    "                                 depth=depths[i_layer],\n",
    "                                 num_heads=num_heads[i_layer],\n",
    "                                 window_size=window_size,\n",
    "                                 mlp_ratio=self.mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                                 drop_path=drop_path_rate,\n",
    "                                 norm_layer=norm_layer)\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        hidden_states = []\n",
    "\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            x, hidden_state = layer(x, h[index])\n",
    "            hidden_states.append(hidden_state)\n",
    "\n",
    "        x = torch.sigmoid(self.PatchInflated(x))\n",
    "\n",
    "        return hidden_states, x\n",
    "\n",
    "\n",
    "class SwinLSTM(nn.Module):\n",
    "    r\"\"\" SwinLSTM\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size.\n",
    "        patch_size (int | tuple(int)): Patch size.\n",
    "        in_chans (int): Number of input image channels.\n",
    "        embed_dim (int): Patch embedding dimension.\n",
    "        depths (tuple(int)): Depth of Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size.\n",
    "        drop_rate (float): Dropout rate.\n",
    "        attn_drop_rate (float): Attention dropout rate.\n",
    "        drop_path_rate (float): Stochastic depth rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim, depths,\n",
    "                 num_heads, window_size, drop_rate, attn_drop_rate, drop_path_rate):\n",
    "        super(SwinLSTM, self).__init__()\n",
    "\n",
    "        self.ST = STconvert(img_size=img_size, patch_size=patch_size, in_chans=in_chans,\n",
    "                            embed_dim=embed_dim, depths=depths,\n",
    "                            num_heads=num_heads, window_size=window_size, drop_rate=drop_rate,\n",
    "                            attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate)\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "\n",
    "    def forward(self, input, states):\n",
    "        states_next, output = self.ST(input, states)\n",
    "        output = self.upconv(output)  # Upscale from [32, 4, 20, 20] to [32, 1, 40, 40]\n",
    "        output = output.permute(0, 2, 3, 1)\n",
    "        return output, states_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load radar data\n",
    "movies = np.load('/home/sushen/PhysNet-RadarNowcast/tests/rect_movie.npy')\n",
    "\n",
    "# Prepare inputs (x) and targets (y)\n",
    "x = movies[:, :, :, :4]  # (980, 40, 40, 4)\n",
    "y = movies[:, :, :, 4:5]  # (980, 40, 40, 1)\n",
    "\n",
    "# Split data into train, validate, test sets\n",
    "tvt = np.tile(['train', 'train', 'train', 'validate', 'test'], y.shape[0])[:y.shape[0]]\n",
    "x_train = x[np.where(tvt == 'train')]\n",
    "y_train = y[np.where(tvt == 'train')]\n",
    "x_validate = x[np.where(tvt == 'validate')]\n",
    "y_validate = y[np.where(tvt == 'validate')]\n",
    "x_test = x[np.where(tvt == 'test')]\n",
    "y_test = y[np.where(tvt == 'test')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99, Training Loss: 0.0520\n",
      "Epoch 0/99, Validation Loss: 0.0067\n",
      "Epoch 1/99, Training Loss: 0.0052\n",
      "Epoch 1/99, Validation Loss: 0.0034\n",
      "Epoch 2/99, Training Loss: 0.0031\n",
      "Epoch 2/99, Validation Loss: 0.0029\n",
      "Epoch 3/99, Training Loss: 0.0028\n",
      "Epoch 3/99, Validation Loss: 0.0029\n",
      "Epoch 4/99, Training Loss: 0.0027\n",
      "Epoch 4/99, Validation Loss: 0.0028\n",
      "Epoch 5/99, Training Loss: 0.0027\n",
      "Epoch 5/99, Validation Loss: 0.0028\n",
      "Epoch 6/99, Training Loss: 0.0027\n",
      "Epoch 6/99, Validation Loss: 0.0028\n",
      "Epoch 7/99, Training Loss: 0.0027\n",
      "Epoch 7/99, Validation Loss: 0.0028\n",
      "Epoch 8/99, Training Loss: 0.0027\n",
      "Epoch 8/99, Validation Loss: 0.0028\n",
      "Epoch 9/99, Training Loss: 0.0027\n",
      "Epoch 9/99, Validation Loss: 0.0028\n",
      "Epoch 10/99, Training Loss: 0.0027\n",
      "Epoch 10/99, Validation Loss: 0.0028\n",
      "Epoch 11/99, Training Loss: 0.0027\n",
      "Epoch 11/99, Validation Loss: 0.0028\n",
      "Epoch 12/99, Training Loss: 0.0027\n",
      "Epoch 12/99, Validation Loss: 0.0028\n",
      "Epoch 13/99, Training Loss: 0.0027\n",
      "Epoch 13/99, Validation Loss: 0.0028\n",
      "Epoch 14/99, Training Loss: 0.0027\n",
      "Epoch 14/99, Validation Loss: 0.0028\n",
      "Epoch 15/99, Training Loss: 0.0027\n",
      "Epoch 15/99, Validation Loss: 0.0028\n",
      "Epoch 16/99, Training Loss: 0.0027\n",
      "Epoch 16/99, Validation Loss: 0.0028\n",
      "Epoch 17/99, Training Loss: 0.0027\n",
      "Epoch 17/99, Validation Loss: 0.0028\n",
      "Epoch 18/99, Training Loss: 0.0027\n",
      "Epoch 18/99, Validation Loss: 0.0028\n",
      "Epoch 19/99, Training Loss: 0.0027\n",
      "Epoch 19/99, Validation Loss: 0.0028\n",
      "Epoch 20/99, Training Loss: 0.0027\n",
      "Epoch 20/99, Validation Loss: 0.0028\n",
      "Epoch 21/99, Training Loss: 0.0027\n",
      "Epoch 21/99, Validation Loss: 0.0028\n",
      "Epoch 22/99, Training Loss: 0.0027\n",
      "Epoch 22/99, Validation Loss: 0.0028\n",
      "Epoch 23/99, Training Loss: 0.0027\n",
      "Epoch 23/99, Validation Loss: 0.0028\n",
      "Epoch 24/99, Training Loss: 0.0027\n",
      "Epoch 24/99, Validation Loss: 0.0028\n",
      "Epoch 25/99, Training Loss: 0.0027\n",
      "Epoch 25/99, Validation Loss: 0.0028\n",
      "Epoch 26/99, Training Loss: 0.0027\n",
      "Epoch 26/99, Validation Loss: 0.0028\n",
      "Epoch 27/99, Training Loss: 0.0027\n",
      "Epoch 27/99, Validation Loss: 0.0028\n",
      "Epoch 28/99, Training Loss: 0.0027\n",
      "Epoch 28/99, Validation Loss: 0.0028\n",
      "Epoch 29/99, Training Loss: 0.0027\n",
      "Epoch 29/99, Validation Loss: 0.0028\n",
      "Epoch 30/99, Training Loss: 0.0027\n",
      "Epoch 30/99, Validation Loss: 0.0028\n",
      "Epoch 31/99, Training Loss: 0.0027\n",
      "Epoch 31/99, Validation Loss: 0.0028\n",
      "Epoch 32/99, Training Loss: 0.0027\n",
      "Epoch 32/99, Validation Loss: 0.0028\n",
      "Epoch 33/99, Training Loss: 0.0027\n",
      "Epoch 33/99, Validation Loss: 0.0028\n",
      "Epoch 34/99, Training Loss: 0.0027\n",
      "Epoch 34/99, Validation Loss: 0.0028\n",
      "Epoch 35/99, Training Loss: 0.0027\n",
      "Epoch 35/99, Validation Loss: 0.0028\n",
      "Epoch 36/99, Training Loss: 0.0027\n",
      "Epoch 36/99, Validation Loss: 0.0028\n",
      "Epoch 37/99, Training Loss: 0.0027\n",
      "Epoch 37/99, Validation Loss: 0.0028\n",
      "Epoch 38/99, Training Loss: 0.0027\n",
      "Epoch 38/99, Validation Loss: 0.0028\n",
      "Epoch 39/99, Training Loss: 0.0027\n",
      "Epoch 39/99, Validation Loss: 0.0028\n",
      "Epoch 40/99, Training Loss: 0.0027\n",
      "Epoch 40/99, Validation Loss: 0.0028\n",
      "Epoch 41/99, Training Loss: 0.0027\n",
      "Epoch 41/99, Validation Loss: 0.0028\n",
      "Epoch 42/99, Training Loss: 0.0027\n",
      "Epoch 42/99, Validation Loss: 0.0028\n",
      "Epoch 43/99, Training Loss: 0.0027\n",
      "Epoch 43/99, Validation Loss: 0.0028\n",
      "Epoch 44/99, Training Loss: 0.0027\n",
      "Epoch 44/99, Validation Loss: 0.0028\n",
      "Epoch 45/99, Training Loss: 0.0027\n",
      "Epoch 45/99, Validation Loss: 0.0028\n",
      "Epoch 46/99, Training Loss: 0.0027\n",
      "Epoch 46/99, Validation Loss: 0.0028\n",
      "Epoch 47/99, Training Loss: 0.0027\n",
      "Epoch 47/99, Validation Loss: 0.0028\n",
      "Epoch 48/99, Training Loss: 0.0027\n",
      "Epoch 48/99, Validation Loss: 0.0028\n",
      "Epoch 49/99, Training Loss: 0.0027\n",
      "Epoch 49/99, Validation Loss: 0.0028\n",
      "Epoch 50/99, Training Loss: 0.0027\n",
      "Epoch 50/99, Validation Loss: 0.0028\n",
      "Epoch 51/99, Training Loss: 0.0027\n",
      "Epoch 51/99, Validation Loss: 0.0028\n",
      "Epoch 52/99, Training Loss: 0.0027\n",
      "Epoch 52/99, Validation Loss: 0.0028\n",
      "Epoch 53/99, Training Loss: 0.0027\n",
      "Epoch 53/99, Validation Loss: 0.0028\n",
      "Epoch 54/99, Training Loss: 0.0027\n",
      "Epoch 54/99, Validation Loss: 0.0028\n",
      "Epoch 55/99, Training Loss: 0.0027\n",
      "Epoch 55/99, Validation Loss: 0.0028\n",
      "Epoch 56/99, Training Loss: 0.0027\n",
      "Epoch 56/99, Validation Loss: 0.0028\n",
      "Epoch 57/99, Training Loss: 0.0027\n",
      "Epoch 57/99, Validation Loss: 0.0028\n",
      "Epoch 58/99, Training Loss: 0.0027\n",
      "Epoch 58/99, Validation Loss: 0.0028\n",
      "Epoch 59/99, Training Loss: 0.0027\n",
      "Epoch 59/99, Validation Loss: 0.0028\n",
      "Epoch 60/99, Training Loss: 0.0027\n",
      "Epoch 60/99, Validation Loss: 0.0028\n",
      "Epoch 61/99, Training Loss: 0.0027\n",
      "Epoch 61/99, Validation Loss: 0.0028\n",
      "Epoch 62/99, Training Loss: 0.0027\n",
      "Epoch 62/99, Validation Loss: 0.0028\n",
      "Epoch 63/99, Training Loss: 0.0027\n",
      "Epoch 63/99, Validation Loss: 0.0028\n",
      "Epoch 64/99, Training Loss: 0.0027\n",
      "Epoch 64/99, Validation Loss: 0.0028\n",
      "Epoch 65/99, Training Loss: 0.0027\n",
      "Epoch 65/99, Validation Loss: 0.0028\n",
      "Epoch 66/99, Training Loss: 0.0027\n",
      "Epoch 66/99, Validation Loss: 0.0028\n",
      "Epoch 67/99, Training Loss: 0.0027\n",
      "Epoch 67/99, Validation Loss: 0.0028\n",
      "Epoch 68/99, Training Loss: 0.0027\n",
      "Epoch 68/99, Validation Loss: 0.0028\n",
      "Epoch 69/99, Training Loss: 0.0027\n",
      "Epoch 69/99, Validation Loss: 0.0028\n",
      "Epoch 70/99, Training Loss: 0.0027\n",
      "Epoch 70/99, Validation Loss: 0.0028\n",
      "Epoch 71/99, Training Loss: 0.0027\n",
      "Epoch 71/99, Validation Loss: 0.0028\n",
      "Epoch 72/99, Training Loss: 0.0027\n",
      "Epoch 72/99, Validation Loss: 0.0028\n",
      "Epoch 73/99, Training Loss: 0.0027\n",
      "Epoch 73/99, Validation Loss: 0.0028\n",
      "Epoch 74/99, Training Loss: 0.0027\n",
      "Epoch 74/99, Validation Loss: 0.0028\n",
      "Epoch 75/99, Training Loss: 0.0027\n",
      "Epoch 75/99, Validation Loss: 0.0028\n",
      "Epoch 76/99, Training Loss: 0.0027\n",
      "Epoch 76/99, Validation Loss: 0.0028\n",
      "Epoch 77/99, Training Loss: 0.0027\n",
      "Epoch 77/99, Validation Loss: 0.0028\n",
      "Epoch 78/99, Training Loss: 0.0027\n",
      "Epoch 78/99, Validation Loss: 0.0028\n",
      "Epoch 79/99, Training Loss: 0.0027\n",
      "Epoch 79/99, Validation Loss: 0.0028\n",
      "Epoch 80/99, Training Loss: 0.0027\n",
      "Epoch 80/99, Validation Loss: 0.0028\n",
      "Epoch 81/99, Training Loss: 0.0027\n",
      "Epoch 81/99, Validation Loss: 0.0028\n",
      "Epoch 82/99, Training Loss: 0.0027\n",
      "Epoch 82/99, Validation Loss: 0.0028\n",
      "Epoch 83/99, Training Loss: 0.0027\n",
      "Epoch 83/99, Validation Loss: 0.0028\n",
      "Epoch 84/99, Training Loss: 0.0027\n",
      "Epoch 84/99, Validation Loss: 0.0028\n",
      "Epoch 85/99, Training Loss: 0.0027\n",
      "Epoch 85/99, Validation Loss: 0.0028\n",
      "Epoch 86/99, Training Loss: 0.0027\n",
      "Epoch 86/99, Validation Loss: 0.0028\n",
      "Epoch 87/99, Training Loss: 0.0027\n",
      "Epoch 87/99, Validation Loss: 0.0028\n",
      "Epoch 88/99, Training Loss: 0.0027\n",
      "Epoch 88/99, Validation Loss: 0.0028\n",
      "Epoch 89/99, Training Loss: 0.0027\n",
      "Epoch 89/99, Validation Loss: 0.0028\n",
      "Epoch 90/99, Training Loss: 0.0027\n",
      "Epoch 90/99, Validation Loss: 0.0028\n",
      "Epoch 91/99, Training Loss: 0.0027\n",
      "Epoch 91/99, Validation Loss: 0.0028\n",
      "Epoch 92/99, Training Loss: 0.0027\n",
      "Epoch 92/99, Validation Loss: 0.0028\n",
      "Epoch 93/99, Training Loss: 0.0027\n",
      "Epoch 93/99, Validation Loss: 0.0028\n",
      "Epoch 94/99, Training Loss: 0.0027\n",
      "Epoch 94/99, Validation Loss: 0.0028\n",
      "Epoch 95/99, Training Loss: 0.0027\n",
      "Epoch 95/99, Validation Loss: 0.0028\n",
      "Epoch 96/99, Training Loss: 0.0027\n",
      "Epoch 96/99, Validation Loss: 0.0028\n",
      "Epoch 97/99, Training Loss: 0.0027\n",
      "Epoch 97/99, Validation Loss: 0.0028\n",
      "Epoch 98/99, Training Loss: 0.0027\n",
      "Epoch 98/99, Validation Loss: 0.0028\n",
      "Epoch 99/99, Training Loss: 0.0027\n",
      "Epoch 99/99, Validation Loss: 0.0028\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(x_validate).float(), torch.from_numpy(y_validate).float())\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test).float())\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SwinLSTM(\n",
    "    patch_size=4,\n",
    "    img_size=[40, 40],\n",
    "    in_chans=4,\n",
    "    embed_dim=4,\n",
    "    depths=[2],\n",
    "    num_heads=[1,1],\n",
    "    window_size=10,\n",
    "    drop_rate=0.,\n",
    "    drop_path_rate=0.,\n",
    "    attn_drop_rate=0.,\n",
    "    \n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # Initialize hidden states\n",
    "            states = [None] * 2\n",
    "\n",
    "            outputs, states = model(inputs,states)\n",
    "            #print(f\"Output shape: {outputs.shape}, Target shape: {targets.shape}\")\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Training Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                 # Initialize hidden states\n",
    "                states = [None] * 2\n",
    "                outputs, states = model(inputs,states)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Validation Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch\n",
    "\n",
    "def animate_comparison(model, data_loader, output_folder):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract a single batch for visualization\n",
    "    inputs, targets = next(iter(data_loader))\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = inputs.to(next(model.parameters()).device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        # Initialize hidden states\n",
    "        states = [None] * 1\n",
    "        outputs,states = model(inputs,states)\n",
    "    \n",
    "    # Assuming outputs and targets are on GPU, move them to CPU and convert to numpy\n",
    "    outputs = outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "    \n",
    "    # Prepare figure for animation\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    def update(i):\n",
    "        # Clear previous content\n",
    "        ax[0].cla()\n",
    "        ax[1].cla()\n",
    "        \n",
    "        # Update content for frame i\n",
    "        ax[0].imshow(outputs[i].squeeze(), cmap='gray')\n",
    "        ax[0].set_title('Output')\n",
    "        ax[1].imshow(targets[i].squeeze(), cmap='gray')\n",
    "        ax[1].set_title('Target')\n",
    "    \n",
    "    # Create animation\n",
    "    anim = FuncAnimation(fig, update, frames=len(outputs), interval=200)\n",
    "    \n",
    "    # Save animation\n",
    "    anim.save(f'{output_folder}/comparison_animation.gif', writer='imagemagick')\n",
    "\n",
    "# Example usage\n",
    "animate_comparison(model, test_loader, '/home/sushen/PhysNet-RadarNowcast/images/vit')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
