{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T09:20:48.062971Z",
     "start_time": "2024-06-24T09:20:48.020498Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super(Mlp, self).__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super(WindowAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=2, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.red = nn.Linear(2 * dim, dim)\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x, hx=None):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        if hx is not None:\n",
    "            hx = self.norm1(hx)\n",
    "            x = torch.cat((x, hx), -1)\n",
    "            x = self.red(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        # FFN\n",
    "        x = x.view(B, H * W, C)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.\n",
    "        patch_size (int): Patch token size.\n",
    "        in_chans (int): Number of input image channels.\n",
    "        embed_dim (int): Number of linear projection output channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "       # print('x shape patchembed forward',x.shape)\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchInflated(nn.Module):\n",
    "    r\"\"\" Tensor to Patch Inflating\n",
    "\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels.\n",
    "        embed_dim (int): Number of linear projection output channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, embed_dim, input_resolution, stride=2, padding=1, output_padding=1):\n",
    "        super(PatchInflated, self).__init__()\n",
    "\n",
    "        stride = to_2tuple(stride)\n",
    "        padding = to_2tuple(padding)\n",
    "        output_padding = to_2tuple(output_padding)\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        self.ConvT = nn.ConvTranspose2d(in_channels=embed_dim, out_channels=in_chans, kernel_size=(3, 3),\n",
    "                                        stride=stride, padding=padding, output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.ConvT(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlocks(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm):\n",
    "        super(SwinTransformerBlocks, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "    def forward(self, xt, hx):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            if index == 0:\n",
    "                x = layer(xt, hx)\n",
    "                outputs.append(x)\n",
    "\n",
    "            else:\n",
    "                if index % 2 == 0:\n",
    "                    x = layer(outputs[-1], xt)\n",
    "                    outputs.append(x)\n",
    "\n",
    "                if index % 2 == 1:\n",
    "                    x = layer(outputs[-1], None)\n",
    "                    outputs.append(x)\n",
    "\n",
    "        return outputs[-1]\n",
    "\n",
    "\n",
    "class SwinLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size, depth,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm):\n",
    "        super(SwinLSTMCell, self).__init__()\n",
    "\n",
    "        self.Swin = SwinTransformerBlocks(dim=dim, input_resolution=input_resolution, depth=depth,\n",
    "                                          num_heads=num_heads, window_size=window_size, mlp_ratio=mlp_ratio,\n",
    "                                          qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, attn_drop=attn_drop,\n",
    "                                          drop_path=drop_path, norm_layer=norm_layer)\n",
    "\n",
    "    def forward(self, xt, hidden_states):\n",
    "        if hidden_states is None:\n",
    "            B, L, C = xt.shape\n",
    "            hx = torch.zeros(B, L, C).to(xt.device)\n",
    "            cx = torch.zeros(B, L, C).to(xt.device)\n",
    "\n",
    "        else:\n",
    "            hx, cx = hidden_states\n",
    "\n",
    "        Ft = self.Swin(xt, hx)\n",
    "\n",
    "        gate = torch.sigmoid(Ft)\n",
    "        cell = torch.tanh(Ft)\n",
    "\n",
    "        cy = gate * (cx + cell)\n",
    "        hy = gate * torch.tanh(cy)\n",
    "        hx = hy\n",
    "        cx = cy\n",
    "\n",
    "        return hx, (hx, cx)\n",
    "\n",
    "\n",
    "class STconvert(nn.Module):\n",
    "    r\"\"\" STconvert\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size.\n",
    "        patch_size (int | tuple(int)): Patch size.\n",
    "        in_chans (int): Number of input image channels.\n",
    "        embed_dim (int): Patch embedding dimension.\n",
    "        depths (tuple(int)): Depth of Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim, depths, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super(STconvert, self).__init__()\n",
    "\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size,\n",
    "                                      in_chans=in_chans, embed_dim=embed_dim)\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "\n",
    "        self.PatchInflated = PatchInflated(in_chans=in_chans, embed_dim=embed_dim, input_resolution=patches_resolution)\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = SwinLSTMCell(dim=embed_dim,\n",
    "                                 input_resolution=(patches_resolution[0], patches_resolution[1]),\n",
    "                                 depth=depths[i_layer],\n",
    "                                 num_heads=num_heads[i_layer],\n",
    "                                 window_size=window_size,\n",
    "                                 mlp_ratio=self.mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                                 drop_path=drop_path_rate,\n",
    "                                 norm_layer=norm_layer)\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "       # print('stconvert x shape',x.shape)\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        hidden_states = []\n",
    "\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            x, hidden_state = layer(x, h[index])\n",
    "            hidden_states.append(hidden_state)\n",
    "       # print('x shape before convert',x.shape)\n",
    "        x = torch.sigmoid(self.PatchInflated(x))\n",
    "       # print('x shape st convert',x.shape)\n",
    "        return hidden_states, x\n",
    "\n",
    "\n",
    "class SwinLSTM(nn.Module):\n",
    "    r\"\"\" SwinLSTM\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size.\n",
    "        patch_size (int | tuple(int)): Patch size.\n",
    "        in_chans (int): Number of input image channels.\n",
    "        embed_dim (int): Patch embedding dimension.\n",
    "        depths (tuple(int)): Depth of Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size.\n",
    "        drop_rate (float): Dropout rate.\n",
    "        attn_drop_rate (float): Attention dropout rate.\n",
    "        drop_path_rate (float): Stochastic depth rate.\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "    def __init__(self, img_size=40, patch_size=4, in_chans=4,out_chans=1, embed_dim=128, depths=[2, 2],\n",
    "                 num_heads=[4, 8], window_size=2, drop_rate=0, attn_drop_rate=0.1, drop_path_rate=0.1):\n",
    "        super(SwinLSTM, self).__init__()\n",
    "\n",
    "        self.ST = STconvert(img_size=img_size, patch_size=patch_size, in_chans=in_chans,\n",
    "                            embed_dim=embed_dim, depths=depths,\n",
    "                            num_heads=num_heads, window_size=window_size, drop_rate=drop_rate,\n",
    "                            attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate)\n",
    "         # Add a final convolution layer to reduce channels from embed_dim to out_chans\n",
    "        self.final_conv = nn.Conv2d(in_channels=4, out_channels=1, kernel_size=1, stride=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bicubic', align_corners=True)\n",
    "\n",
    "    def forward(self, input, states):\n",
    "        # input shape: (batch_size, 40, 40, 4)\n",
    "        states_next, output = self.ST(input, states)\n",
    "       # print('x shape st convert', output.shape)  # Should be [32, 4, 20, 20]\n",
    "        \n",
    "        output = self.final_conv(output)  # Should maintain [32, 4, 20, 20]\n",
    "        output = self.upsample(output)  # Will upsample to [32, 4, 40, 40]\n",
    "        \n",
    "        # If you need to reduce to a single channel:\n",
    "        output = output.mean(dim=1, keepdim=True)  # Will be [32, 1, 40, 40]\n",
    "        \n",
    "        return output.squeeze(1), states_next  # Will be [32, 40, 40]"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T07:48:28.310473Z",
     "start_time": "2024-06-24T07:48:28.177993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load radar data\n",
    "movies = np.load('/home/sushen/PhysNet-RadarNowcast/tests/rect_movie.npy')import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch\n",
    "\n",
    "def animate_comparison(model, data_loader, output_folder):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract a single batch for visualization\n",
    "    inputs, targets = next(iter(data_loader))\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = inputs.to(next(model.parameters()).device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        # Initialize hidden states\n",
    "        states = [None] * 1\n",
    "        outputs,states = model(inputs,states)\n",
    "    \n",
    "    # Assuming outputs and targets are on GPU, move them to CPU and convert to numpy\n",
    "    outputs = outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "    \n",
    "    # Prepare figure for animation\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    def update(i):\n",
    "        # Clear previous content\n",
    "        ax[0].cla()\n",
    "        ax[1].cla()\n",
    "        \n",
    "        # Update content for frame i\n",
    "        ax[0].imshow(outputs[i].squeeze(), cmap='gray')\n",
    "        ax[0].set_title('Output')\n",
    "        ax[1].imshow(targets[i].squeeze(), cmap='gray')\n",
    "        ax[1].set_title('Target')\n",
    "    \n",
    "    # Create animation\n",
    "    anim = FuncAnimation(fig, update, frames=len(outputs), interval=200)\n",
    "    \n",
    "    # Save animation\n",
    "    anim.save(f'{output_folder}/comparison_animation.gif', writer='imagemagick')\n",
    "\n",
    "# Example usage\n",
    "animate_comparison(model, test_loader, '/home/sushen/PhysNet-RadarNowcast/images/vit')\n",
    "\n",
    "# Prepare inputs (x) and targets (y)\n",
    "x = movies[:, :, :, :4]  # (980, 40, 40, 4)\n",
    "y = movies[:, :, :, 4:5]  # (980, 40, 40, 1)\n",
    "\n",
    "# Split data into train, validate, test sets\n",
    "tvt = np.tile(['train', 'train', 'train', 'validate', 'test'], y.shape[0])[:y.shape[0]]\n",
    "x_train = x[np.where(tvt == 'train')]\n",
    "y_train = y[np.where(tvt == 'train')]\n",
    "x_validate = x[np.where(tvt == 'validate')]\n",
    "y_validate = y[np.where(tvt == 'validate')]\n",
    "x_test = x[np.where(tvt == 'test')]\n",
    "y_test = y[np.where(tvt == 'test')]"
   ],
   "id": "68401c3a266a06a3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T07:49:42.153973Z",
     "start_time": "2024-06-24T07:49:42.139976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx].permute(2, 0, 1), self.y[idx].squeeze(2)  # (4, 40, 40), (40, 40)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RadarDataset(x_train, y_train)\n",
    "val_dataset = RadarDataset(x_validate, y_validate)\n",
    "test_dataset = RadarDataset(x_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "f0f9c803e222975e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:21:11.132655Z",
     "start_time": "2024-06-24T09:20:51.860917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "       # print('inputs shape',inputs.shape)  \n",
    "        states = [None] * len(model.ST.layers)\n",
    "        outputs, _ = model(inputs, states)\n",
    "        #print(outputs.shape, targets.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            states = [None] * len(model.ST.layers)\n",
    "            outputs, _ = model(inputs, states)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "# Set up training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize the model\n",
    "model = SwinLSTM().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = test(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = test(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ],
   "id": "2acd2682a538a2a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 51.40it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 171.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.0273, Val Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.85it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 146.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.0048, Val Loss: 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.28it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 187.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 0.0031, Val Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.13it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 172.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 0.0028, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.86it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 179.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.51it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 170.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 60.49it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 190.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 54.88it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 163.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 56.24it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 190.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 56.22it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 175.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 58.70it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 184.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.93it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 196.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 58.00it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 195.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 55.48it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 157.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 54.77it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 177.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 54.84it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 178.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 58.56it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 178.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.95it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 145.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 53.98it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 166.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.14it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 182.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 56.61it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 183.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 58.20it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 110.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 54.61it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 196.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.34it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 179.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 53.80it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 145.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 53.16it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 161.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 47.05it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 134.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 51.22it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 167.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 47.02it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 180.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 56.88it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 178.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.92it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 171.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 56.74it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 170.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 53.76it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 137.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 56.65it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 177.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 55.86it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 178.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 54.96it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 130.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.16it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 178.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.41it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 181.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 55.76it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 174.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.26it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 170.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.28it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 168.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 56.29it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 189.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Train Loss: 0.0028, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 53.94it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 192.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 59.27it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 190.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 58.81it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 190.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 58.11it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 178.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 59.49it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 170.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 57.68it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 190.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 54.92it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 177.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 58.52it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 176.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0027, Val Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 192.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a716ea95c559ea73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:21:22.081210Z",
     "start_time": "2024-06-24T09:21:14.597117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm\n",
    "\n",
    "def animate_comparison(model, data_loader, output_folder, device):\n",
    "    model.eval()\n",
    "    \n",
    "    inputs, targets = next(iter(data_loader))\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        states = [None] * len(model.ST.layers)\n",
    "        outputs, _ = model(inputs, states)\n",
    "    \n",
    "    outputs = outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    def update(i):\n",
    "        ax[0].cla()\n",
    "        ax[1].cla()\n",
    "        \n",
    "        ax[0].imshow(outputs[i].squeeze(), cmap='gray')\n",
    "        ax[0].set_title('Output')\n",
    "        ax[1].imshow(targets[i].squeeze(), cmap='gray')\n",
    "        ax[1].set_title('Target')\n",
    "    \n",
    "    anim = FuncAnimation(fig, update, frames=len(outputs), interval=200)\n",
    "    anim.save(f'{output_folder}/swinLSTM2_comparison_animation.gif', writer='imagemagick')\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SwinLSTM().to(device)  # Assuming SwinLSTM is defined elsewhere\n",
    "animate_comparison(model, test_loader, '/home/sushen/PhysNet-RadarNowcast/images/vit', device)"
   ],
   "id": "953c35c375df6af3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzYAAAGgCAYAAAB47/I2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFd0lEQVR4nO3de3TU9Z3/8dcQkkkCSSBAbgIxlauAKMTlpnJpocbWWlGKWi3YXU9b1LMc29qCWuOelihnl2oXpZd1EffUYnvqpa2K0iOXKrJcFKEgChIgSEIIlyTkMiHJ9/dHf8waIfP+hEzIfM3zcc6cY+bz5jPv+cz3Ox/f+U7mHfA8zxMAAAAA+Fi3zk4AAAAAANqLwgYAAACA71HYAAAAAPA9ChsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4HsUNgAAAAB8j8IGXc7GjRs1a9YsZWdnKyEhQVlZWbr55pv1zjvvnPecixYt0ksvvRS9JCM4fPiwCgsLtW3btgvyeACAjhMIBJxua9eu7exUW9i1a5cKCwu1f//+zk4FCKOwQZfyn//5n5o0aZIOHTqkxYsX669//av+/d//XZ988omuuuoqLV269LzmvdCFzSOPPEJhAwCfA++8806L23XXXaekpKSz7h8zZkxnp9rCrl279Mgjj1DYIKZ07+wEgAvl7bff1vz583XdddfpxRdfVPfu/3f433LLLbrxxhv1r//6r7riiis0adKkTswUANBVjB8/vsXP/fr1U7du3c66/3zV1tYqOTk5KnMBsY4rNugyioqKFAgEtGzZshZFjSR1795dTz31lAKBgB599FFJ0ty5c3XxxRefNU9hYaECgUD450AgoJqaGq1YsSL8kYEpU6ZIkp555hkFAgGtXr1ad955p9LT09WjRw9df/312rdvX4t5L774Ys2dO/esx5syZUp4vrVr1+rKK6+UJN15553hxyssLDy/RQEAxLwnn3xS11xzjTIyMtSjRw+NGjVKixcv1unTp1vETZkyRSNHjtT69es1ceJEJScn69vf/rYk6dChQ7r55puVkpKiXr166Zvf/KY2b96sQCCgZ555psU8W7Zs0de+9jWlp6crMTFRV1xxhX7/+9+Hx5955hnNmjVLkjR16tTwXvTZeYALjSs26BKampq0Zs0a5efnq3///ueMGTBggMaOHas333xTTU1NznO/8847mjZtmqZOnaqHHnpIkpSamtoi5p//+Z81ffp0PffccyopKdGDDz6oKVOmaPv27erVq5fzY40ZM0bLly/XnXfeqQcffFBf+cpXJKnV5wQA8L+PP/5Yt912m/Ly8pSQkKD3339fP/vZz7R7927993//d4vY0tJS3X777br//vu1aNEidevWTTU1NZo6daqOHz+uxx57TIMGDdKqVas0e/bssx5rzZo1uvbaazVu3Dj98pe/VFpamlauXKnZs2ertrZWc+fO1Ve+8hUtWrRICxcu1JNPPhn+mNwll1xyQdYDaA2FDbqEiooK1dbWKi8vL2JcXl6eNm3apGPHjjnPPX78eHXr1k39+vVr9aMD+fn5evrpp8M/jxgxQpMmTdKTTz6pBx54wPmxUlNTNXLkSEn/2ECi9VEFAEDsWrJkSfi/m5ubdfXVV6tPnz6688479R//8R/q3bt3ePz48eP6wx/+oGnTpoXve+qpp7R371699tpruvbaayVJM2bMUG1trX71q1+1eKx58+ZpxIgRevPNN8Ofbvjyl7+siooKLVy4UN/61rfUr18/DR48WJJ06aWXshchZvBRNOBTPM+TpBYfNYuGb37zmy1+njhxonJzc7VmzZqoPg4A4PPnvffe09e+9jX16dNHcXFxio+P17e+9S01NTXpo48+ahHbu3fvFkWNJK1bt04pKSnhouaMW2+9tcXPe/fu1e7du8N7VmNjY/h23XXXqbS0VB9++GEHPEMgOrhigy6hb9++Sk5OVnFxccS4/fv3Kzk5Wenp6VF9/KysrHPe15YrQwCArufgwYO6+uqrNXToUD3xxBO6+OKLlZiYqE2bNunuu+9WXV1di/js7Oyz5jh27JgyMzPPuv+z9x05ckSS9IMf/EA/+MEPzplPRUXF+T4VoMNR2KBLiIuL09SpU7Vq1SodOnTonH+TcujQIW3dulUFBQWKi4tTYmKiQqHQWXHn86ZeVlZ2zvsGDRoU/jnS4/Xt27fNjwkA8L+XXnpJNTU1euGFF5Sbmxu+v7Wv/D/XJw769OmjTZs2nXX/Z/emM3vNggULNHPmzHPOP3ToUNfUgQuOj6Khy1iwYIE8z9O8efPO+nKApqYmfe9735PneVqwYIGkf3xLWXl5efg3WJLU0NCg119//ay5g8HgWb81+7Tf/va3LX7esGGDDhw4EP62szOPt3379hZxH3300VmX/YPBoCRFfDwAwOfDmULlzHu/9I+PTf/mN79xnmPy5Mmqrq7Wa6+91uL+lStXtvh56NChGjx4sN5//33l5+ef85aSktIiH/YixBKu2KDLmDRpkh5//HHNnz9fV111le655x4NHDhQBw8e1JNPPqn//d//1eOPP66JEydKkmbPnq2f/OQnuuWWW/TDH/5Q9fX1+sUvfnHOb0wbNWqU1q5dqz//+c/Kzs5WSkpKi99qbdmyRf/yL/+iWbNmqaSkRA888IAuuugizZs3Lxxzxx136Pbbb9e8efN000036cCBA1q8eLH69evX4rEuueQSJSUl6be//a2GDx+unj17KicnRzk5OR20cgCAzjJ9+nQlJCTo1ltv1f3336/6+notW7ZMJ06ccJ5jzpw5+vnPf67bb79dP/3pTzVo0CC99tpr4V/Udev2f7/n/tWvfqWCggJ9+ctf1ty5c3XRRRfp+PHj+uCDD/Tuu+/qD3/4gySFv8jm17/+tVJSUpSYmKi8vDz16dMnis8eaCMP6GLeeecd7+abb/YyMzO97t27exkZGd7MmTO9DRs2nBX76quvepdffrmXlJTkfeELX/CWLl3qPfzww95nT51t27Z5kyZN8pKTkz1J3uTJkz3P87zly5d7krw33njDu+OOO7xevXp5SUlJ3nXXXeft2bOnxRzNzc3e4sWLvS984QteYmKil5+f77355pve5MmTw/Od8bvf/c4bNmyYFx8f70nyHn744WguEQCgk8yZM8fr0aNHi/v+/Oc/e6NHj/YSExO9iy66yPvhD3/ovfbaa54kb82aNeG4yZMneyNGjDjnvAcPHvRmzpzp9ezZ00tJSfFuuukm79VXX/UkeS+//HKL2Pfff9/7xje+4WVkZHjx8fFeVlaWN23aNO+Xv/xli7jHH3/cy8vL8+Li4jxJ3vLly6OyBsD5Cnje//8aKABR98wzz+jOO+/U5s2blZ+f39npAAAQtmjRIj344IM6ePAg/dDwucBH0QAAAD7nli5dKkkaNmyYTp8+rTfffFO/+MUvdPvtt1PU4HODwgYAAOBzLjk5WT//+c+1f/9+hUIhDRw4UD/60Y/04IMPdnZqQNTwUTQAAAAAvsfXPQMAAADwPQobAAAAAL5HYQMAAADA92LuywOam5t1+PBhpaSkhLvtAgAuDM/zVF1drZycnBZN+7o69iYA6Bxt2pc6qkHOk08+6V188cVeMBj0xowZ461fv97p35WUlHiSuHHjxo1bJ95KSko6anvoNOe7L3keexM3bty4dfbNZV/qkCs2zz//vObPn6+nnnpKkyZN0q9+9SsVFBRo165dGjhwYMR/m5KSIkn66le/qvj4+FbjamtrzTxCoVDE8WAwaM6RnJzc7seR3PJNSEgwY3r06BFxvKGhISq5xMXFmTE9e/Y0YxobG9udj8tvja11kSTP4QsAT506ZcZYv611ycXlN741NTVmTHNzsxnjcgx37x75rcBlXZqamswYl7WxcolWPklJSeYcLu8RLq+Ty3np8rxTU1Mjjvfr18+cIz09PeJ4KBTSkiVLwu/Fnxft2Zckfe7WAwD8xuV9uEO+7nncuHEaM2aMli1bFr5v+PDh+vrXv66ioqKI/7aqqkppaWm68cYb213Y1NfXRxxPTEw054hWYePyPz8u/xMVjcLGJReX/8mKVmFj5eNS2Ljk4nKoV1dXmzFWUeKSi0th4/I/7y6FTTSKCZdcXF5rl7WJVmFj5eNybkersHF5j3B53mlpaRHHMzMzzTn69OkTcby+vl5FRUWqrKw0Cyk/ac++JP3f3gQA6Bwu+1LUP0Dd0NCgrVu3asaMGS3unzFjhjZs2HBWfCgUUlVVVYsbAADR0tZ9SWJvAgA/inphU1FRoaamprN+c5iZmamysrKz4ouKipSWlha+DRgwINopAQC6sLbuSxJ7EwD4UYd95c1nP2rjed45P36zYMECVVZWhm8lJSUdlRIAoAtz3Zck9iYA8KOof3lA3759FRcXd9ZvwcrLy8/5+e9gMOj0OXYAAM5HW/clib0JAPwo6ldsEhISNHbsWK1evbrF/atXr9bEiROj/XAAAETEvgQAXUOHfN3zfffdpzvuuEP5+fmaMGGCfv3rX+vgwYP67ne/6zxHampqxK8/HjlypDmH9dWn5eXl5hyHDh0yY/r27WvGuHw++9ixY+3Ox+VbjFy+2tTlm572799vxrh8pW5eXl7EcZdvlNq3b58ZE+lb9lxzkexvVysuLm73HK65uHyTlks+1hpfeeWV5hwur/WBAwfMGJdvPLviiivMGOsbrA4ePGjOceLECTNm1KhRZozLe0RFRYUZY62fy8eldu/eHXHc5dvt/Cga+xIAILZ1SGEze/ZsHTt2TP/2b/+m0tJSjRw5Uq+++qpyc3M74uEAAIiIfQkAPv86pLCRpHnz5mnevHkdNT0AAG3CvgQAn28d9q1oAAAAAHChUNgAAAAA8D0KGwAAAAC+R2EDAAAAwPcobAAAAAD4HoUNAAAAAN/rsK97bq/k5GQFg8FWx4cNG2bOMXz48IjjO3fuNOc4efKkGePSB2HChAlmzMcff9zufPr372/OMWnSJDOmrKzMjDl+/LgZ06dPHzNm/PjxEcerqqqikktycrIZk5+fb8ZYzTVd8m1qajJjxo4da8YkJiaaMS7NVq2cR48ebc7h8lo3NDSYMS7HnktTTJemuBaXfF3ei1xi9u7da8ZYTYVdmg5bjXWbm5vNOQAAiEVcsQEAAADgexQ2AAAAAHyPwgYAAACA71HYAAAAAPA9ChsAAAAAvkdhAwAAAMD3KGwAAAAA+F7M9rGpra1VY2Njq+O7du0y5zhy5EjE8YqKCnOOU6dOmTFWXwhJOn36tBlz4sQJM6a6ujrieElJiTnHW2+9Zca4PG+Xfi0uz/vtt9+OOO7SS8Sl35BLP5dNmzaZMVYfG5fjyppDkjZv3mzGdO9un8LWeSDZa/zuu++acyQlJZkxhw4dMmNqa2vNmG3btpkx1nnpcq64HDMu70XR6gtlrU1GRoY5R0pKSsTxxsZGlZaWmvMAABBruGIDAAAAwPcobAAAAAD4HoUNAAAAAN+jsAEAAADgexQ2AAAAAHyPwgYAAACA71HYAAAAAPA9ChsAAAAAvhfwXDoFXkBVVVVKS0vTjTfeqPj4+Fbj6urqzLnq6+sjjicmJppzuDQdDIVCZoxL08GEhAQzpkePHhHHXRpiujQdjIuLM2OsRn+u+Vhr062bXX/37NnTjGlubjZjXBqTBgKBdufiwiUXl9PXJR9rjV1yaWpqMmOs41dSxPP+DJdj2Dr2kpOTzTlczkmXc9vlPcKl2WpqamrEcZcGnX369Ik4Xl9fr0cffVSVlZXm43UlZ/YmAEDncNmXuGIDAAAAwPcobAAAAAD4HoUNAAAAAN+jsAEAAADgexQ2AAAAAHyPwgYAAACA71HYAAAAAPA9ChsAAAAAvmd3hOskycnJEZvjXXTRReYcVgPJ6upqc46TJ0+aMS5N8Xr37m3GuDT6O378eMRxl6ajVoM+ya2hYEVFhRkTDAbbnU9jY2NUcnFpOjpkyBAzxmqKeezYsXbPIUmDBg0yY1yek8vaWGucl5dnzuHSzNJlbazGupI0cOBAM8ZqrnvixAlzDpdGoNF4L5Lc3o+snF1e6/Ly8ojjLk11AQCIRVG/YlNYWKhAINDilpWVFe2HAQDAGXsTAHz+dcgVmxEjRuivf/1r+GeX3yoDANCR2JsA4POtQwqb7t2785swAEBMYW8CgM+3DvnygD179ignJ0d5eXm65ZZbtG/fvlZjQ6GQqqqqWtwAAIg29iYA+HyLemEzbtw4Pfvss3r99df1m9/8RmVlZZo4cWKrfzRcVFSktLS08G3AgAHRTgkA0MWxNwHA51/UC5uCggLddNNNGjVqlL70pS/plVdekSStWLHinPELFixQZWVl+FZSUhLtlAAAXRx7EwB8/nX41z336NFDo0aN0p49e845HgwGnb4SGACAaGFvAoDPnw5v0BkKhfTBBx8oOzu7ox8KAAAn7E0A8PkT9Ss2P/jBD3T99ddr4MCBKi8v109/+lNVVVVpzpw5bZonNTU14m/Lxo0bZ84xfPjwiOM7d+4059i0aZMZ49LUcfz48WZMpD9kPePtt9+OOO7SuPDqq682Y0pLS82Y9evXmzHp6elmzJQpUyKOu/zR7tq1a80Yl+alVi6S1NzcHHF8zZo17Z5DkqZNm2bGuDwnl3ysNXZZF5fGry7HTFlZmRnjcgxbfxNhnUuS2zk5YcIEM2bo0KFmzN69e80Y67V0mWP//v0Rx12OTT+K1t4EAIhdUS9sDh06pFtvvVUVFRXq16+fxo8fr40bNyo3NzfaDwUAgBP2JgD4/It6YbNy5cpoTwkAQLuwNwHA51+H/40NAAAAAHQ0ChsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4HtR/7rnaKmvr4/YKK64uNico66uLuK4SxNKaw5JOnz4sBmzdetWM+bYsWPtzufIkSPmHO+++64ZU11dbcbU1taaMS7ef//9iOP19fXmHKdOnTJjXOb5+9//bsZYDQwrKyvNOTzPM2NcGsgmJCSYMcePHzdjrLXZvXu3OUfPnj3NmKNHj5oxNTU1ZsyePXvMmIqKiojjLo1AXc5/lyaeLs+pvLzcjAmFQhHHXRridu8e+W2/sbHR6T0NAIBYwxUbAAAAAL5HYQMAAADA9yhsAAAAAPgehQ0AAAAA36OwAQAAAOB7FDYAAAAAfI/CBgAAAIDvUdgAAAAA8L2A59Ip8AKqqqpSWlqaZs6cqfj4+FbjXJotWs3sgsGgOUdiYmK7H0dya2bp0mwxOTk54vjp06ejkovVxE9ya8jY2NhoxljNNbt1s+vvlJQUM8blUHdpTGpxWZdAIGDGuDSHbGpqMmN69Ohhxlhr7HLMuOTicj5FOu/PcFkb69hzycXlnHTJpaGhwYyJi4szY6zjvG/fvuYcVhPPUCikxx57TJWVlUpNTTXn6yrO7E0AgM7hsi9xxQYAAACA71HYAAAAAPA9ChsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4Ht2s5JOEgwGI/aQcOnXkJSUFHHcpf+E1WNFklNvA5deIi69LmpqaiKOu/TdcMnFpU/QyZMnzRir744kZWdnRxx36Y/ikotLP5y8vLx2z+PS88VFTk6OGePSD8elN4+1xllZWeYcLn1YXM4nl15MmZmZZox1Lrisi0uPKpf3IpeeOS7HjXX+V1ZWmnNYz9vlfQgAgFjEFRsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4HsUNgAAAAB8j8IGAAAAgO9R2AAAAADwvZht0JmWlqZgMNjq+Lhx48w5hg0bFnF8586d5hxbtmwxYwYNGmTGjB8/3owpLi42Y955552I4wMHDjTnmDRpkhlz4MABM2b16tVmTJ8+fcyYL33pSxHHq6qqzDneeOMNM8alSeK0adPMmO7dI582GzZsMOdwcdVVV5kxkc6RM/72t7+ZMVbjTJdcXF7rt956y4w5cuSIGeNyDPfv3z/iuHUuSW7npMt70eDBg82Yffv2mTHr16+POL59+3ZzDuvcbm5uNucAACAWtfmKzfr163X99dcrJydHgUBAL730Uotxz/NUWFionJwcJSUlacqUKU4FBAAA54N9CQAgnUdhU1NTo9GjR2vp0qXnHF+8eLGWLFmipUuXavPmzcrKytL06dNVXV3d7mQBAPgs9iUAgHQeH0UrKChQQUHBOcc8z9Pjjz+uBx54QDNnzpQkrVixQpmZmXruuef0ne98p33ZAgDwGexLAAApyl8eUFxcrLKyMs2YMSN8XzAY1OTJk1v9u4NQKKSqqqoWNwAAouF89iWJvQkA/CiqhU1ZWZkkKTMzs8X9mZmZ4bHPKioqUlpaWvg2YMCAaKYEAOjCzmdfktibAMCPOuTrngOBQIufPc87674zFixYoMrKyvCtpKSkI1ICAHRhbdmXJPYmAPCjqH7dc1ZWlqR//IYsOzs7fH95eflZvy07IxgMOn1lLQAAbXU++5LE3gQAfhTVKzZ5eXnKyspq0d+koaFB69at08SJE6P5UAAAmNiXAKDraPMVm1OnTmnv3r3hn4uLi7Vt2zalp6dr4MCBmj9/vhYtWqTBgwdr8ODBWrRokZKTk3Xbbbe16XFOnz4d8WMCkT4bfYbVSLG8vNyco6GhwYw5ceKEGfPpNWtPPvX19RHHXf7A9eDBg2aMy/qePn3ajKmrqzNjDh8+HHG8pqbGnMNaF0lqbGxsdy6SFB8fH3Hc5Tk3NTWZMS6vk5WLZDfflOz1O3TokDmHy3ngEuPyWn7yySdmTCgUancu1hyS27nSrZv9O6TS0lIzxjrnevToYc7x6asW59LY2Oi7j11dqH0JABDb2lzYbNmyRVOnTg3/fN9990mS5syZo2eeeUb333+/6urqNG/ePJ04cULjxo3TG2+8oZSUlOhlDQDA/8e+BACQzqOwmTJlijzPa3U8EAiosLBQhYWF7ckLAAAn7EsAAKmDvhUNAAAAAC4kChsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4HsBL9J3ZHaCqqoqpaWl6aabborYfNClcZ7VBDEuLs6cIxgMmjEuubg0+rQaikpSYmJixHGXRoAuzSFdDguXGJd8rJjm5mZzDpdmoS7zuORrvU4ur6NLLi6NKl0kJSW1ew6XJqkuDVCTk5PNGJemoy7nnJWPy7nt8lpGIxdJERsSn2E14ExPTzfnSE1NjTgeCoW0ZMkSVVZWmrFdyZm9CQDQOVz2Ja7YAAAAAPA9ChsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4HsUNgAAAAB8j8IGAAAAgO/Z3ec6Sffu3SM26uvZs6c5h9Xoz6Wpo0vzPZcGiAkJCWaMSxM/q9FnNJp8Sm5NPOvq6swYl+dk5ezSSDFazRaj0QzU5ZhxOR6i1Rzx5MmTZoz1WmZmZppzuDS8dVkbl9cgJSXFjLFeb5cGqC7HbzTeiyS3Jr5WTG1trTmHdfy65AEAQCziig0AAAAA36OwAQAAAOB7FDYAAAAAfI/CBgAAAIDvUdgAAAAA8D0KGwAAAAC+R2EDAAAAwPcobAAAAAD4Xsw26OzVq1fEpov5+fnmHEOGDIk4vnv3bnOOd99914y55JJLzBiXfA8ePGjGbNq0KeJ4RkaGOceYMWPMmPLycjNmy5YtZkwgEDBjBg8eHHHcpQGiS2NCl3mys7PNGKvh5fbt2805XJpvfvnLXzZjXJqOvvzyy2ZMWVlZxPFrrrnGnKNfv35mzDvvvGPGHDlyxIwZN26cGWO9lps3bzbnOHDggBnjcm4PGjTIjCkuLjZj3n777YjjO3fuNOcoKSmJOO7SIBUAgFjEFRsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4HsUNgAAAAB8j8IGAAAAgO/FbB+bxsZGdevWet1VXV1tznH06NGI4y5znD592oypqakxYyoqKsyYyspKM6axsTHieH19vTnHiRMnzJiqqiozJhQKmTE9evQwY5KTkyOOp6SkmHO4aGhoMGMOHz5sxljrV1dXZ84RHx9vxrj0c3HpE+SSj3Wcuxybkc7XaOYiuR2fkfpgueZinW+uuUTr/G9qaoo4bj1nSUpLS2vXYwAAEKvafMVm/fr1uv7665WTk6NAIKCXXnqpxfjcuXMVCARa3MaPHx+tfAEAaIF9CQAgnUdhU1NTo9GjR2vp0qWtxlx77bUqLS0N31599dV2JQkAQGvYlwAA0nl8FK2goEAFBQURY4LBoLKyss47KQAAXLEvAQCkDvrygLVr1yojI0NDhgzRXXfdpfLy8o54GAAAnLAvAcDnX9S/PKCgoECzZs1Sbm6uiouL9dBDD2natGnaunXrOf+wNRQKtfgjdJc/xAUAwFVb9yWJvQkA/Cjqhc3s2bPD/z1y5Ejl5+crNzdXr7zyimbOnHlWfFFRkR555JFopwEAgKS270sSexMA+FGH97HJzs5Wbm6u9uzZc87xBQsWqLKyMnwrKSnp6JQAAF2YtS9J7E0A4Ecd3sfm2LFjKikpUXZ29jnHg8GgU+8FAACiwdqXJPYmAPCjNhc2p06d0t69e8M/FxcXa9u2bUpPT1d6eroKCwt10003KTs7W/v379fChQvVt29f3XjjjW16nMrKyohNDNetW2fO0dzcHHE8Li7OnMOl6aBL873NmzdH5bESEhLanctHH31kxrg0SXRp5JeRkWHGlJaWRhw/deqUOYdLs8UDBw6YMS5rYz2WyzcvuTQu3bFjhxnj8jpZx4xkN/r8y1/+Ys5hnW+SW2NSl/PyjTfeMGM8z2v347g0QHV5L4pW00vrtczLyzPnGD16dMTxUCik999/v015dbYLtS8BAGJbmwubLVu2aOrUqeGf77vvPknSnDlztGzZMu3YsUPPPvusTp48qezsbE2dOlXPP/981LrHAwDwaexLAADpPAqbKVOmRPxN6Ouvv96uhAAAaAv2JQCAdAG+PAAAAAAAOhqFDQAAAADfo7ABAAAA4HsUNgAAAAB8j8IGAAAAgO9R2AAAAADwvTZ/3fOF0q1bt4gNK10a/VkNL10aCro01nPpTp2YmGjGWA0FXWJcmg66rJ1L40eXmLS0NDMmMzMz4rhLM8ujR4+aMS6vt8tzso6r9PR0c47U1FQzJlrPKRqNX12afLrk4nKMu3DJx3rejY2N5hzRajrq0ujT5b3GWj+X52Q1mG1oaDDnAAAgFnHFBgAAAIDvUdgAAAAA8D0KGwAAAAC+R2EDAAAAwPcobAAAAAD4HoUNAAAAAN+jsAEAAADgexQ2AAAAAHwvZht09u7dO2ITvssvv9ycY9CgQRHHP/roI3OO7du3mzF5eXlmjEu+hw4dMmPefffdiON9+vQx57jsssvMmJMnT5oxH3zwgRlz0UUXmTH5+fkRx12am3744YdmjIvc3Fwzxmr82KtXL3OOfv36mTEujT5dXqf33nvPjKmvr484PnbsWHMOl6ajW7ZsMWMqKirMGOuYkaTs7OyI41u3bjXnOHjwoBlzxRVXmDEu7xEuj7Vx48aI47t37zbnKC0tjTju0igUAIBYxBUbAAAAAL5HYQMAAADA9yhsAAAAAPgehQ0AAAAA36OwAQAAAOB7FDYAAAAAfI/CBgAAAIDvUdgAAAAA8L2YbdDpeZ48z2t1vLm52ZyjsbEx4rjLHJFyOMOloZ2Vi+s8Vj4uc5w+fbrdjyNJcXFxZkxVVZUZYzXXdGnQefz4cTMmGusr2U1Qhw8fbs4RDAbNGJeGjR9//LEZU1tb2+58XI4Zl8dxOQ9cXKjz38WFPP8tLuekSwwAAH7EFRsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4HsUNgAAAAB8j8IGAAAAgO8FPJfGHRdQVVWV0tLSdPPNNys+Pr7VuG7d7JosEAhEHHd56tHqdeE3CQkJZozLa+DSx+bo0aMRx3v06GHOcfHFF5sxLr1YXPrCjB49OuL4XXfdZc5hPWdJ+vGPf2zGbNy40Yz54he/aMZYvXcqKirMOaqrq80Yl55ESUlJZox1bkv2+R2t89/lPHARjcdKTk4257DOp1AopGXLlqmyslKpqanmfF3Fmb0JANA5XPalNu3IRUVFuvLKK5WSkqKMjAx9/etfP6u5oud5KiwsVE5OjpKSkjRlyhTt3Lmz7dkDAOCAvQkAILWxsFm3bp3uvvtubdy4UatXr1ZjY6NmzJihmpqacMzixYu1ZMkSLV26VJs3b1ZWVpamT5/u9NtcAADair0JACBJ3dsSvGrVqhY/L1++XBkZGdq6dauuueYaeZ6nxx9/XA888IBmzpwpSVqxYoUyMzP13HPP6Tvf+U70MgcAQOxNAIB/aNeHwysrKyVJ6enpkqTi4mKVlZVpxowZ4ZhgMKjJkydrw4YN7XkoAACcsDcBQNfUpis2n+Z5nu677z5dddVVGjlypCSprKxMkpSZmdkiNjMzUwcOHDjnPKFQSKFQKPyzyx+bAwBwLuxNANB1nfcVm3vuuUfbt2/X7373u7PGPvuNRZ7ntfotRkVFRUpLSwvfBgwYcL4pAQC6OPYmAOi6zquwuffee/WnP/1Ja9asUf/+/cP3Z2VlSfq/346dUV5eftZvys5YsGCBKisrw7eSkpLzSQkA0MWxNwFA19amwsbzPN1zzz164YUX9OabbyovL6/FeF5enrKysrR69erwfQ0NDVq3bp0mTpx4zjmDwaBSU1Nb3AAAcMXeBACQ2vg3Nnfffbeee+45vfzyy0pJSQn/9istLU1JSUkKBAKaP3++Fi1apMGDB2vw4MFatGiRkpOTddttt0U18aampnbP4dLkz4VLoz+XfF3yiYuLa9e46+O4PCeXhpcNDQ1mjNWYsL6+3pzj5MmTZoxLc8icnBwzplevXhHHjxw5Ys7xySefmDEujR979+5txrg0W7WOT5fj1yXGpQllNM5tyT6Go3X+u+Trcj5Fo1eyyzETjfeQWBNLexMAoPO0qbBZtmyZJGnKlCkt7l++fLnmzp0rSbr//vtVV1enefPm6cSJExo3bpzeeOMNpaSkRCVhAAA+jb0JACC1sbBx+Y1iIBBQYWGhCgsLzzcnAACcsTcBAKR29rEBAAAAgFhAYQMAAADA9yhsAAAAAPgehQ0AAAAA36OwAQAAAOB7FDYAAAAAfK9NX/d8IfXu3TtiY8FRo0aZc1x88cURx/ft22fOsXPnTjMmNzfXjBk5cqQZc/jwYTNm+/btEcezsrLMOa644goz5ujRo2bM5s2bzRiXtRk7dmzEcZcmnzt27DBjXJpZXn311WaM9dWyb731ljlHKBQyYx555BEzxqX55qZNm8yY48ePRxxvrTv7p/Xs2dOMef/9982YEydOmDGXX365GZOZmdnuXFwaqbqc2y7nwaFDh8yYd999N+L43r17zTmsBrLRapAKAMCFxhUbAAAAAL5HYQMAAADA9yhsAAAAAPgehQ0AAAAA36OwAQAAAOB7FDYAAAAAfI/CBgAAAIDvUdgAAAAA8L2YbdDZrVs3devWet2VmJhoztGjR4+I4y5zxMXFmTHBYNCMSUlJMWOSkpLanU98fLw5h7UuklRdXW3GRHp9zkhOTjZjsrOzI47X1dWZc7g8b5dmln379jVjKioqIo5/+OGH5hzdu9un3qxZs8yY/v37mzEujWitBp0ux6bLMe7yGrgcVy75WA1Do5WLy/uIS/NSl3msfBobG8056uvrI47ToBNAe/3whz80Y1z22x/96EfRSAddCFdsAAAAAPgehQ0AAAAA36OwAQAAAOB7FDYAAAAAfI/CBgAAAIDvUdgAAAAA8D0KGwAAAAC+R2EDAAAAwPcCnud5nZ3Ep1VVVSktLU3f+MY3IjbQc2nIaDVBdGlmd/r06XY/juSWbzTycXkcl0aALk36rEZ/UnSaLbocoqFQyIxxed69evUyY6y1KS8vN+dobm42Y/r162fGuDSqdGlwajVkrampMedoaGgwY1yOB5fGmdForulybrucky7nnEujX5d8rGMvGud/Q0ODnn76aVVWVio1NdWcr6s4szcBsD322GNmzP3332/GBAKBaKSDzwmXfYkrNgAAAAB8j8IGAAAAgO9R2AAAAADwPQobAAAAAL5HYQMAAADA9yhsAAAAAPgehQ0AAAAA36OwAQAAAOB7dmfJTykqKtILL7yg3bt3KykpSRMnTtRjjz2moUOHhmPmzp2rFStWtPh348aN08aNG9uUWFNTU8TmeC7NIa3mei6NNV1iXJpDujTfc2niZzXgc2ko6NJs0aWRokvjLJdGlFZzSBcua+dyzJw6dcqMsZpDujQCdTkeDh48aMa4rG80miyePHnSjKmtrY1KLi7HXjTOf5dmli65uDRAdWl463I+WcdWSkqKOYfVZNLl/SzWXMi9CYDtRz/6UVRigLZq0xWbdevW6e6779bGjRu1evVqNTY2asaMGWf9j/K1116r0tLS8O3VV1+NatIAAJzB3gQAkNp4xWbVqlUtfl6+fLkyMjK0detWXXPNNeH7g8GgsrKyopMhAAARsDcBAKR2/o1NZWWlJCk9Pb3F/WvXrlVGRoaGDBmiu+66S+Xl5e15GAAAnLE3AUDX1KYrNp/meZ7uu+8+XXXVVRo5cmT4/oKCAs2aNUu5ubkqLi7WQw89pGnTpmnr1q0KBoNnzRMKhVp8pruqqup8UwIAdHHsTQDQdZ13YXPPPfdo+/bteuutt1rcP3v27PB/jxw5Uvn5+crNzdUrr7yimTNnnjVPUVGRHnnkkfNNAwCAMPYmAOi6zuujaPfee6/+9Kc/ac2aNerfv3/E2OzsbOXm5mrPnj3nHF+wYIEqKyvDt5KSkvNJCQDQxbE3AUDX1qYrNp7n6d5779WLL76otWvXKi8vz/w3x44dU0lJibKzs885HgwGz/kxAAAAXLA3AQCkNhY2d999t5577jm9/PLLSklJUVlZmaR/9EVISkrSqVOnVFhYqJtuuknZ2dnav3+/Fi5cqL59++rGG29sU2K9evWK2C9k2LBh5hwDBgyIOO7SJ+TDDz80Y6zfDEpu+R4+fNiM2b17d8TxzMxMc45Ro0aZMRUVFWbMu+++a8b06tXLjBk7dmzEcZe+O1u2bDFjevToYcZMnDjRjLH6jWzatMmcw6Xf0HXXXWfGWD11JLfXyepTM2nSJHOOnj17mjE7d+5sdy6SNGLECDOmX79+Ecd37dplzuFyTg4fPtyMcXmP+OSTT8yYv//97xHHi4uLzTmsc9ul506suZB7EwAgdrWpsFm2bJkkacqUKS3uX758uebOnau4uDjt2LFDzz77rE6ePKns7GxNnTpVzz//vFPjOAAA2oq9CQAgncdH0SJJSkrS66+/3q6EAABoC/YmAIDUzj42AAAAABALKGwAAAAA+B6FDQAAAADfo7ABAAAA4HsUNgAAAAB8j8IGAAAAgO+16eueL6T4+PiIzQfT09PNOaymeKdOnTLncGmAmJqa2u5cJOn06dNmzL59+yKOuzRJbK3T9qdZX58qua1NcnKyGZOTkxNxvKqqypzDpUN4YmKiGePS4LS5udmMscTFxZkx1rpI//gaW4vV1NEln4suusicw2qIKbk1vKytrTVj+vbta8ZYOZeUlJhzuBzjffr0MWNczv+GhgYzpnv3yG/ZLmtnNeiMxvENAEBn4IoNAAAAAN+jsAEAAADgexQ2AAAAAHyPwgYAAACA71HYAAAAAPA9ChsAAAAAvkdhAwAAAMD3KGwAAAAA+F7Ac+nEeAFVVVUpLS1Nt9xyS8TmeC6NCa2mjaFQyJyjrq7OjIlWo0qXBn1WA774+Hhzjh49epgxLs1Cq6urzRiXfFJSUiKONzU1mXO4NPG0mhtKUq9evcwY65Q5efKkOYeL3r17mzEujT6PHTtmxlhr7NKE1uW1djlmXM4Dl2PYOi9ramqikks03oskqb6+vt0xgUDAnMM6DxoaGvQ///M/qqysdHrdu4ozexMAoHO47EtcsQEAAADgexQ2AAAAAHyPwgYAAACA71HYAAAAAPA9ChsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfszsWdpKGhoaIjRBdGjJaDThdmua5xLg0HSwtLTVjXBpIJiYmRhx3aTpYVlZmxrg0fnRpTOjSdLC8vDzieLdudv0drVyOHj1qxkQjFxcVFRVmjEt/XZd8rDV2aTrq0kjVOn4lt2OvsrKy3fm4nNsu52Q03otcH8tqTNqnTx9zDqvxq0uuAADEIq7YAAAAAPA9ChsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4HsUNgAAAAB8j8IGAAAAgO+1qUHnsmXLtGzZMu3fv1+SNGLECP3kJz9RQUGBpH80C3zkkUf061//WidOnNC4ceP05JNPasSIEW1OrFevXkpISGh1/JJLLjHnyMrKijh++PBhc47i4uJ2P47klq9Lc8g9e/ZEHHdp0DdkyBAzxqUB4ocffmjGWA0FJWnYsGERx+vq6sw5du/ebcbEx8ebMcOHDzdjmpubI45/8MEH7Z7DNReX5+SyNtYaDx061JwjJSXFjHE5ZlyagVrHjCSlp6dHHLfOJcluHitF573I9bGs9Ttw4IA5x/vvvx9xvLGx0Zwj1lzIvQkAELvadMWmf//+evTRR7VlyxZt2bJF06ZN0w033KCdO3dKkhYvXqwlS5Zo6dKl2rx5s7KysjR9+nRVV1d3SPIAALA3AQCkNhY2119/va677joNGTJEQ4YM0c9+9jP17NlTGzdulOd5evzxx/XAAw9o5syZGjlypFasWKHa2lo999xzHZU/AKCLY28CAEjt+BubpqYmrVy5UjU1NZowYYKKi4tVVlamGTNmhGOCwaAmT56sDRs2RCVZAAAiYW8CgK6rTX9jI0k7duzQhAkTVF9fr549e+rFF1/UpZdeGt4gMjMzW8RnZmZG/Nx3KBRSKBQK/1xVVdXWlAAAXRx7EwCgzVdshg4dqm3btmnjxo363ve+pzlz5mjXrl3h8UAg0CLe87yz7vu0oqIipaWlhW8DBgxoa0oAgC6OvQkA0ObCJiEhQYMGDVJ+fr6Kioo0evRoPfHEE+Fv/SkrK2sRX15eftZvyj5twYIFqqysDN9KSkramhIAoItjbwIAtLuPjed5CoVCysvLU1ZWllavXh0ea2ho0Lp16zRx4sRW/30wGFRqamqLGwAA7cHeBABdT5v+xmbhwoUqKCjQgAEDVF1drZUrV2rt2rVatWqVAoGA5s+fr0WLFmnw4MEaPHiwFi1apOTkZN12220dlT8AoItjbwIASG0sbI4cOaI77rhDpaWlSktL02WXXaZVq1Zp+vTpkqT7779fdXV1mjdvXrgJ2htvvOHUuO+zgsGggsFgq+P9+/c357Ca+HXrZl+wKi0tNWMyMjLMGJdGcPv27TNjrI9DuDTovPTSS80Yl+d98OBBM6Z3795mjPU6ufSacPmYSFJSkhnj0oiyqakp4vgnn3zS7jkkafDgwWZMYmKiGfPZj+Cci3UuuDShdDn2KioqzJjTp0+bMbm5uWaM9TcRJ06cMOdwOfZc/vbC5bj6+OOPzRirYbBLc1PrvHVpHhtrLuTeBACIXW0qbJ5++umI44FAQIWFhSosLGxPTgAAOGNvAgBIUfgbGwAAAADobBQ2AAAAAHyPwgYAAACA71HYAAAAAPA9ChsAAAAAvkdhAwAAAMD3Ap7neZ2dxKdVVVUpLS1Nt912mxISElqNc+kC3aNHj4jjNTU1TvlYXPqjpKWlmTF1dXVmTGVlZcRxl74mvXr1MmNCoZAZ49IzIz4+3oyxet00NjZGJReXvkUufXesU8alP4rLaeeSi8tzcsnHWmOXYybS+XqGy+vU0NBgxrjkE6kPlmSfS5LbOenyXpScnGzG1NbWmjFWzi7niuX06dP6/e9/r8rKSqfn1lWc2ZsAAJ3DZV/iig0AAAAA36OwAQAAAOB7FDYAAAAAfI/CBgAAAIDvUdgAAAAA8D0KGwAAAAC+R2EDAAAAwPcobAAAAAD4XvfOTqA19fX1ampqanX82LFj5hxWk0mXZpYuMS5NB/ft22fGuDSztBr9uTQdLC4uNmO6d7cPDasBqvSPZn+WgwcPRhwPBALmHD179jRjXJpiHjp0yIyxuKyLy3M6fPiwGePynFzysRp9lpeXm3O4NId0aVTpch4cPXrUjLGOvWjlEo33IkmKi4szY6wGkf369TPnSE9PjzjukisAALGIKzYAAAAAfI/CBgAAAIDvUdgAAAAA8D0KGwAAAAC+R2EDAAAAwPcobAAAAAD4HoUNAAAAAN+jsAEAAADgezHboDM1NVUJCQmtjo8cOdKcIyMjI+K4S9PBkpISM8alKd7AgQPNmOPHj5sx+/fvjzjeq1cvc46LL77YjKmurjZjXBp9ujRB/MIXvhBx3KVh4Mcff2zGuDRbvOSSS8yY5ubmdufi0ljTJReXpo4uzWHr6+vbnYvLa+1yzLgce2PGjDFjrHPhwIED5hwu5+SoUaPMGOu9SJIqKirMGGv9XN6vPvjgg4jjLo1WAQCIRVyxAQAAAOB7FDYAAAAAfI/CBgAAAIDvUdgAAAAA8D0KGwAAAAC+R2EDAAAAwPcobAAAAAD4HoUNAAAAAN9rU4POZcuWadmyZeEmkSNGjNBPfvITFRQUSJLmzp2rFStWtPg348aN08aNG9ucWFJSkoLBYKvjQ4YMMecYPnx4xPGdO3eac7g06BswYIAZM378eDPGpbGj1cQvJyfHnGPcuHFmTFlZWbtzkaQ+ffqYMVdeeWXE8aqqKnOOo0ePmjEuDSRdGj9aDTpPnDhhztHU1GTGXH755WZMYmKiGePS8NJaY5cmlC6vdV1dnRnj0rz00ksvNWOsprinT58257Aal0rS0KFDzZhhw4aZMXv37jVjSktLI467nAdWk1/r+I5FF3JvAgDErjYVNv3799ejjz6qQYMGSZJWrFihG264Qe+9955GjBghSbr22mu1fPny8L9JSEiIYroAALTE3gQAkNpY2Fx//fUtfv7Zz36mZcuWaePGjeHNIxgMKisrK3oZAgAQAXsTAEBqx9/YNDU1aeXKlaqpqdGECRPC969du1YZGRkaMmSI7rrrLpWXl0clUQAALOxNANB1temKjSTt2LFDEyZMUH19vXr27KkXX3wx/Hn3goICzZo1S7m5uSouLtZDDz2kadOmaevWra3+vUwoFFIoFAr/7PL3FAAAfBp7EwCgzYXN0KFDtW3bNp08eVJ//OMfNWfOHK1bt06XXnqpZs+eHY4bOXKk8vPzlZubq1deeUUzZ84853xFRUV65JFHzv8ZAAC6PPYmAECbP4qWkJCgQYMGKT8/X0VFRRo9erSeeOKJc8ZmZ2crNzdXe/bsaXW+BQsWqLKyMnwrKSlpa0oAgC6OvQkA0OYrNp/leV6Ly/WfduzYMZWUlCg7O7vVfx8MBiN+rTMAAG3F3gQAXU+bCpuFCxeqoKBAAwYMUHV1tVauXKm1a9dq1apVOnXqlAoLC3XTTTcpOztb+/fv18KFC9W3b1/deOONHZU/AKCLY28CAEhtLGyOHDmiO+64Q6WlpUpLS9Nll12mVatWafr06aqrq9OOHTv07LPP6uTJk8rOztbUqVP1/PPPKyUlpc2JlZWVKT4+vtXxt99+25zjgw8+iDju0nzTpVFlbW1tVOaprKw0Yw4fPtzuOVyed01NjRlz6NAhM8alYeCpU6cijjc0NJhzHDx40IyJdDyd4dKQ0Wpg6LIuLk0QXRpIdu9un8IuH6Gx1vivf/2rOUdSUlJUcnFp4rlmzRozJjU1NeK4dS5JbueTy3vRrl27zJhjx46ZMUeOHIk43q9fP3OOjIyMiOOnT5/WX/7yF3OeWHIh9yYAQOxqU2Hz9NNPtzqWlJSk119/vd0JAQDQFuxNAACpHX1sAAAAACBWUNgAAAAA8D0KGwAAAAC+R2EDAAAAwPcobAAAAAD4HoUNAAAAAN+jsAEAAADgewHP87zOTuLTqqqqlJaWpptvvjliQ0WrqaNkN1t0aSiYnJxsxoRCITPGJd9gMGjG9OzZs925uDTfdGlmaeUiSY2NjWZMdXV1xPG4uDhzDpdGey6HelVVlRkTCATanYs1h2Svi+TW6NMlH2uNXdalqakpKrm4HHsua2M1HXU5fl3OSZfzyaXxq0uz1V69ekUct5pvSlJ6enrE8fr6ehUVFamystJsctqVnNmbAACdw2Vf4ooNAAAAAN+jsAEAAADgexQ2AAAAAHyPwgYAAACA71HYAAAAAPA9ChsAAAAAvkdhAwAAAMD37MYJnWTq1KkR+8y49Gux+lgkJCSYc7j0sTh9+rQZ49LHwqV/R2JiYsRxl74xdXV1ZoxLTw0rF8mtz4qVj0vPF5d+Qy59bFzWxuLSH8mFSy4uz8kln27dIv+Oo7a2Niq5uBwzLseey9pY54JLLi7npMu57fIe4dJXx1JSUmLG7N+/P+K49b4JAECs4ooNAAAAAN+jsAEAAADgexQ2AAAAAHyPwgYAAACA71HYAAAAAPA9ChsAAAAAvkdhAwAAAMD3KGwAAAAA+F7MNuicPn26UlJSWh13aZxpNddzaZrn0gjUpYlftBp9Wvm4NDd0yaWpqanduUh240fJbpTo0uTTpUmiS6NPl6aN0cjFhcvr5PKcXF4na41d1sXltXZZG5fX22Vt4uLiIo67rItLw9tovBdJbg06Dx06FHH8ueeeM+fYvn17xHGXcx8AgFjEFRsAAAAAvkdhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4HsUNgAAAAB8j8IGAAAAgO/FXB8bz/MkSadOnYoYF43+Mi59YxoaGtr9OJJbvtHIx6WPjUsuLr0sXNbGpbeJNY9LXxOX5+TS88XlOUUjl2jNE63nZK1xtF7raPTUcZ3H6mPj8pxc+thEq9eVy/Ourq6OOO7SJ8g6t8+Mn3kvxj+wHgDQuVzeh2OusDmzcY8ZM6aTMwGArqu6ulppaWmdnUbMsIpKAEDHctmXAl6M/RqqublZhw8fVkpKSvi30VVVVRowYIBKSkqUmprayRnayLdjkW/HIt+OFev5ep6n6upq5eTkOF2F6yrYmy488u1Y5NuxyDd62rIvxdwVm27duql///7nHEtNTY25xY6EfDsW+XYs8u1YsZwvV2rOxt7Ueci3Y5FvxyLf6HDdl/h1HAAAAADfo7ABAAAA4Hu+KGyCwaAefvhhBYPBzk7FCfl2LPLtWOTbsfyWL1rnt9eSfDsW+XYs8u1Yfsu3NTH35QEAAAAA0Fa+uGIDAAAAAJFQ2AAAAADwPQobAAAAAL5HYQMAAADA92K+sHnqqaeUl5enxMREjR07Vn/72986O6VWFRYWKhAItLhlZWV1dlph69ev1/XXX6+cnBwFAgG99NJLLcY9z1NhYaFycnKUlJSkKVOmaOfOnZ2TrOx8586de9Z6jx8/vlNyLSoq0pVXXqmUlBRlZGTo61//uj788MMWMbG0vi75xtL6Llu2TJdddlm4cdiECRP02muvhcdjaW1d8o2ltcX58cvexL4UXX7alyT2po7G3hR7Yrqwef755zV//nw98MADeu+993T11VeroKBABw8e7OzUWjVixAiVlpaGbzt27OjslMJqamo0evRoLV269Jzjixcv1pIlS7R06VJt3rxZWVlZmj59uqqrqy9wpv9g5StJ1157bYv1fvXVVy9ghv9n3bp1uvvuu7Vx40atXr1ajY2NmjFjhmpqasIxsbS+LvlKsbO+/fv316OPPqotW7Zoy5YtmjZtmm644YbwBhFLa+uSrxQ7a4u289vexL4UPX7alyT2po7G3hSDvBj2T//0T953v/vdFvcNGzbM+/GPf9xJGUX28MMPe6NHj+7sNJxI8l588cXwz83NzV5WVpb36KOPhu+rr6/30tLSvF/+8pedkGFLn83X8zxvzpw53g033NAp+VjKy8s9Sd66des8z4v99f1svp4X2+vreZ7Xu3dv77/+679ifm3POJOv58X+2iIyP+1N7Esdx2/7kuexN10I7E2dK2av2DQ0NGjr1q2aMWNGi/tnzJihDRs2dFJWtj179ignJ0d5eXm65ZZbtG/fvs5OyUlxcbHKysparHcwGNTkyZNjer3Xrl2rjIwMDRkyRHfddZfKy8s7OyVJUmVlpSQpPT1dUuyv72fzPSMW17epqUkrV65UTU2NJkyYEPNr+9l8z4jFtYXNj3sT+9KFFcvnNntTx2Fvig3dOzuB1lRUVKipqUmZmZkt7s/MzFRZWVknZRXZuHHj9Oyzz2rIkCE6cuSIfvrTn2rixInauXOn+vTp09npRXRmTc+13gcOHOiMlEwFBQWaNWuWcnNzVVxcrIceekjTpk3T1q1bO7Vzrud5uu+++3TVVVdp5MiRkmJ7fc+VrxR767tjxw5NmDBB9fX16tmzp1588UVdeuml4Q0i1ta2tXyl2FtbuPPb3sS+dGHF8rnN3tQx2JtiS8wWNmcEAoEWP3ued9Z9saKgoCD836NGjdKECRN0ySWXaMWKFbrvvvs6MTN3flrv2bNnh/975MiRys/PV25url555RXNnDmz0/K65557tH37dr311ltnjcXi+raWb6yt79ChQ7Vt2zadPHlSf/zjHzVnzhytW7cuPB5ra9tavpdeemnMrS3aLtaOt9awL11YsXxuszd1DPam2BKzH0Xr27ev4uLizvoNWHl5+VnVb6zq0aOHRo0apT179nR2KqYz35Lj5/XOzs5Wbm5up673vffeqz/96U9as2aN+vfvH74/Vte3tXzPpbPXNyEhQYMGDVJ+fr6Kioo0evRoPfHEEzG7tq3ley6dvbZw5/e9iX3pwoqVc5u9qeOwN8WWmC1sEhISNHbsWK1evbrF/atXr9bEiRM7Kau2CYVC+uCDD5Sdnd3ZqZjy8vKUlZXVYr0bGhq0bt0636z3sWPHVFJS0inr7Xme7rnnHr3wwgt68803lZeX12I81tbXyvdcOnN9z8XzPIVCoZhb29acyfdcYm1t0Tq/703sSxdWZ5/b7E0XHntTJ7uw31XQNitXrvTi4+O9p59+2tu1a5c3f/58r0ePHt7+/fs7O7Vz+v73v++tXbvW27dvn7dx40bvq1/9qpeSkhIz+VZXV3vvvfee995773mSvCVLlnjvvfeed+DAAc/zPO/RRx/10tLSvBdeeMHbsWOHd+utt3rZ2dleVVVVzOVbXV3tff/73/c2bNjgFRcXe2vWrPEmTJjgXXTRRZ2S7/e+9z0vLS3NW7t2rVdaWhq+1dbWhmNiaX2tfGNtfRcsWOCtX7/eKy4u9rZv3+4tXLjQ69atm/fGG294nhdba2vlG2tri7bz097EvnTh8o3Fc5u9qWOxN8WemC5sPM/znnzySS83N9dLSEjwxowZ0+Ir/2LN7NmzvezsbC8+Pt7LycnxZs6c6e3cubOz0wpbs2aNJ+ms25w5czzP+8fXPj788MNeVlaWFwwGvWuuucbbsWNHTOZbW1vrzZgxw+vXr58XHx/vDRw40JszZ4538ODBTsn1XHlK8pYvXx6OiaX1tfKNtfX99re/HX4f6Nevn/fFL34xvHF4XmytrZVvrK0tzo9f9ib2pQuXbyye2+xNHYu9KfYEPM/zon8dCAAAAAAunJj9GxsAAAAAcEVhAwAAAMD3KGwAAAAA+B6FDQAAAADfo7ABAAAA4HsUNgAAAAB8j8IGAAAAgO9R2AAAAADwPQobAAAAAL5HYQMAAADA9yhsAAAAAPgehQ0AAAAA3/t/0O9QPnzQrhMAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 67
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
