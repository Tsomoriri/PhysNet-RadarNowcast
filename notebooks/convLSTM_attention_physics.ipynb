{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T18:43:26.838931Z",
     "start_time": "2024-06-24T18:43:25.415083Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T18:43:27.008815Z",
     "start_time": "2024-06-24T18:43:26.840419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ShiftingWindowAttention(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        pad_h = (self.window_size - H % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - W % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        x = x.view(B, Hp // self.window_size, self.window_size, Wp // self.window_size, self.window_size, C)\n",
    "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        qkv = self.qkv(windows).reshape(-1, self.window_size * self.window_size, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(-1, self.window_size * self.window_size, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        x = x.view(B, Hp // self.window_size, Wp // self.window_size, self.window_size, self.window_size, C)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "        return x\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias, physics_kernel_size, window_size, num_heads):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "        self.physics_conv_x = nn.Conv2d(in_channels=self.input_dim,\n",
    "                                        out_channels=self.hidden_dim,\n",
    "                                        kernel_size=physics_kernel_size,\n",
    "                                        padding=physics_kernel_size[0] // 2,\n",
    "                                        bias=False)\n",
    "\n",
    "        self.physics_conv_y = nn.Conv2d(in_channels=self.input_dim,\n",
    "                                        out_channels=self.hidden_dim,\n",
    "                                        kernel_size=physics_kernel_size,\n",
    "                                        padding=physics_kernel_size[1] // 2,\n",
    "                                        bias=False)\n",
    "\n",
    "        self.attention = ShiftingWindowAttention(hidden_dim, window_size, num_heads)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        if input_tensor.dim() == 5:\n",
    "            input_tensor = input_tensor.squeeze(1)\n",
    "\n",
    "        if input_tensor.size(1) != self.input_dim:\n",
    "            raise ValueError(f\"Expected input_tensor to have {self.input_dim} channels, but got {input_tensor.size(1)} channels instead\")\n",
    "\n",
    "        if h_cur.size(1) != self.hidden_dim:\n",
    "            raise ValueError(f\"Expected h_cur to have {self.hidden_dim} channels, but got {h_cur.size(1)} channels instead\")\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "\n",
    "        physics_conv_x = self.physics_conv_x(input_tensor)\n",
    "        physics_conv_y = self.physics_conv_y(input_tensor)\n",
    "\n",
    "        i = torch.sigmoid(cc_i + physics_conv_x)\n",
    "        f = torch.sigmoid(cc_f + physics_conv_x)\n",
    "        o = torch.sigmoid(cc_o + physics_conv_y)\n",
    "        g = torch.tanh(cc_g + physics_conv_y)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        # Apply shifting window attention\n",
    "        h_next = h_next.permute(0, 2, 3, 1)  # Change to (B, H, W, C)\n",
    "        h_next = self.attention(h_next)\n",
    "        h_next = h_next.permute(0, 3, 1, 2)  # Change back to (B, C, H, W)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, physics_kernel_size, output_dim,\n",
    "                 batch_first=False, bias=True, return_all_layers=False, window_size=8, num_heads=4):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias,\n",
    "                                          physics_kernel_size=physics_kernel_size,\n",
    "                                          window_size=window_size,\n",
    "                                          num_heads=num_heads))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        self.output_conv = nn.Conv2d(in_channels=hidden_dim[-1],\n",
    "                                     out_channels=output_dim,\n",
    "                                     kernel_size=1,\n",
    "                                     padding=0)\n",
    "        # Initialize velocities as trainable parameters\n",
    "        self.velocity_x = nn.Parameter(torch.tensor(0.1))\n",
    "        self.velocity_y = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "   \n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if input_tensor.dim() == 4:\n",
    "            # (b, h, w, c) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(0, 3, 1, 2).unsqueeze(1)\n",
    "        elif input_tensor.dim() == 5:\n",
    "            if not self.batch_first:\n",
    "                # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "                input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, t, _, h, w = input_tensor.size()\n",
    "\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        # Remove the sequence length dimension before applying the output convolution\n",
    "        output = self.output_conv(layer_output_list[0].squeeze(1))\n",
    "        # Permute the output to have shape (b, h, w, c)\n",
    "        output = output.permute(0, 2, 3, 1)\n",
    "        return output, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "    \n",
    "    def advection_loss(self, input_tensor, output_tensor):\n",
    "        grad = torch.autograd.grad(outputs=output_tensor, inputs=input_tensor,\n",
    "                                   grad_outputs=torch.ones_like(output_tensor), create_graph=True)[0]\n",
    "        dudx = grad[:, :, 0]\n",
    "        dudy = grad[:, :, 1]\n",
    "        dudt = grad[:, :, 2]\n",
    "\n",
    "        physics = dudt + self.velocity_x * dudx + self.velocity_y * dudy\n",
    "        loss = torch.mean((physics) ** 2)\n",
    "\n",
    "        return loss"
   ],
   "id": "f2290a98f0fe48bf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T18:44:03.790588Z",
     "start_time": "2024-06-24T18:44:03.301515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load radar data\n",
    "movies = np.load('/home/sushen/PhysNet-RadarNowcast/src/datasets/rect_movie.npy')\n",
    "movies.shape # (980, 40, 40, 20) -- here each movie is of length 20\n",
    "\n",
    "# in our model we will use the first four images as inputs and predict the\n",
    "# fifth image\n",
    "x = movies[:, :, :,  :4]\n",
    "y = movies[:, :, :, 4:5]\n",
    "\n",
    "\n",
    "# function: animation of a sequence of radar data (shape = nx,ny,ntime)\n",
    "def animate(x):\n",
    "  fig, ax = plt.subplots()\n",
    "  vmax = np.max(x)\n",
    "  im = ax.imshow(x[:,:,0], vmin=0, vmax=vmax)\n",
    "  fig.colorbar(im)\n",
    "  plt.axis('off')\n",
    "  def anim_(i):\n",
    "      im.set_data(x[:,:,i])\n",
    "      ax.set_title(str(i+1) + '/' + str(x.shape[2]))\n",
    "  anim = animation.FuncAnimation(\n",
    "      fig, anim_, interval=300, frames=x.shape[2], repeat_delay=1000)\n",
    "  plt.show()\n",
    "\n",
    "# i_plt = 340\n",
    "# i_plt = 123\n",
    "i_plt = np.int32(np.random.sample() * movies.shape[0])\n",
    "animate(x[i_plt,:,:,:])\n",
    "plt.show()\n",
    "\n",
    "# train validate test split\n",
    "tvt = np.tile(['train','train','train','validate','test'], y.shape[0])[:y.shape[0]]\n",
    "x_train = x[np.where(tvt == 'train')]\n",
    "y_train = y[np.where(tvt == 'train')]\n",
    "x_validate = x[np.where(tvt == 'validate')]\n",
    "y_validate = y[np.where(tvt == 'validate')]\n",
    "x_test = x[np.where(tvt == 'test')]\n",
    "y_test = y[np.where(tvt == 'test')]\n",
    "\n",
    "n_test = x_test.shape[0]\n",
    "i_plt = np.int32(np.random.sample() * n_test)\n",
    "true = np.append(x_test[i_plt,:,:,:], y_test[i_plt,:,:,:], axis=2)\n",
    "# plot an input/output pair\n",
    "i_plt = 20\n",
    "i_plt = np.int32(np.random.sample() * x_train.shape[0])\n",
    "for jj in range(4):\n",
    "  plt.subplot(1,5,jj+1)\n",
    "  plt.imshow(x_train[i_plt,:,:,jj])\n",
    "  plt.axis('off')\n",
    "  plt.title('input')\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(y_train[i_plt,:,:,0])\n",
    "plt.title('target output')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "e61b4bc43c4f8cce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sushen/anaconda3/envs/pinn/lib/python3.11/site-packages/matplotlib/animation.py:892: UserWarning: Animation was deleted without rendering anything. This is most likely not intended. To prevent deletion, assign the Animation to a variable, e.g. `anim`, that exists until you output the Animation using `plt.show()` or `anim.save()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T18:44:09.152302Z",
     "start_time": "2024-06-24T18:44:07.429152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "def update_grid(rin_physics):\n",
    "    # Get the shape of the input tensor\n",
    "    shape = rin_physics.shape\n",
    "    # Create an empty tensor with the same shape\n",
    "    updated_grid = np.zeros(shape)\n",
    "\n",
    "    # Iterate through each element in the batch\n",
    "    for i in range(shape[0]):\n",
    "        # Extract the individual grid\n",
    "        grid = rin_physics[i]\n",
    "\n",
    "        # Find the max and min x, y values\n",
    "        max_x, max_y = np.unravel_index(np.argmax(grid[:, :, 0]), grid[:, :, 0].shape)\n",
    "        min_x, min_y = np.unravel_index(np.argmin(grid[:, :, 0]), grid[:, :, 0].shape)\n",
    "\n",
    "        # Set the pattern\n",
    "        updated_grid[i, max_x, max_y, :] = 1\n",
    "        updated_grid[i, min_x, min_y, :] = 0\n",
    "\n",
    "    return updated_grid"
   ],
   "id": "c2ce273010cdf8ff",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T18:44:37.968595Z",
     "start_time": "2024-06-24T18:44:27.111677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming x_train, x_validate, x_test, y_train, y_validate, and y_test are defined\n",
    "print(\"x_train shape:\", np.shape(x_train))\n",
    "print(\"x_validate shape:\", np.shape(x_validate))\n",
    "print(\"x_test shape:\", np.shape(x_test))\n",
    "print(\"y_train shape:\", np.shape(y_train))\n",
    "print(\"y_validate shape:\", np.shape(y_validate))\n",
    "print(\"y_test shape:\", np.shape(y_test))\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "train_dataset = TensorDataset(torch.from_numpy(x_train).float().requires_grad_(), torch.from_numpy(y_train).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(x_validate).float().requires_grad_(), torch.from_numpy(y_validate).float())\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(x_test).float().requires_grad_(), torch.from_numpy(y_test).float())\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = ConvLSTM(input_dim=4, hidden_dim=[128, 64], kernel_size=(3,3), num_layers=2, \n",
    "                 physics_kernel_size=(3,3), output_dim=1, batch_first=True, bias=True, \n",
    "                 return_all_layers=False, window_size=1, num_heads=8)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 5\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, _ = model(batch_x)\n",
    "        output = output.squeeze(1)\n",
    "        # Enable gradients for the output tensor\n",
    "        output.requires_grad_(True)\n",
    "        \n",
    "        # Compute data loss\n",
    "        \n",
    "        data_loss = criterion(output, batch_y)\n",
    "        \n",
    "        # Compute physics loss\n",
    "\n",
    "        # Create an empty tensor with the same shape as batch_x\n",
    "        rin_physics = torch.zeros_like(batch_x, device=device, requires_grad=True)\n",
    "        rin_physics = update_grid(rin_physics.cpu().detach().numpy())\n",
    "        # print(rin_physics.shape, batch_x.shape)\n",
    "        # print(\"Shape after update_grid:\", rin_physics.shape)\n",
    "       # rin_physics = rin_physics.view(32, 40, 40, 4)  # Use view instead of reshape\n",
    "        rin_physics = torch.tensor(rin_physics,dtype=torch.float32,device=device, requires_grad=True)  # Move back to GPU if needed\n",
    "        \n",
    "        output, _ = model(rin_physics)\n",
    "        physics_loss = model.advection_loss(rin_physics, output)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = data_loss   +  physics_loss \n",
    "        \n",
    "        # loss = torch.max(0.5 * torch.abs(data_loss - physics_loss))\n",
    "        # Backward pass\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        # Update weights and velocities\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output, _ = model(batch_x)\n",
    "            output = output.squeeze(1)\n",
    "            \n",
    "            # Compute data loss\n",
    "            data_loss = criterion(output, batch_y)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Combine losses\n",
    "            loss = data_loss \n",
    "            \n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "       \n",
    "        # Forward pass\n",
    "        output, _ = model(batch_x)\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        # Compute data loss\n",
    "        data_loss = criterion(output, batch_y)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = data_loss \n",
    "        \n",
    "        test_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ],
   "id": "6798790d41b24543",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (588, 40, 40, 4)\n",
      "x_validate shape: (196, 40, 40, 4)\n",
      "x_test shape: (196, 40, 40, 4)\n",
      "y_train shape: (588, 40, 40, 1)\n",
      "y_validate shape: (196, 40, 40, 1)\n",
      "y_test shape: (196, 40, 40, 1)\n",
      "Epoch [1/5]\n",
      "Train Loss: 0.0030, Val Loss: 0.0029\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 79\u001B[0m\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;66;03m# Update weights and velocities\u001B[39;00m\n\u001B[1;32m     77\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 79\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m batch_x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     81\u001B[0m train_loss \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m# Validation loop\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch\n",
    "\n",
    "def animate_comparison(model, data_loader, output_folder):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract a single batch for visualization\n",
    "    inputs, targets = next(iter(data_loader))\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = inputs.to(next(model.parameters()).device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        outputs,states = model(inputs)\n",
    "    \n",
    "    # Assuming outputs and targets are on GPU, move them to CPU and convert to numpy\n",
    "    outputs = outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "    \n",
    "    # Prepare figure for animation\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    def update(i):\n",
    "        # Clear previous content\n",
    "        ax[0].cla()\n",
    "        ax[1].cla()\n",
    "        \n",
    "        # Update content for frame i\n",
    "        ax[0].imshow(outputs[i].squeeze(), cmap='gray')\n",
    "        ax[0].set_title('Output')\n",
    "        ax[1].imshow(targets[i].squeeze(), cmap='gray')\n",
    "        ax[1].set_title('Target')\n",
    "    \n",
    "    # Create animation\n",
    "    anim = FuncAnimation(fig, update, frames=len(outputs), interval=200)\n",
    "    \n",
    "    # Save animation\n",
    "    anim.save(f'{output_folder}/convLSTM_attention_physics_comparison_animation.gif', writer='imagemagick')\n",
    "\n",
    "# Example usage\n",
    "animate_comparison(model, test_loader, '/home/sushen/PhysNet-RadarNowcast/images/convLSTM_attention_ipinn')"
   ],
   "id": "7370c7b763a59edc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
