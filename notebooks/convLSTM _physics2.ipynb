{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torch.nn import functional as F\n",
    "import torch.autograd as autograd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias).float()\n",
    "\n",
    "        self.physics_conv = nn.Conv2d(in_channels=self.input_dim,\n",
    "                                      out_channels=4 * self.hidden_dim,\n",
    "                                      kernel_size=self.kernel_size,\n",
    "                                      padding=self.padding,\n",
    "                                      bias=False).float()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        # Check if input_tensor has an extra dimension (sequence length)\n",
    "        if input_tensor.dim() == 5:\n",
    "            input_tensor = input_tensor.squeeze(1)\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        physics_conv = self.physics_conv(input_tensor)\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        pc_i, pc_f, pc_o, pc_g = torch.split(physics_conv, self.hidden_dim, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i + pc_i)\n",
    "        f = torch.sigmoid(cc_f + pc_f)\n",
    "        o = torch.sigmoid(cc_o + pc_o)\n",
    "        g = torch.tanh(cc_g + pc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM_iPINN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,output_dim,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM_iPINN, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "        # Initialize velocities as trainable parameters\n",
    "        self.velocity_x = nn.Parameter(torch.tensor(0.1))\n",
    "        self.velocity_y = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "        self.output_conv = nn.Conv2d(in_channels=hidden_dim[-1],\n",
    "                                      out_channels=output_dim,\n",
    "                                      kernel_size=1,\n",
    "                                      padding=0)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if input_tensor.dim() == 4:\n",
    "            # (b, h, w, c) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(0, 3, 1, 2).unsqueeze(1)\n",
    "        elif input_tensor.dim() == 5:\n",
    "            if not self.batch_first:\n",
    "                # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "                input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, t, _, h, w = input_tensor.size()\n",
    "\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        # Remove the sequence length dimension before applying the output convolution\n",
    "        output = self.output_conv(layer_output_list[0].squeeze(1))\n",
    "        # Permute the output to have shape (b, h, w, c)\n",
    "        output = output.permute(0, 2, 3, 1)\n",
    "        return output, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "\n",
    "    def advection_loss(self, input_tensor, output_tensor):\n",
    "        grad = torch.autograd.grad(outputs=output_tensor, inputs=input_tensor,\n",
    "                                   grad_outputs=torch.ones_like(output_tensor), create_graph=True)[0]\n",
    "        dudx = grad[:, :, 0]\n",
    "        dudy = grad[:, :, 1]\n",
    "        dudt = grad[:, :, 2]\n",
    "\n",
    "        physics = dudt + self.velocity_x * dudx + self.velocity_y * dudy\n",
    "        loss = torch.mean((physics) ** 2)\n",
    "\n",
    "        return loss"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# load radar data\n",
    "movies = np.load('data/radar_movies.npy')\n",
    "movies.shape # (980, 40, 40, 20) -- here each movie is of length 20\n",
    "\n",
    "# in our model we will use the first four images as inputs and predict the\n",
    "# fifth image\n",
    "x = movies[:, :, :,  :4]\n",
    "y = movies[:, :, :, 4:5]\n",
    "\n",
    "\n",
    "# function: animation of a sequence of radar data (shape = nx,ny,ntime)\n",
    "def animate(x):\n",
    "  fig, ax = plt.subplots()\n",
    "  vmax = np.max(x)\n",
    "  im = ax.imshow(x[:,:,0], vmin=0, vmax=vmax)\n",
    "  fig.colorbar(im)\n",
    "  plt.axis('off')\n",
    "  def anim_(i):\n",
    "      im.set_data(x[:,:,i])\n",
    "      ax.set_title(str(i+1) + '/' + str(x.shape[2]))\n",
    "  anim = animation.FuncAnimation(\n",
    "      fig, anim_, interval=300, frames=x.shape[2], repeat_delay=1000)\n",
    "  plt.show()\n",
    "\n",
    "# i_plt = 340\n",
    "# i_plt = 123\n",
    "i_plt = np.int32(np.random.sample() * movies.shape[0])\n",
    "animate(x[i_plt,:,:,:])\n",
    "plt.show()\n",
    "\n",
    "# train validate test split\n",
    "tvt = np.tile(['train','train','train','validate','test'], y.shape[0])[:y.shape[0]]\n",
    "x_train = x[np.where(tvt == 'train')]\n",
    "y_train = y[np.where(tvt == 'train')]\n",
    "x_validate = x[np.where(tvt == 'validate')]\n",
    "y_validate = y[np.where(tvt == 'validate')]\n",
    "x_test = x[np.where(tvt == 'test')]\n",
    "y_test = y[np.where(tvt == 'test')]\n",
    "\n",
    "n_test = x_test.shape[0]\n",
    "i_plt = np.int32(np.random.sample() * n_test)\n",
    "true = np.append(x_test[i_plt,:,:,:], y_test[i_plt,:,:,:], axis=2)\n",
    "# plot an input/output pair\n",
    "i_plt = 20\n",
    "i_plt = np.int32(np.random.sample() * x_train.shape[0])\n",
    "for jj in range(4):\n",
    "  plt.subplot(1,5,jj+1)\n",
    "  plt.imshow(x_train[i_plt,:,:,jj])\n",
    "  plt.axis('off')\n",
    "  plt.title('input')\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(y_train[i_plt,:,:,0])\n",
    "plt.title('target output')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming x_train, x_validate, x_test, y_train, y_validate, and y_test are defined\n",
    "print(\"x_train shape:\", np.shape(x_train))\n",
    "print(\"x_validate shape:\", np.shape(x_validate))\n",
    "print(\"x_test shape:\", np.shape(x_test))\n",
    "print(\"y_train shape:\", np.shape(y_train))\n",
    "print(\"y_validate shape:\", np.shape(y_validate))\n",
    "print(\"y_test shape:\", np.shape(y_test))\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(x_train).float().requires_grad_(), torch.from_numpy(y_train).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(x_validate).float().requires_grad_(), torch.from_numpy(y_validate).float())\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(x_test).float().requires_grad_(), torch.from_numpy(y_test).float())\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = ConvLSTM_iPINN(input_dim=4, hidden_dim=40, kernel_size=(3, 3), num_layers=3, output_dim=1, bias=True, return_all_layers=False)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, _ = model(batch_x)\n",
    "        output = output.squeeze(1)\n",
    "        # Enable gradients for the output tensor\n",
    "        output.requires_grad_(True)\n",
    "        \n",
    "        # Compute data loss\n",
    "        \n",
    "        data_loss = criterion(output, batch_y)\n",
    "        \n",
    "        # Compute physics loss\n",
    "        physics_loss = model.advection_loss(batch_x, output)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = data_loss + physics_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights and velocities\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output, _ = model(batch_x)\n",
    "            output = output.squeeze(1)\n",
    "            \n",
    "            # Compute data loss\n",
    "            data_loss = criterion(output, batch_y)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Combine losses\n",
    "            loss = data_loss \n",
    "            \n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "       \n",
    "        # Forward pass\n",
    "        output, _ = model(batch_x)\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        # Compute data loss\n",
    "        data_loss = criterion(output, batch_y)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = data_loss \n",
    "        \n",
    "        test_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def animate2(x1, x2, i_plt, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    vmax = np.max(np.append(x1, x2))\n",
    "    vmin = np.min(np.append(x1, x2))\n",
    "\n",
    "    frames = min(x1.shape[2], x2.shape[2])  # Get the minimum number of frames\n",
    "\n",
    "    # Initialize lists to store MAE and RMSE for each frame\n",
    "    mae_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    for i in range(frames):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(18, 5))  # Adjust the figure size as needed\n",
    "\n",
    "        im1 = ax[0].imshow(x1[:, :, i], vmin=vmin, vmax=vmax)\n",
    "        im2 = ax[1].imshow(x2[:, :, i], vmin=vmin, vmax=vmax)\n",
    "\n",
    "        diff = x1[:, :, i] - x2[:, :, i]\n",
    "        vmin_diff, vmax_diff = np.min(diff), np.max(diff)\n",
    "        norm = mcolors.Normalize(vmin=vmin_diff, vmax=vmax_diff)\n",
    "        im3 = ax[2].imshow(diff, cmap='viridis', norm=norm)\n",
    "\n",
    "        fig.subplots_adjust(right=0.8)\n",
    "        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "        fig.colorbar(im3, cax=cbar_ax)\n",
    "\n",
    "        # Add labels\n",
    "        ax[0].set_title('Predicted')\n",
    "        ax[1].set_title('True')\n",
    "        ax[2].set_title('Difference')\n",
    "\n",
    "        # Remove axis ticks\n",
    "        for a in ax:\n",
    "            a.set_xticks([])\n",
    "            a.set_yticks([])\n",
    "\n",
    "        # Calculate MAE and RMSE for the current frame\n",
    "        mae = np.mean(np.abs(diff))\n",
    "        rmse = np.sqrt(np.mean(diff ** 2))\n",
    "\n",
    "        # Append MAE and RMSE to the lists\n",
    "        mae_list.append(mae)\n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "        # Add MAE and RMSE to the plot title\n",
    "        fig.suptitle(f'Physics informed ConvLSTM | i = {i_plt} Predict Frame - MAE: {mae:.4f}, RMSE: {rmse:.4f}', fontsize=12)\n",
    "\n",
    "        # Save the current frame as an image file\n",
    "        filename = f'PhyConvLSTMframe_{i_plt:04d}_{i+1:04d}.png'\n",
    "        filepath = os.path.join(output_folder, filename)\n",
    "        plt.savefig(filepath, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "    # # Save MAE and RMSE for each frame to a CSV file\n",
    "    # csv_file = os.path.join(output_folder, f'mae_rmse_{i_plt:04d}.csv')\n",
    "    # with open(csv_file, mode='w', newline='') as file:\n",
    "    #     writer = csv.writer(file)\n",
    "    #     writer.writerow(['Frame', 'MAE', 'RMSE'])\n",
    "    #     for i in range(frames):\n",
    "    #         writer.writerow([i+1, mae_list[i], rmse_list[i]])\n",
    "\n",
    "    print(f\"Images and CSV file saved in the folder: {output_folder}\")\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "# # Convert y_test to a PyTorch tensor\n",
    "# y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "# y_test_pred = torch.empty_like(y_test_tensor,device=device)  # Initialize an empty tensor to store predictions\n",
    "\n",
    "# with torch.no_grad():  # Disable gradient computation for inference\n",
    "#     for i, (batch_x, batch_y) in enumerate(test_loader):\n",
    "#         batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "#         outputs = model(batch_x)  # Get the model outputs for this batch\n",
    "        \n",
    "        \n",
    "\n",
    "#         y_test_pred[i * test_loader.batch_size:(i + 1) * test_loader.batch_size] = outputs  # Store the predictions\n",
    "        \n",
    "\n",
    "# # Now you have y_test_pred ready to use\n",
    "# # Assuming x_test, y_test, and y_test_pred are PyTorch tensors\n",
    "# n_test = x_test.shape[0]\n",
    "# i_plt = np.int32(np.random.sample() * n_test)\n",
    "\n",
    "# print('Showing test case i =', i_plt)\n",
    "\n",
    "# # Convert NumPy arrays to PyTorch tensors\n",
    "# x_test_tensor = torch.from_numpy(x_test).float()\n",
    "# y_test_tensor = torch.from_numpy(y_test).float()\n",
    "# # Assuming your model is defined and trained\n",
    "\n",
    "\n",
    "# # Repeat the channel dimension of y_test_tensor to match x_test_tensor\n",
    "\n",
    "\n",
    "# print(f'shape of x test tensor: {x_test_tensor[i_plt].shape}')\n",
    "# print(f'shape of y test tensor: {y_test_tensor[i_plt].shape}')\n",
    "# print(f'shape of y test pred: {y_test_pred[i_plt].shape}')\n",
    "\n",
    "# for i_plt in range(10):\n",
    "#     true = np.append(x_test[i_plt,:,:,:], y_test[i_plt,:,:,:], axis=2)\n",
    "#     pred = np.append(x_test[i_plt,:,:,:], y_test_pred.cpu()[i_plt,:,:,:], axis=2)\n",
    "\n",
    "#     anim = animate2(pred, true,i_plt,'/home/sushen/PhysNet-RadarNowcast/2d-advection/pinn_plot')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    for i_plt, (batch_x, batch_y) in enumerate(test_loader):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output, _ = model(batch_x)\n",
    "        \n",
    "        # Compute data loss\n",
    "        data_loss = criterion(output, batch_y)\n",
    "\n",
    "        # Combine losses\n",
    "        loss = data_loss\n",
    "\n",
    "        test_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        # Plot the results for the current batch\n",
    "        for j in range(batch_x.size(0)):\n",
    "            print(f'Batch {i_plt}, Sample {j}, batch_x.shape: {batch_x[j].shape}, batch_y.shape: {batch_y[j].shape}, output.shape: {output[j].shape}')\n",
    "            true = np.append(batch_x[j].cpu().numpy(), batch_y[j].cpu().numpy(), axis=2)\n",
    "            pred = np.append(batch_x[j].cpu().numpy(), output[j].cpu().numpy(), axis=2)\n",
    "            anim = animate2(pred, true, i_plt * batch_x.size(0) + j, '/home/sushen/PhysNet-RadarNowcast/2d-advection/pinn_plot')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Set the directories where the images are located\n",
    "convlstm_dir = '/home/sushen/PhysNet-RadarNowcast/2d-advection/pinn_plot'\n",
    "phyconvlstm_dir = '/home/sushen/PhysNet-RadarNowcast/2d-advection/pinn_plot'\n",
    "\n",
    "# Set the output directory for the combined images\n",
    "output_dir = '/home/sushen/PhysNet-RadarNowcast/2d-advection/pinn_plot/10_combined_images'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Set the range of image numbers\n",
    "start_num = 0\n",
    "end_num = 191  # Inclusive\n",
    "# Set the font and font size for the text\n",
    "font_path = '/home/sushen/PhysNet-RadarNowcast/data/Ubuntu-BoldItalic.ttf'\n",
    "font_size = 18\n",
    "\n",
    "# Loop through the image numbers\n",
    "for i in range(start_num, end_num + 1):\n",
    "    # Generate the filenames for the current image pair\n",
    "    convlstm_filename = f'ConvLSTMframe_{i:04d}_{5:04d}.png'\n",
    "    phyconvlstm_filename = f'PhyConvLSTMframe_{i:04d}_{5:04d}.png'\n",
    "\n",
    "    # Open the images\n",
    "    convlstm_path = os.path.join(convlstm_dir, convlstm_filename)\n",
    "    phyconvlstm_path = os.path.join(phyconvlstm_dir, phyconvlstm_filename)\n",
    "\n",
    "    convlstm_image = Image.open(convlstm_path)\n",
    "    phyconvlstm_image = Image.open(phyconvlstm_path)\n",
    "\n",
    "    # Get the dimensions of the images\n",
    "    width, height = convlstm_image.size\n",
    "\n",
    "    # Create a new blank image with double the height\n",
    "    combined_image = Image.new('RGB', (width, height * 2))\n",
    "\n",
    "    # Paste the ConvLSTMframe image at the top\n",
    "    combined_image.paste(convlstm_image, (0, 0))\n",
    "\n",
    "    # Paste the PhyConvLSTMframe image at the bottom\n",
    "    combined_image.paste(phyconvlstm_image, (0, height))\n",
    "\n",
    "    # Save the combined image\n",
    "    output_filename = f'combined_frame_{i:04d}.png'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    combined_image.save(output_path)\n",
    "\n",
    "    print(f'Combined image saved: {output_path}')\n",
    "\n",
    "print('All images combined successfully.')"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
